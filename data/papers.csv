Timestamp,Title,URL/DOI (please check DOI by collating DOI at the end of https://doi.org/  ),Provider/Creators,Material Type (see: https://help.oercommons.org/support/solutions/articles/42000046908-material-types for Glossary of terms),Education Level,Abstract,Language,Conditions of Use,Primary User,Subject Areas,FORRT Clusters  | see: https://forrt.org/clusters,"User Tags (separate by comma if more than one; capitalize each word, except for prepositions)"
2020-05-06T14:52:48.000Z,Replication and Open Science for Undergraduates,https://funderstorms.wordpress.com/2018/05/10/replication-and-open-science-for-undergraduates/,David Funder,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",Blog post going over the replication crisis and how it has led to the open science movement.,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science",,"Blog,Open Science,Reproducibility Crisis and Credibility Revolution"
10/7/2020 16:08:25,Course materials,https://www.projecttier.org/tier-classroom/course-materials/,Project Tier,"Module, Syllabus",College / Upper Division (Undergraduates),"Learn about courses, in a wide range of fields at a variety of institutions, where principles and resources from Project TIER have been used to teach transparent research methods.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/12/2020 12:58:25,Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations,https://psyarxiv.com/24ncs/,"Mario Gollwitzer, Andrea Abele-Brehm, Christian Fiebach, Roland Ramthun, Anne Scheel, Felix Schönbrodt, Ulf Steinberg",Reading,College / Upper Division (Undergraduates),"Providing access to research data collected as part of scientific publications and publicly funded research projects is now regarded as a central aspect of an open and transparent scientific practice and is increasingly being called for by funding institutions and scientific journals. To this end, researchers should strive to comply with the so-called FAIR principles (of scientific data management), that is, research data should be findable, accessible, interoperable, and reusable. Systematic data management supports these goals and, at the same time, makes it possible to achieve them efficiently. With these revised recommendations on data management and data sharing, which also draw on feedback from a 2018 survey of its members, the German Psychological Society (Deutsche Gesellschaft für Psychologie; DGPs) specifies important basic principles of data management in psychology. Initially, based on discipline-specific definitions of raw data, primary data, secondary data, and metadata, we provide recommendations on the degree of data processing necessary when publishing data. We then discuss data protection as well as aspects of copyright and data usage before defining the qualitative requirements for trustworthy research data repositories. This is followed by a detailed discussion of pragmatic aspects of data sharing, such as the differences between Type 1 and Type 2 data publications, restrictions on use (embargo period), the definition of ""scientific use"" by secondary users of shared data, and recommendations on how to resolve potential disputes. Particularly noteworthy is the new recommendation of distinct ""access categories"" for data, each with different requirements in terms of data protection or research ethics. These range from completely open data without usage restrictions (""access category 0"") to data shared under a set of standardized conditions (e.g., reuse restricted to scientific purposes; ""access category 1""), individualized usage agreements (""access category 2""), and secure data access under strictly controlled conditions (e.g., in a research data center; “access category 3""). The practical implementation of this important innovation, however, will require data repositories to provide the necessary technical functionalities. In summary, the revised recommendations aim to present pragmatic guidelines for researchers to handle psychological research data in an open and transparent manner, while addressing structural challenges to data sharing solutions that are beneficial for all involved parties.",English,CC BY,Student,Social Science,Open Data and Materials,Open data
10/13/2020 9:15:36,Teaching replicable and reproducible science,https://osf.io/x7d45/,Kirsten Lane et al.,"Module, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","Participants will develop materials for teaching replicability and reproducible science. Possible materials to be generated include syllabi, specific assignments, or single lectures or lesson plans. We will provide existing teaching materials and structured activities designed to help participants define learning goals, develop teaching resources to facilitate those goals, and to create appropriate learning assessments.",English,I don't see any of these,Teacher,Social Science,Conceptual and Statistical Knowledge,Teaching
10/16/2020 6:20:13,CREP tutorials,https://www.youtube.com/playlist?list=PLECcejm-vUj-kOyCGXxN2TeAFTRFj1HU1,Dr Jordan Wagge,Videos,College / Upper Division (Undergraduates),This playlist was created to help students and their instructors complete CREP projects.,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge, Replication Research",Reproducibility and replicability
10/16/2020 6:22:33,"Open Research:  Examples of good practice, and resources across disciplines",https://docs.google.com/document/d/1Vblnwym18kX2tmL3VcooSdN1CLg3xkppARGCGB39lvw/edit,"Emily K Farran, Priya Silverstein, Camilla Gilmore",Reading,"College / Upper Division (Undergraduates), Adult Education","This is a document about open science across disciplines, together with resources to these disciplines.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Open Data and Materials,Open science
10/17/2020 17:43:27,The p-value misconception eradication challenge,http://daniellakens.blogspot.com/2020/10/the-p-value-misconception-eradication.html,Daniel Lakens,Reading,College / Upper Division (Undergraduates),"If you have educational material that you think will do a better job at preventing p-value misconceptions than the material in my MOOC, join the p-value misconception eradication challenge by proposing an improvement to my current material in a new A/B test in my MOOC.",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Teaching
10/20/2020 8:05:06,Managing Qualitative Social Science Data,https://managing-qualitative-data.org/,Anon,"Interactive, Lesson, Module",College / Upper Division (Undergraduates),A website with modules and lessons on qualitative research and data management,English,I don't see any of these,Student,"Applied Science, Social Science",Reproducibility and Replicability Knowledge,Qualitative research
10/24/2020 9:10:15,Taking stock of the credibility revolution: Scientific reform 2011-no,https://doi.org/10.13140/RG.2.2.21721.47200/1,Gilad Feldman,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",A book about the credibility revolution,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge, Reproducible Analyses, Replication Research",Credibility revolution
10/25/2020 3:00:22,Science Literacy,https://www.coursera.org/learn/science-literacy,University of Alberta,Full Course,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","Fake news or good science? In a world where we have access to unlimited information, it is hard to sift through the echo chamber of opinions fueled by emotions and personal biases, rather than scientific evidence. Science Literacy will teach you about the process of science, how to think critically, how to differentiate science from pseudoscience, how indigenous wisdom can inform science, how to understand and design a scientific study, and how to critically evaluate scientific communication in the media. Every module will build your new skill-base with real life examples, and at the end of each module you will have to apply these skills to scientific questions, talking points and controversies in the world. Warning: this course requires an open mind and the ability to self-reflect.",English,I don't see any of these,"Student, Parent, Laypeople","Applied Science, Social Science",Conceptual and Statistical Knowledge,Scientific communication
10/25/2020 18:58:28,"Free and low cost resources for graduate  students, postdocs, and early career researchers  (or really anyone else)",https://docs.google.com/document/d/1IFbHIN5OOAO0qz-VfCU9nEx4-x6CfArj1-d8ylA2vsU/edit,Dr Jaclyn Siegel,"Reading, Resource list","College / Upper Division (Undergraduates), Graduate / Professional",A list of free or cheap resources (e.g. free and open statistical software) that support the processes of science ,English,I don't see any of these,"Student, Teacher, Librarian, Early career researchers","Applied Science, Social Science",Open Data and Materials,Open science
10/28/2020 8:00:56,Preregistration for Quantitative Research in Psychology Template,https://docs.google.com/spreadsheets/d/1vlp5GN-HXrtrjCdjE28f_3tT6RiwhQO2vVeOZGOaFsQ/edit#gid=0,Pre-registration standards for psychology,Reading,College / Upper Division (Undergraduates),A template to use for pre-registration,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Preregistration,Transparency
10/28/2020 13:19:48,Credibility of preprints: an interdisciplinary survey of researchers,https://doi.org/10.1098/rsos.201520,Courtney K. Soderberg et al.,Reading,"College / Upper Division (Undergraduates), Graduate / Professional","Preprints increase accessibility and can speed scholarly communication if researchers view them as credible enough to read and use. Preprint services do not provide the heuristic cues of a journal's reputation, selection, and peer-review processes that, regardless of their flaws, are often used as a guide for deciding what to read. We conducted a survey of 3759 researchers across a wide range of disciplines to determine the importance of different cues for assessing the credibility of individual preprints and preprint services. We found that cues related to information about open science content and independent verification of author claims were rated as highly important for judging preprint credibility, and peer views and author information were rated as less important. As of early 2020, very few preprint services display any of the most important cues. By adding such cues, services may be able to help researchers better assess the credibility of preprints, enabling scholars to more confidently use preprints, thereby accelerating scientific communication and discovery.",English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Reproducible Analyses, Open Data and Materials, Replication Research",Preprints
10/29/2020 12:40:49,Data tutorial: Power analysis using G*Power,https://www.youtube.com/watch?v=5J4IKzDfT0w&feature=youtu.be,Andrew Livingstone,Video,"College / Upper Division (Undergraduates), Graduate / Professional",A tutorial on a priori and sensitivity power analyses using G*power,English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,Power analysis
10/29/2020 13:08:48,RIOT science club,https://www.youtube.com/c/RIOTScienceClub/videos,RIOT,Video,"College / Upper Division (Undergraduates), Graduate / Professional",A videos that discusses world-leading open science,English,I don't see any of these,"Student, Teacher, Librarian","Applied Science, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Open science
10/29/2020 13:10:26,"Rethinking Research Assessment:Addressing Institutional Biases in Review, Promotion, and Tenure Decision-Making (part IV)",https://theplosblog.plos.org/2020/10/rethinking-research-assessmentaddressing-institutional-biases-in-review-promotion-and-tenure-decision-making-part-iv/,Ruth Schmidt,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",In our final installment we get into how incumbent processes and perceptions have the advantage.,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Academic Life and Culture,"Diversity, Equity and Inclusion"
10/31/2020 13:16:26,Open Science Saves Lives: Lessons from the COVID-19 Pandemic,https://www.biorxiv.org/content/10.1101/2020.08.13.249847v2,Lonni Besançon et al. ,Reading,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","In the last decade Open Science principles, such as Open Access, study preregistration, use of preprints, making available data and code, and open peer review, have been successfully advocated for and are being slowly adopted in many different research communities. In response to the COVID-19 pandemic many publishers and researchers have sped up their adoption of some of these Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/.",English,I don't see any of these,"Student, Teacher, Policy maker","Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge, Reproducible Analyses, Open Data and Materials, Replication Research",Open science
10/31/2020 13:19:10,Race to the Bottom: Competition and Quality in Science,http://economics.mit.edu/files/20679,Ryan Hill and Caroline Stein,Reading,College / Upper Division (Undergraduates),"This paper investigates how competition to publish first and thereby establish priority impacts the quality of scientific research. We begin by developing a model where scientists decide whether and how long to work on a given project. When deciding how long to let their projects mature, scientists trade off the marginal benefit of higher quality research against the marginal risk of being preempted. The most important (highest potential) projects are the most competitive because they induce the most entry. Therefore, the model predicts these projects are also the most rushed and lowest quality. We test the predictions of this model in the field of structural biology using data from the Protein Data Bank (PDB), a repository for structures of large macromolecules. An important feature of the PDB is that it assigns objective measures of scientific quality to each structure. As suggested by the model, we find that structures with higher ex-ante potential generate more competition, are completed faster, and are lower quality. Consistent with the model, and with a causal interpretation of our empirical results, these relationships are mitigated when we focus on structures deposited by scientists who – by nature of their employment position – are less focused on publication and priority.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge, Academic Life and Culture","Publish or perish culture, Quality of publication"
10/31/2020 13:20:41,"Crisis research, fast and slow",http://www.the100.ci/2020/03/26/crisis-research-fast-and-slow/,Anna Scheel,Reading,College / Upper Division (Undergraduates),A blog about the crisis of research,English,I don't see any of these,Student,"Applied Science, Social Science",Open Data and Materials,Open science
11/2/2020 11:13:04,Towards inclusive funding practices for early career researchers,https://osf.io/9sfm8/,Charlotte de Winde,Reading,College / Upper Division (Undergraduates),"Securing research funding is a challenge faced by most scientists in academic institutions worldwide. Funding success rates for all career stages are low, but the burden falls most heavily on early career researchers (ECRs) - young investigators in training and new principal investigators - who have a shorter track record and are dependent on funding to establish their academic career. The low number of career development awards and the lack of sustained research funding results in the loss of ECR talent in academia. Several steps in the current funding process, from grant conditions to the review process, play significant roles in the distribution of funds. Furthermore, there is an imbalance among certain research disciplines and labs of influential researchers that receive more funding. As a group of ECRs with global representation, we examined funding practices, barriers, facilitators, and alternatives to the current funding systems to diversify risk or award grants on a partly random basis. Based on our discussions, research, and collective opinions, we detail recommendations for funding agencies and grant reviewers to improve ECR funding prospects worldwide and promote a fairer and more inclusive funding landscape for ECRs.",English,I don't see any of these,Student,"Applied Science, Social Science",Academic Life and Culture,Funding
11/19/2020 12:37:56,Data management,https://www.youtube.com/playlist?list=PLy1c0eubiYXlBLTk4T_i-IQGopzSWfSuj,UARK PsychStats,Video,College / Upper Division (Undergraduates),Videos that provide information on data management,English,I don't see any of these,Student,"Applied Science, Life Science, Social Science",Reproducibility and Replicability Knowledge,Data management
11/21/2020 7:16:27,Bayesian Cognitive Modelling,https://bayesmodels.com/,Michael Lee and Eric-Jan Wagenmakers,Reading,College / Upper Division (Undergraduates),"This site is dedicated to the book “Bayesian Cognitive Modeling: A Practical Course”, published by Cambridge University Press.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,Bayesian statistics
11/21/2020 7:20:31,A. Solomon Kurz website,https://solomonkurz.netlify.app/post/,A.Solomon Kurz,Reading,College / Upper Division (Undergraduates),A blog posts about Bayesian statistics,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,Bayesian statistics
11/21/2020 7:21:40,Richard McElreath,https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/videos,Richard McElreath,Videos,College / Upper Division (Undergraduates),A series of youtube videos about his book on statistical rethinking and Bayesian statistics,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Bayesian statistics
11/21/2020 7:24:19,Bayes Rules! An Introduction to Bayesian Modelling in R,https://www.bayesrulesbook.com/,"Alicia A. Johnson, Miles Ott, Mine Dogucu",Reading,College / Upper Division (Undergraduates),"Bayesian statistics?! Once an obscure term outside specialized industry and research circles, Bayesian methods are enjoying a renaissance. The title of this book speaks to what all the fuss is about: Bayes rules! Bayesian methods provide a powerful alternative to the frequentist methods that are ingrained in the standard statistics curriculum. Though frequentist and Bayesian methods share a common goal – learning from data – the Bayesian approach to this goal is gaining popularity for many reasons: (1) Bayesian methods allow us to interpret new data in light of prior information, formally weaving both into a set of updated information; (2) relative to the confidence intervals and p-values utilized in frequentist analyses, Bayesian results are easier to interpret; (3) Bayesian methods can shine in settings where frequentist “likelihood” methods break down; and (4) the computational tools required for applying Bayesian techniques are increasingly accessible. Unfortunately, the popularity of Bayesian statistics has outpaced the curricular resources needed to support it. To this end, the primary goal of Bayes Rules! is to make modern Bayesian thinking, modeling, and computing accessible to a broad audience.",English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Bayesian statistics
12/3/2020 12:59:00,Online Experiments for Language Scientists,https://kennysmithed.github.io/oels2020/,Kenny Smith,Syllabus,College / Upper Division (Undergraduates),"Many areas in the language sciences rely on collecting data from human participants, from grammaticality judgments to behavioural responses (key presses, mouse clicks, spoken responses). While data collection traditionally takes place face-to-face, recent years have seen an explosion in the use of online data collection: participants take part remotely, providing responses through a survey tool or custom experimental software running in their web browser, with surveys or experiments often being advertised on crowdsourcing websites like Amazon Mechanical Turk (MTurk) or Prolific. Online methods potentially allow rapid and low-effort collection of large samples, and are particularly useful in situations where face-to-face data collection is not possible (e.g. during a pandemic); however, building and running these experiments poses challenges that differ from lab-based methods.

This course will provide a rapid tour of online experimental methods in the language sciences, covering a range of paradigms, from survey-like responses (e.g. as required for grammaticality judgments) through more standard psycholinguistic methods (button presses, mouse clicks) up to more ambitious and challenging techniques (e.g. voice recording, real-time interaction through text and/or streaming audio, iterated learning). Each week we will read a paper detailing a study using online methods, and look at code (written in javascript using jspsych) to implement a similar experiment - the examples will skew towards the topics I am interested in (language learning, communication, language evolution), but we’ll cover more standard paradigms too (grammaticality judgments, self-paced reading) and the techniques are fairly general anyway. We’ll also look at the main platforms for reaching paid participants, e.g. MTurk and Prolific, and discuss some of the challenges around data quality and the ethics of running on those platforms.

No prior experience in coding is assumed, but you have to be prepared to dive in and try things out; the assessment will involve elements of both literature review and coding.",English,CC BY,"Student, Teacher, Researchers",Social Science,Open Data and Materials,Programming
12/5/2020 13:47:07,GGPlot Colors Best Tricks You Will Love,https://www.datanovia.com/en/blog/ggplot-colors-best-tricks-you-will-love/,Alboukadel,Reading,College / Upper Division (Undergraduates),This article presents multiple great solutions you should know for changing ggplot colors.,English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Data visualisation
12/5/2020 15:09:45,Reproducible statistics for psychologists with R Lab Tutorials,https://crumplab.github.io/rstatsforpsych/index.html,Matthew C.Crump,"Reading, Simulation, Textbook",College / Upper Division (Undergraduates),"This is a series of labs/tutorials currently under development (2020-2021) for a two-semester graduate-level statistics sequence in Psychology @ Brooklyn College of CUNY. The goal of these tutorials is to 1) develop a deeper conceptual understanding of the principles of statistical analysis and inference; and 2) develop practical skills for data-analysis, such as using the increasingly popular statistical software environment R to code reproducible analyses. The first set of 13 labs roughly track chapters in “Thinking with Data” (Vokey & Allen, 2018), and the second set of labs (to be written on a weekly basis during the Spring 2021 semester) will roughly track chapters in “Experimental Design and Analysis for Psychology” (Abdi et al., 2009). Although the primary aim is to create lab exercises that reinforce stats concepts and also train basic R coding skills for data-analysis, there are many side goals, including showing students the advantages of using R markdown and Github for creating and communicating research products. For example, aside from these tutorials, I have been developing an R package called vertical (Vuorre & Crump, 2020), that highlights the advantages of learning R for researchers in psychology. And, where possible, I hope to inject some of this broader discussion about awesome R tools and how to use them into the labs (at the same time, a deep-dive requires a separate course…maybe coming soon to a browser near you).",English,I don't see any of these,"Student, Teacher",Math & Statistics,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses",Statistics
12/9/2020 20:08:44,Reproducibility/Open Science and Clinical Psych Reading List,https://docs.google.com/document/d/1ewFMFBEASv7UL0qkOP5HNtFDngzdBax8tDKK-L_0knc/edit,Cody Christopherson,Reading,College / Upper Division (Undergraduates),A reading list for reproducibility in clinical psychology,English,I don't see any of these,Student,Social Science,Reproducibility and Replicability Knowledge,Clinical psychology
12/9/2020 20:09:57,The General Linear Model: Semester 2,https://osf.io/x3vyf/,Patrick S. Forscher,Syllabus,College / Upper Division (Undergraduates),A syllabi used for general linear model,English,CC BY,Student,Math & Statistics,Conceptual and Statistical Knowledge,Statistics
12/9/2020 20:11:55,Data management,https://www.youtube.com/playlist?list=PLy1c0eubiYXlBLTk4T_i-IQGopzSWfSuj,Patrick S. Forscher,Video,College / Upper Division (Undergraduates),A series of video of data management,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,Data management
12/9/2020 20:13:00,Common statistical tests are linear models (or: how to teach stats),https://lindeloev.github.io/tests-as-linear/,Jonas Kristoffer Lindeløv,Reading,College / Upper Division (Undergraduates),This document is summarised in the table below. It shows the linear models underlying common parametric and “non-parametric” tests. Formulating all the tests in the same language highlights the many similarities between them. ,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,Statistics
12/13/2020 10:43:39,Key Practices for the Language Scientist,https://osf.io/ftxnk/,Drs Julia Egger and Eirini Zormpa,"Module, Syllabus",College / Upper Division (Undergraduates),Materials for the Key Practices for the Language Scientist course taught from January to March 2020 at the Max Planck Institute for Psycholinguistics in Nijmegen.,English,CC BY,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Open Data and Materials",Teaching
1/23/2021 13:10:43,All the Weight of Our Dreams On Living Racialized Autism,https://autismandrace.com/,"Lydia X. Z. Brown, E. Ashkenazy and Morénike Giwa Onaiwu",Reading,College / Upper Division (Undergraduates),"For those of us who are autistic and racialized, we often struggle to find representation in mass media, academic work about autism or race, and the activist and advocacy movements that focus on autism, neurodiversity, disability rights, or racial justice. Most autism and autistic organizations, publications about autism, and broader neurodiversity campaigns are predominantly white. Yet disabled Black and Brown students are most likely to be impacted by the school to prison pipeline; the vast majority of U.S. prisoners are disabled and Black or Brown; racialized people are a global majority (which means that autistic people of color far outnumber white autistic people in the world); and the combined impact of race and disability severely increase likelihood for hate crimes, police violence, all other forms of abuse, and repeated retraumatization. Our stories matter and must be told. We hope that this collection will not only speak sharply against our constant erasure and invisibility as (at least) doubly impacted, but will also provide solace and familiarity for our own out there waiting for stories like theirs to be told.",English,I don't see any of these,"Student, Teacher, Administrator, Parent, Librarian","Applied Science, Education, Social Science",Academic Life and Culture,"Diversity, Equity and Inclusion"
1/23/2021 13:47:35,What is Universal Design for Learning (UDL)?,https://www.understood.org/en/learning-thinking-differences/treatments-approaches/educational-strategies/universal-design-for-learning-what-it-is-and-how-it-works,Amanda Morin and Ace Parsi,Reading,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","Universal Design for Learning (UDL) is a way of thinking about teaching and learning that helps give all students an equal opportunity to succeed. This approach offers flexibility in the ways students access material, engage with it and show what they know. Developing lesson plans this way helps all kids, but it may be especially helpful for kids with learning and thinking difference.",English,I don't see any of these,"Student, Teacher, Parent","Career and Technical Education, Education, Social Science",Academic Life and Culture,"Diversity, Equity, Inclusion, Neurodiversity"
1/23/2021 15:13:34,Landmark College Institute for Research & Training (LCIRT) ,https://www.landmark.edu/research-training,Landmark College Institute,Website,"College / Upper Division (Undergraduates), Adult Education","LCIRT was established in 2001 to pioneer LD research, discover innovative strategies and practices, and improve teaching and learning outcomes for students with learning disabilities (like dyslexia), ADHD, and autism spectrum disorder (ASD), and educators in high school and college settings.  Currently, the Institute staff shares this information with education professionals through webinars, online certificate courses, on-site and online workshops, and the signature Summer Institute for educators, among other activities. Fully integrated within the College, LCIRT is instrumental in promoting and leveraging the knowledge and expertise of Landmark College's faculty and staff. The Neurocognitive Lab, which includes vitual reality (VR) equipment, makes LCIRT a popular environment for student research.",English,I don't see any of these,"Student, Teacher, Parent","Education, Social Science",Academic Life and Culture,"Diversity, Equity, Inclusion, Neurodiversity"
1/23/2021 19:28:04,Autistic community and the Neurodiversity movement: Stories from the Frontline,https://researchportal.port.ac.uk/portal/files/16791605/Autistic_Community_and_the_Neurodiversity_Movement.pdf,Steven K. Kapp,Reading,College / Upper Division (Undergraduates),A book about the autistic community and the neurodivergent movement,English,I don't see any of these,"Student, Teacher, Parent","Education, Social Science",Academic Life and Culture,"Diversity, Equity, Inclusion, Neurodiversity, Autism"
3/14/2021 15:14:25,"How racist policing took over American cities, explained by a historian",https://www.vox.com/2020/6/6/21280643/police-brutality-violence-protests-racism-khalil-muhammad,Anna North,Reading,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical, Adult Education","“The problem is the way policing was built,” historian Khalil Muhammad says.",English,I don't see any of these,"Student, Teacher, Administrator, Parent, Librarian",Social Science,Academic Life and Culture,"Diversity, Equity, Inclusion"
5/13/2021 11:54:46,Social Identity and morality lab teaching,https://www.jayvanbavel.com/teachings,Jay Van Bavel,"Module, Reading",College / Upper Division (Undergraduates),"I believe that higher education should be focused on developing passionate, critical, independent and creative thought, and the transmission of knowledge should be in the service of developing these skills. In the classroom, I try to communicate the core issues and controversies within an area in an interactive fashion, drawing students into the material though exercises and debate. This experience provides a deeper understanding of the material and encourages students to think critically about how scientific conclusions depend on the research process. In addition, I try to teach case studies of scientific controversy on topical issues to illustrate that science is not simply an assembly of facts but a method for developing and testing ideas. In my experience, this approach has led naturally to classroom discussions where students share their own insights and critical perspectives.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Open Data and Materials, Academic Life and Culture",Materials; Mentoring
5/16/2021 15:20:04,Elsevier Title Level Pricing: Dissecting the Bowl of Spaghetti,https://jlsc-pub.org/articles/abstract/10.7710/2162-3309.2410/, Joel B. Thornton  and Curtis Brundy,Reading,College / Upper Division (Undergraduates),"INTRODUCTION This study will explore the issue of pricing opacity associated with prices paid by academic libraries that have recently unbundled from the Elsevier Big Deal journal package. Additionally, this study will provide metrics for assessing the fair market value (FMV) of unbundled journal packages. The pricing metrics will assist academic libraries in negotiations of subscription and open access agreements. METHODS Pricing information was gathered from five academic libraries. The data was analyzed to arrive at two key metrics (adjustment from list price and the average cost per journal) for establishing comparables, i.e., prices paid by similarly sized institutions, to assess the collective FMVs for unbundled Elsevier journal packages. RESULTS & DISCUSSION The study results show that significant variations existed in the way institutions were charged for content. Additionally, the comparables show wide variations among institutions when measured by the overall adjustment from list price and the average cost per journal. CONCLUSION The pricing metrics developed in this study, adjustment from list price (ALP) and average cost per journal (ACJ), will help libraries assess their final net prices for individual journal subscriptions. The results will be useful to administrators, collection development personnel, and negotiating teams in understanding the prices paid by other institutions for unbundled journal packages to determine FMVs.",French,I don't see any of these,"Student, Teacher, Librarian","Applied Science, Business and Communication, Career and Technical Education, Social Science",Academic Life and Culture,Journal; Subscription
5/16/2021 15:21:08,Joy and rigor in behavioral science,https://www.sciencedirect.com/science/article/pii/S0749597821000327,Hanne K.Collins et al.,Reading,"College / Upper Division (Undergraduates), Graduate / Professional","In the past decade, behavioral science has seen the introduction of beneficial reforms to reduce false positive results. Serving as the motivational backdrop for the present research, we wondered whether these reforms might have unintended negative consequences for researchers’ behavior and emotional experiences. In an experiment simulating the research process, Study 1 (N = 449 researchers) suggested that engaging in a pre-registration task impeded the discovery of an interesting but non-hypothesized result. Study 2 (N = 400 researchers) indicated that relative to confirmatory research, researchers found exploratory research more enjoyable, motivating, and interesting; and less anxiety-inducing, frustrating, boring, and scientific. These studies raise the possibility that emphasizing confirmation can shift researchers away from exploration, and that such a shift could degrade the subjective experience of conducting research. Study 3 (N = 314 researchers) introduced a scale to measure “prediction preoccupation”—the feeling of heightened concern over, and fixation with, confirming predictions.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge, Replication Research, Academic Life and Culture",Kindness
5/24/2021 12:47:20,Oxford-Berlin Open Research summer school 2019,https://osf.io/6ytne/,Heise et al.,Module,College / Upper Division (Undergraduates),This projects contains materials from lectures and workshops associated with the Oxford-Berlin Open Research Summer School 2019.,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Open Research
5/24/2021 12:48:30,Berlin|Oxford Summer School 2020,https://osf.io/9qy6s/,Toelch et al. ,"Lesson, Module","College / Upper Division (Undergraduates), Graduate / Professional",This repository contains materials and talks for the Berlin|Oxford Summer School 2020 from 28th September to 1st of October,English,CC BY-NC-ND,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Open Research
6/5/2021 21:30:45,Introduction to simulations,https://www.youtube.com/watch?v=LD5t57MAEB0&list=PLB_6juUbmewMfkdwM5cV_5n7Sroufo-Qb,Professor Dorothy Bishop,Video,"College / Upper Division (Undergraduates), Graduate / Professional","Introduction to simulation - first of a series of short lectures given at University of Oxford in February 2021. Very basic for those with little or no background, starting with simulation in Excel. In part 1, we simulate random data to show how easy it is to get a 'significant' effect if you adopt methods of p-hacking. In part 2, we simulate data with a real group difference and show how you can fail to find the effect in a sample if you have insufficient statistical power. For full course description see: https://drive.google.com/file/d/1yt1m... Sorry - no in-person follow-up available on YouTube, but I hope you find the lectures useful.",English,I don't see any of these,"Student, Teacher","Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Simulation
8/6/2021 16:49:19,Data Management for Psychological Science:  A Crowdsourced Syllabus,https://docs.google.com/document/d/1z15bL9cP84re6d4zdkO60q06lnknnN3xEktN7GnLFFQ/edit,Debolt et al.,Syllabus,College / Upper Division (Undergraduates),"Data management - including data preparation, cleaning, storage, and sharing - is critical to psychological research. Despite its importance, data management is rarely formally taught to students. This syllabus provides detailed descriptions of data management topics, resources, and activities that can be used to create a course or workshop on data management. The syllabus is formatted as a series of modules that motivate the importance of high-quality data management and provide information on best practices at various stages - (1) What is data management and why should we care about it? , (2) Data setup and collection, (3) Data storage, (4) Data cleaning and analysis, (5) Data sharing, (6) Locating existing data, and (7) Writing a data management plan. Each module raises key questions and common errors, as well as resources and suggested assignments to help identify and circumvent mistakes and vulnerabilities. The syllabus is extremely comprehensive and should be tailored for individual use, including putting more emphasis on data management practices specific to one’s subfield. There are many different types of resources, including journal articles, blog posts, podcasts, slide decks, etc. and the syllabus can be adapted for graduate seminars, advanced undergraduate courses, or individual study. Because this syllabus links to work from many researchers, please be sure to give appropriate credit to content creators.",English,I don't see any of these,Teacher,"Applied Science, Business and Communication, Career and Technical Education, Education, English Language Arts, History, Life Science, Math & Statistics, Physical Science, Social Science","Reproducible Analyses, Open Data and Materials","Data management, Reproducible Analyses"
9/13/2021 13:24:28,Repository of Psychological Instruments in Serbian (REPOPSI),https://osf.io/5zb8p/,Laboratory for Research of Individual Differences (LIRA) at the University of Belgrade,Research Material,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","An Open-Access repository of psychological measures, scales, tests, and other research instruments in Serbian. Documented are psychological instruments that have been translated into Serbian and/or adapted for the Serbian population. Hosted on the Open Science Framework and maintained by researchers at the Laboratory for Research of Individual Differences (LIRA) at the University of Belgrade, Serbia.","English, Serbian",CC BY-NC-SA,"Student, Teacher, Researcher","Applied Science, Social Science",Open Data and Materials,"Digital Repository, Research Data, Psychological Measures, Open Repository, Scales, Translation, Adaptation"
10/14/2021 12:00:01,Psychology as a Robust Science,https://osf.io/sxrkn/,Amy Orben,Syllabus,Graduate / Professional,"Is psychology a robust science? To answer such a question, this course will encourage you to think critically about how psychological research is conducted and how conclusions are drawn. 
To enable you to truly understand how psychology functions as a science, however, this course will also need to discuss how psychologists are incentivised, how they publish and how their beliefs influence  the  inferences  they  make.  By  engaging  with  such  issues,  this  course  will  probe  and challenge   the   basic   features   and   functions   of   our   discipline.   We   will  uncover   multiple methodological,  statistical  and  systematic  issues  that  could  impair  the  robustness  of  scientific claims  we  encounter  every  day.  We  will  discuss  the  controversy  around  psychology  and  the replicability of its results, while learning about new initiatives that are currently reinventing the basic foundations of our field. 
The course will equip you with some of the basic tools necessary to conduct robust psychological research fit for the 21st century.The course will be based on a mix of set readings, class discussions and lectures. Readings will include  a  diverse  range  of  journal  articles,  reviews,  editorials,  blog  posts,  newspaper  articles, commentaries, podcasts,videos, and tweets. No exams or papers will be set; but come along with a critical eye and a willingness to discuss some difficult and controversial issues.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Reproducible Analyses, Preregistration, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:04:20,Diversifying Research Methods Syllabi,https://osf.io/y5gr9/,Kane et al. ,Syllabus,Graduate / Professional,"This is a collection of research methodology articles, which are first- or senior-authored by women, to promote diverse perspectives in teaching students about research methods and contributing to improving research practices.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research, Academic Life and Culture",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:05:39,Overcoming the Knowledge Barrier in Open Science,https://osf.io/bk6r7/,Mellor et al.,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",Getting started with open science and knowing where to go. This webinar will introduce participants to major practices in open science and then dive into the resources available to learn how to use these in your own work.,English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Academic Life and Culture",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:08:53,Teaching and Mentoring Open Science,https://osf.io/3dp52/,"Matthew Kim, Adrienne D. Woods, Alexa Ellis and Pamela Davis-Kean",Syllabus,College / Upper Division (Undergraduates),A syllabi about mentoring and teaching open science,English,I don't see any of these,Teacher,Social Science,Academic Life and Culture,Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:10:32,Campbell Open and Reproducible Science Syllabus,https://osf.io/qbm89/,Lorne Campbell,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","The purpose of this course is to acquaint students with recent developments in open science and reproducibility of the research workflow. By the end of this course students will be familiar with documenting their research workflow (e.g., idea generation, hypotheses, study materials and procedures, re-usable data sets, annotated code, meta-data, output), in both a private and public manner, from beginning to end in a way that allows others to reproduce their methods, analyses, and results. Students will alsobecome familiar with using the Open Science Framework to document their own research workflow.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:33:52,Open Science Workshop Materials of the LMU Open Science Center,https://osf.io/zjrhu/,Angelika Stefan Felix Schönbrodt and Lena Schiestel,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Open Science workshop materials created at LMU Munich, available for everyone under a CC-BY license. Look in the README folder for more information.",English,CC BY,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 12:35:58,Open Science and Methodological Improvements,https://osf.io/jvsbg/,John M. Zelenski,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","In recent years, meta-research methods issues (e.g., reproducibility, open science, etc.) have yielded provocative findings, vigorous discussions, and novel innovations. This course will engage with these issues in ways that are primarily practical, i.e., useful to current graduate students’ research. However, doing this will require addressing some basic issues in measurement, statistical inference, ethics, and philosophy of science.Readings, discussions, and activities will provide tools that will help you improve your own research practices, and to better interpret the evidence in published reports. The standards, requirements, and values of publishing high quality research in psychology are changing quickly, and we will engage with the very latest developments in ways that will likely increase the quality and impact of your own work. We will also discuss the advantages and disadvantages of potential reforms to the way psychologists (and otherscientists) conduct and report on research.",English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution, Open Science"
10/14/2021 12:37:44,Draft dated 2016-01-11; please see cuLearn for possible updatesp.1PSYC 5601:Contemporary Research in Personality,https://osf.io/3e2pg/,John M. Zelenski,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course will provide a survey of current personality research and theorywith an emphasis on recent discussions around methods and scientific reporting. We will consider recent accomplishments, questions, and controversies in personality psychology. This includes significant breadth, but given the potential scope,cannot be comprehensive. We will touch on prominent approaches in personality (i.e., traits, goals, motives, emotions, the self, the unconscious, genetics, physiology, culture, evolution, development and change, etc.). I believe the course content will berelevant to most psychology students, and I encourage students to bring their own lines of research to the course for discussion. There are individual differences in almost everything, and personality is also concerned with how psychological processes ‘come together’ within people.
Beyond personality content, it is critical to understand the methods used to generate knowledge about personality. In recent years, meta-research method issues (e.g., reproducibility, open science, etc.) have yielded provocative findings, vigorous discussion, and innovations. This movement will comprise another major theme for the course in 2016. Readings and activities will provide tools that will help you improve your own research practices, and to better interpret the evidencein published reports. The standards, requirements, and values of publishing high quality research in psychology are changing quickly, and we will engage with the very latest developments in ways that will likely increase the quality and impact of your ownwork. We will also discuss the advantages and disadvantages of potential reforms to the way psychologists (and other scientists) conduct and report on research.",English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 12:39:00,Replicability syllabus,https://osf.io/xs89g/,Simine Vazire,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A syllabus about replicability seminar,English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 12:41:54,Economic Statistics with Calculus,https://osf.io/hurgd/,Richard Ball,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A syllabus about economics and statistics,English,I don't see any of these,Student,Social Science,Conceptual and Statistical Knowledge,"Economics, statistics"
10/14/2021 12:43:38,Open and Reproducible Research in Psychology,https://osf.io/zvfu4/,Krista Byers-Heinlein,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Psychological research is in the midst of what has been called the “replication crisis”, in which questions have been raised about the validity of even seminal research findings. How can we tell if the research we’re reading and doing is rigorous and reproducible?  In this course, we will discuss the controversy around and the replicability of our results, while learning about new initiatives that are reinventing our day-to-day scientific workflow. These include preregistration, open sharing of data and materials, new models of scientific publishing, large-scale open collaborations, and statistical reform. The course will involve hands-on practice implementing cutting-edge tools for conducting open and reproducible research in psychology. 
---We acknowledge that Concordia University is located on unceded Indigenous lands. The Kanien’kehá:ka Nation is recognized as the custodians of the lands and waters on which we gather today. Tiohtià:ke/Montréal is historically known as a gathering place for many First Nations. Today, it is home to a diverse population of Indigenous and other peoples. We respect the continued connections with the past, present and future in our ongoing relationships with Indigenous and other peoples within the Montreal community.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Transparency,Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 12:45:26,Research Methods in Social Psychology ,https://osf.io/nxytf/,Lorne Campbell,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course will acquaint students with the major research designs and procedures in social psychology, as well as explore recent methodological innovations that were designed to address issues unique to social psychological research. The objectives are to develop a firm grasp of the research methods available, including the application of these methods in research settings, and statistical considerations of these methods. A consideration of how we “do” science is a theme permeating the entire class (e.g., research transparency, replicability of research findings). 
Topics to be covered include, but are not limited to, transparency of the research process and the replicability of research findings, validity and reliability, mediation and moderation, field research, modelling interdependence (data from groups of 2 or more), multi-level modelling, methods for the study of social cognition, and meta-analysis. Half course; one term.",English,I don't see any of these,Teacher,"Math & Statistics, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:23:34,Capstone: Advanced General,https://osf.io/24ws6/,Katie Corker,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","How do psychologists determine what is true and what is false about human behavior, affect, and cognition? The question encompasses more than we can know from a single study or even a single research paper, and the issues run deeper than just research methods. Instead, we need to consider what it means to conduct and understand science.
We must consider why scientists, as humans themselves, can fall prey to biases and fallacies that interfere with their ability to draw sound conclusions. This course focuses on an emerging field known as meta-science (i.e., the study of the process of science itself) in the context of research in psychology. 
We begin by considering whether and if the research practices of psychologists need to change. We touch briefly on philosophy of science and epistemology before attempting to determine how psychologists form cumulative knowledge and theories. 
We examine meta-analysis and its criticisms, drawing on several prominent historical and recent cases. Student pairs will lead discussion on reviews of diverse research. Capstone serves as a bookend for the psychology major. We examine the field of psychology in a broad, integrative way using a seminar format. A seminar means that this course primarily involves close reading, reflective writing, andthoughtful discussion. With almost no lecture in this-course, students should expect to read and write a lot.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Transparency,Reproducibility Crisis and Credibility Revolution, Open Science"
10/14/2021 13:25:21,Emerging Scientific Research Practices,https://osf.io/m2uh7/,Rodica Damian,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course aims to introduce students to current controversies and new developments in recommended scientific practices. The course is meant to help students think critically about how to conduct better empirical research and how to draw better-informed statistical inferences. The course will be conducted in a “seminar meets workshop” style, with a focus on discussion, understanding, and accumulating hands-on experience with different research practices supporting inference. The course covers a range of approaches that aim to enhance the transparency and reproducibility of scientific research. Although many examples will stem from social and personality psychology, this course is appropriate for Ph.D. students across social science disciplines and related fields. Topics will include: Understanding the problem: introduction to the “replication crisis,” developments and debate How issues related to the interpretation of p-values vs. effect sizes and confidence intervals have contributed to the reproducibility issues and the accumulation of knowledge Understand how questionable research practices (QRPs) and publication bias distort the scientific record; learn how to interpret evidence from the scientific literature given these biases (e.g., with the use of p-curve analysis) Recommendations for improving conduct and reporting of research: e.g., pre-registration, open science tools (OSF) and methods, power analysis The future of science and publishing in the new era.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:27:12,Tópicos Especiais em Biotecnologia: Planejamento e Otimização de Experimentos,https://osf.io/qc2w4/,Caio Maximino and  Monica Gomes Lima-Maximino,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A crise de confiança na ciência. Práticas e condutas questionáveis na pesquisa. Delineamento experimental adequado. Definição de variáveis. Armadilhas à validade. Cálculo amostral. Transparência na pesquisa. Pré-registro. Dados e código aberto.,Portuguese,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:29:37,"Repligate"": Reliability and Reproducibility in Psychology",https://osf.io/ms7ix/,Funder,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This	seminar	will	address	issues	relevant	to	the	current	controversy	over the	reliability	of	psychological	research,	a	controversy	which	seems	to	be	white-hot	right	now. Topics	will	include	(but	not	be	limited	to) 
- Critiques	of	the	current	science,	including	the	claim	that	""most	published	research findings	are	false""
- Controversies	over	the	replicability	of	particular	findings,	including	behavioral	priming	and	ESP
- The	way	practices	by	journal	editors,	granting	agencies,	and	hiring	committees	do	and	do	not	 encourage	reliable,	replicable	research
- Recommendations	for	improving	conduct	and	reporting	of	research,	including	statements	by	 professional	societies,	journals,	and	government	agencies
- Related	methodological	issues	including
o Null-hypothesis	statistical	testing
oThe	""new	statistics""	emphasizing	effect	sizes	and	confidence	intervals
o Exploratory	vs.	confirmatory	research
o p-curving,	the	test	for	excess	significance,	and	other	statistical	tools	intended	to	detect	questionable research
- Defenses	of	the	current	state	of	psychological	research,	and	the	various	kinds	of	push	back	against	what	some	call	the	""anti-false-positives	movement""	and	others	simply	call	""shameless	little	bullies."" Readings	will	include	journal	articles,	editorials,	blog	posts	and	probably	even	a	tweet	or	two. While a	 lengthy	reading	list	is provided,	we	will	be	selective	in	what	we	actually	read	in	depth;	skimming	will	be	 encouraged	when	appropriate.  The	course	structure	will	be	discussion	of	the	readings. That's	all. No	 exams	or	papers. But	do	come	to	class	ready	to	talk.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:30:42,Reproducibility in the Psychological Sciences,https://osf.io/c6ve4/,Jon Grahe,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Science is undergoing a paradigm shift in the way research is conducted and what is considered evidence. Psychology is leading other disciplines in innovative methods to address concerns related to replication and transparency by creating meta-science projects (i.e., Reproducibility Project,  Many Labs) and other tools (Transparency of Publication Guidelines, Open Data Badges) to maximize reproducibility in psychology and other sciences. In addition to reading primary articles reporting findings emerging from these efforts, students will engage in the projects through critical evaluation of research protocols, reanalysis of data, and writing APA research reports of findings.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:34:16,•	[Perspectives on Improving Methods in Psychological Science,https://osf.io/87t4g/,Charlotte Hartwright,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A syllabus about open science,English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:38:38,Boas práticas em ciência,https://osf.io/dwf3z/,Ana Paula Herrmann and Mailton França de Vasconcelos,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Reprodutibilidade e replicabilidade de estudos científicos. 
Randomização, cegamento, planejamento amostral e pré-registro de projeto. Diferença entre estudos exploratórios e confirmatórios, P hacking, HARKing e double dipping. Práticas de openscience. Publicação de artigos científicos, critérios de autoria, diretrizes e checklists de redação, revisão pelos pares. Má conduta científica (plágio, fabricação e falsificação).",Portuguese,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:39:53,Quantitative Emerging Practices and Methods in Psychology,https://osf.io/k7bfc/,Jennifer Howell,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Recently, a reporter in the Chronicle of Higher Education wrote that “psychology is having an uneasy moment” (Zamudio-Suarez, 2016). The “uneasy moment” to which she referred is a movementof field self-criticism that has gained incredible steam and features media coverage, twitter wars, and a numerous methods-focused blogs.The purpose of this course is to sort through the criticisms and recommendations that have emerged in our field. We will address a wide swath of topics including historical and emerging concerns about our practices, modern journal requirements, and recommendations on the horizon. The course will center primarily around three types of topics: 1) philosophy of psychological science, 2) criticisms of past and present practices, 3) practical (andimpractical) solutions.The course will be driven by both discussion and hands-on practice. On most days, we will spend the first half of class discussing readings and the second half of class learning hands-on practices.",English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:41:13, Research Methods in Psychology,https://osf.io/3967b/,Michael Kane,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional"," Broad-based philosophical and methodological perspectives on conducting and interpreting psychological research; considers basic, applied, and translational research, laboratory- and field-based research, and experimental, quasi-experimental, correlational, and longitudinal research designs.",English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:42:42,Research Methods 2020,https://osf.io/au735/,Michael Kane ,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Broad-based philosophical and methodological perspectives on conducting and interpreting psychological research; considers basic, applied, and translational research, laboratory- and field-based research, and experimental, quasi-experimental, correlational, and longitudinal research designs.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:46:56,Open Science & Inclusive Psychology,https://osf.io/sg9ej/,Benjamin Le,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course is an introduction to the Open Science approach to psychology. We will investigate if how the field has experienced a “replicability crisis” and explore the potential structural and methodological factors that may be creating false positives within the psychological literature, using case studies of particular research topics in social/personality and cognitive psychology. Students will learn about advances in methods and novel approaches to conducting research that have been developed in response to critiques of past practices. The process of scientific publishing and alternative models of disseminating knowledge will be examined, along with issues of civil and productive scientific discourse and community-building in the social media era. We will discuss issues of inclusivity and accessibility in psychological science, as well as pathways to conducting research in academic and industry settings.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research, Academic Life and Culture","Reproducibility Crisis and Credibility Revolution,Open Science, Inclusivity, Diversity, Equity"
10/14/2021 13:48:05,History and Philosophy of Psychology,https://osf.io/73zyu/,Professor E.Machery,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","In this course, we will examine the on-going methodological controversies around psychology, cognitive science, and cognitive neuroscience. We will look at the question of replication, statistical reform, measurement of psychological attributes, incentivesfor a successful science, etc. We will read articles and book chapters by scientists and statisticians in addition to some relevant articles by philosophers of science. There is no prerequisites for this course.",English,I don't see any of these,Teacher,Social Science,Conceptual and Statistical Knowledge,"Philosophy, History of Psychology"
10/14/2021 13:49:16,Topics in Social Psychology and Personality,https://osf.io/8ecbz/,Sean Mackinnon,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",This seminar class will focus on the theme of Reproducibility in Social Psychology. We will discuss issues surrounding open science as well as the “replication crisis” in social psychology.,English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:51:02,1Economics 270D: Research Transparency Methods in the Social Sciences,https://osf.io/bt6j2/,Edward Miguel,Syllabus,Graduate / Professional,This course coversa range of approachesthat aim to enhance the transparency and reproducibility of social science research. It is appropriate for Ph.D. students in social science disciplines and related fields.,English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:52:29,"Improving (Our) Science: Reproducibility, Reporting, and Openness",https://osf.io/bcdpv/,Don Moore and Leif Nelson ,Syllabus,Graduate / Professional,"The goal of the course is to become a better scientist. You will learn about newest standards for scientific openness, and how they influence the reporting and interpretation of empirical evidence. One component of the course is an intervention to assist you as a practicing scientist. The hope is that the course will help you to lay out the ideal norms and practices and then give you a bit of practice in implementing them. A second component of the course will examine the discrepancy between scientific values and normative scientific practices. What are the ordinary daily practices of scientists, laboratories, and disciplines and when do they fail to follow best practices? We will consider (and propose) solutions for bringing scientific practice in concert with scientific goals.The third component will be the evaluation of those solutions. Which interventions hold the most promise for improving scientific practice and how will we be able to judge?This is not a typical graduate course. However, it is very relevant to doing effective research. It is not quite a research methods class, not quite a professional issues class, and not quite a self-improvement class, but it is a bit of all of three. The substantive content of interest is your own graduate field and research projects.  The focus of this class is on (a) the practices you use to become expert and contribute to that field, and (b) the normative practices that disrupt the development of knowledge in that field. ",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 13:55:20,Improving (Our) Science,https://osf.io/rifzn/,Brian Nosek,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","The goal of science is to accumulate knowledge about nature. There are scientific values guiding how scientists should work, and scientific practices guiding how scientists do work. This course will examine the discrepancy between scientific values and scientific practices. What are the ordinary daily practices of scientists ,laboratories, and disciplines that deviate from scientific values? Why are they different What can, and should, be done about it We will develop and implement strategies to improve alignment between our own scientific values and practices.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Transparency,Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 13:57:55,Graduate Research Methods,https://osf.io/26c7f/,Brian Nosek,Syllabus,Graduate / Professional,"Completion of this course will provide a foundation for the practice of science. We will wrestle with the fundamental issues for designing and executing a program of research, and in the interpretation and reporting of research results. The class is organized around the development and execution of a single, actual research project from conception through completion. Class hours are devoted to conceptual issues in research design, execution and interpretation. Lab hours are devoted to presentation and critique of research plans.",English,I don't see any of these,Teacher,"Applied Science, Social Science","Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Transparency,Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 16:02:43,Open for Insight (Experimental Methods + Open Science),https://osf.io/qstmv/,Rahal,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A syllabus about open science and experimental methods,English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 16:03:57,When is science (un)reliable?,https://osf.io/9hytb/,Tim Parker and Tom Armstrong,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","In this course, we will explore the so‐called “reproducibility crisis” that has struck fields from psychology and economics to ecology and cancer biology. You will learn statistical principles at the heart of the reproducibility crisis, how disregard for those principles undermines the reliability of scientific inference, and how such disregard has been incentivized by various institutions. You will learn to recognize problematic research practices and will critically evaluate scientific claims both in the scientific literature and in the popular press. Further, you will evaluate and debate proposals for institutional policies designed to reduce bias and improve reproducibility.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 16:06:17,Procedimentos para investigação científica,https://osf.io/4gwd2/,Caio Maximino and Lúcia Cavalcante,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","O profissional formado pelo Curso de Graduação em Psicologia da Unifesspa caracterizar-se-á porpossuir   uma   formação   pluralista   e   generalista,   preparado   para   atuação   multiprofissional   pelaformação interdisciplinar com enfoque crítico, científico e reflexivo visando à promoção da Saúde edo bem-estar humano, nos seus mais variados aspectos.",Portuguese,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 16:08:00,Open Science Seminar: How to do credible research with a high informational value (and how not to do it),https://osf.io/9cd7h/,Felix Schönbrodt,Syllabus,Graduate / Professional,"Students have an overview about the ""historical"" developments and debates of the replication movement in the last years, understand questionable research practices (QRPs) and publication bias and how their prevalence distorts the scientific record.•can judge the quality of (set of) studies based on cues and formally (p-curve, R-index, power, etc.).•know best practices for a reproducible workflow using Rmarkdown, Open Science Framework, and other tools.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 16:09:19,Methodological Advances in Behavioral Research: Crowdsourcing Science,https://osf.io/78s5c/,"artin Schweinsberg, Neil Bearden, & Eric Luis Uhlmann, INSEAD",Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","The results of many published studies across many scientific domains are not easily reproduced by independent laboratories. For example, an initiative by Bayer Healthcare to replicate 67 pre-clinical studies led to a reproducibility rate of 20-25% (Prinz et al., 2011), and researchers at Amgen were only able to replicate 6 of 53 influential cancer biology studies (Begley & Ellis, 2012). Similar replication failures have been reported in social and cognitive psychology (Ebersole et al., 2015; Klein et al., 2014; Open Science Collaboration, 2015). This PhD Boot camp introduces PhD students in the behavioral sciences to 1) the ongoing “crisis of confidence” in science, 2) typical methodological challenges of conducting replications, 3) the philosophy of science and statistical background of replications, 4) highly collaborative approaches to replication, in which findings are replicated in independent laboratories before (rather than after) they are published. As part of the boot camp, students will be organized into replication teams and take part in a crowdsourced pre-publication independent replication project on which they will be creditedas co-authors. Participation in the bootcamp is free, the replications will be funded by a grant from INSEAD, and the infrastructure for data collection is already in place. Crowdsourcing research involves recruiting numerous scientific teams to achieve large-scale projects no single team could feasibly carry out. Leveraging crowds of researchers increases the statistical power and generalizability of research designs, reduces investigator error and bias, and enhances scientific transparency. Actively participating in a large-scale replication effort provides an opportunity for students to experience the power of a crowd of researchers firsthand. Lecture topics will include the scientific crisis caused by high-profile replication failures, publication bias, questionable research practices, the open data movement, and crowdsourced replication efforts, among others.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
10/14/2021 16:27:14,VAZIRE PSYC 180c,https://osf.io/staze/,Simine Vazire,Syllabus,College / Upper Division (Undergraduates),"This course will examine current controversies and new developments in research methods in psychology. The goal of the course is to learn to think critically about how psychological science is conducted and how conclusions are drawn. We will cover both methodological and statistical issues that affect the validity of research in psychology, with an emphasis on social and personality psychology. We will discuss the research process from designing a study to how a study gets published. We will also discuss the recent controversy in psychology about the replicability of scientific results. This course is most suited for students who plan to pursue graduate school in psychology and are preparing for a career conducting research in psychological science.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 16:28:47,PSYC 180C,https://osf.io/yj36v/,Simine Vazire,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course will examine current controversies and new developments in research methods in psychology. The goal of the course is to learn to think critically about how psychological science is conducted and how conclusions are drawn. We will cover both methodological and statistical issues that affect the validity of research in psychology, with an emphasis on social and personality psychology. We will discuss the research process from designing a study to how a study gets published. We will also discuss the recent controversy in psychology about the replicability of scientific results. This course is most suited for students who plan to pursue graduate school in psychology and are preparing for a career conducting research in psychological science.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 16:30:06,VAZIRE PSYC190,https://osf.io/rah35/,Simine Vazire,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course will examine current controversies and new developments in research methods in psychology. The goal of the course is to learn to think critically about how psychological science is conducted and how conclusions are drawn. We will cover both methodological and statistical issues that affect the validity of research in psychology, with an emphasis on social and personality psychology.We will cover the debate about Null Hypothesis Significance Testing (NHST) and alternatives to NHST (e.g., effectestimation).We will also discuss the recent controversy in psychology about the replicability of scientific results.This course is most suited for students who plan to pursue graduate school in psychology and are preparing for a career conducting research in psychological science",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 16:31:28,Rigorous & Reproducible Research Practices,https://osf.io/8bxau/,Heather Urry,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","How do you know whether the quantitative research you’re consuming and producing is rigorous and reproducible? This course will draw on contemporary perspectives to help you answer this question. We'll discuss the whys and hows of statistical inference and transparent research practices (e.g., sample size planning, preregistration, sharing data and materials), and best practices for reporting and evaluating research. We'll also consider thorny issues surrounding conducting and evaluating replication research, giving and responding to scientific criticism, and the everyday incentives that shape scientists' behavior. Students will come away having developed a principled understanding of relevant concepts and a set of concrete tools for producing and consuming high quality science. ",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research, Academic Life and Culture","Reproducibility Crisis and Credibility Revolution,Open Science, Ethics"
10/14/2021 17:04:29,Ciência aberta e reprodutível,https://osf.io/wmakh/,Daniel Umpierre de Moraes,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Esta disciplina destina-se a alunos em níveis de Mestrado e Doutorado do Programade Pós-Graduação em Ciências da Saúde: Cardiologia e Ciências Cardiovasculares e outros programas de pós-graduação. A disciplina está estruturada para promover discussão e capacitação em recursos de transparência e reprodutibilidade em diferentes momentos da pesquisa científica. Os encontros envolverão leituras prévias, as quais são essenciais para real aproveitamento das discussões. Esta disciplina busca compor elementos sobre filosofia da ciência, métodos em pesquisa, erecursos/tecnologia para projetos, e visa aplicabilidade para uma pesquisa mais robusta e eficiente. De forma transversal aos conteúdos, serão trabalhadas as práticas individuais para processos transparentes em pesquisa e análise crítica sobre a ciência atual.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 17:09:36,Open and reproducible science,https://osf.io/swcv2/,Daniel Umpierre de Moraes,Syllabus,Graduate / Professional,"This course is designed to be delivered to master and doctoral students at the Universidade Federaldo Rio Grande do Sul (UFRGS). The structure is organized to (1) provide basic skills to enhance overall transparency and methodological reproducibility, and (2) promote reflection and discussion about open science-not only for simple and good things but also for barriers and some complex issues. Each class requires preparation through reading papers and watching videos. In an ambitious sense, this course aims to compose topics about the philosophy of science, research methods, and useful resources, ultimately targeting more efficient and robust research projects.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution, Open Science"
10/14/2021 17:11:19,Social Sciences Research Methods Centre Replication Workshop,https://osf.io/t2q78/,Nicole Janz,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This workshop will   introduce   students   to   the   process   of   reproducing   published   work. Replicating  other  scholars’  work  is  an  essential  tool for becoming familiar  with  methods, learning to select suitable models, and getting a chance to publish early during their academic career. This replication workshop will therefore provide students with a deeper understanding of statistical modeling and professionalism in their field. With the right amount of value added, a replication study can be submitted to a journal, as has been done by several students in the past.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 17:12:49,PSY 8960-006: History and Methods of Psychology,https://osf.io/r4w2q/,Moin Syed,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","This course is designed to expose students to the history of psychology through a study of the methods used in research over time, with an emphasis on methods used in developmental psychology. Unlike most traditional history of psychology courses, we will focus less on the emergence and differentiation of different schools of psychological thought, and more about the emergence and differentiation of different methods of empirical inquiry. Unlike traditional methods courses, we will focus less on specific research designs and analytic techniques, and more on broader issues of inference that permeate all psychological research (i.e., meta-psychology). Importantly, the historical focus of the course will be grounded in contemporary methodological issues, both to illustrate how many of the current issues have persisted for decades, but also to highlight the tremendous advances in potential solutions we have seen in recent years. Through this course, students will develop a basic familiarity with a core set of issues in the history of psychology and will be competent in rudimentary meta-psychology. These skills are intended to greatly enhance the research acumen of the students, both as rigorous producers of new research and informed consumers of existing work.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
10/14/2021 17:19:55,Psychology’s Credibility Revolution,https://osf.io/9v2sy/,Julia Strand,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","In 2011, Daryl Bem published a paper that seemed to demonstrate evidence for extra sensory perception (ESP). Four years later, the Open Science Collaboration failed to replicate 67 of 100 published psychological studies. These results and others have rocked the field of Psychology (and science more generally) and caused many to re- examine how research is designed, analyzed, and reported. In this seminar, we will explore the factors that contribute to false positives in the literature, including questionable research practices like p-hacking and selective reporting, as well as publication bias, and the incentive structure of science. Along the way, we’ll also discuss the strategies being used to improve the discipline and how to apply them to your own research and consumption of science.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research","Reproducibility Crisis and Credibility Revolution, Open Science"
10/14/2021 17:22:25,Psych 593: Reproducibility in Psychology,https://osf.io/xpe6c/,Brent Roberts and Dan Simons,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional","Psychological science has been going through a crisis of confidence concerning theveracity of the findings that serve as the foundation to our field. This seminar willcover the problems of reproducibility and replicability in psychological science, aswell as the proposed solutions to this crisis.",English,I don't see any of these,Teacher,Social Science,"Reproducibility and Replicability Knowledge, Conceptual and Statistical Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Replication Research",Reproducibility Crisis and Credibility Revolution; Open Science; Conceptual and statistical knowledge
1/31/2022 4:39:18,Mini course on reproducibility & open research,https://www.youtube.com/playlist?list=PLo1sDp2zqD4Rl9AQblQBMQrVblZfEfHcJ,"Dr Alex Mitchell (University of Edinburgh), Niamh MacSweeney (Edinburgh ReproducibiliTea) and Laura Klinkhamer (Edinburgh ReproducibiliTea)",Lecture,College / Upper Division (Undergraduates),"This mini course was designed to introduce Psychology undergraduate students at the University of Edinburgh (who are working on their dissertation in the academic year 2021-2022) to open research principles and practices. 
The course contains 5 videos introducing several core principles of reproducible working which we clustered into the Three T's: Think before you do (about principles of pre-registration), Trace your steps (about detailed record-keeping and version control) and Be Transparent (about adopting a transparent working style and why this is important). 

The videos were created by Dr. Alex Mitchell (Teaching Fellow in Psychology, University of Edinburgh), Niamh MacSweeney (PhD student at the University of Edinburgh & co-organiser Edinburgh ReproducibiliTea) and Laura Klinkhamer (PhD student at the University of Edinburgh & co-organiser Edinburgh ReproducibiliTea). ",English,CC BY,Student,Social Science,"Reproducibility and Replicability Knowledge, Reproducible Analyses, Preregistration, Open Data and Materials, Academic Life and Culture","INTRODUCTION, INTRODUCTORY, REPRODUCIBLE, PRINCIPLES, UNDERGRADUATE, DISSERTATION, THESIS, LECTURE, SERIES, VIDEOS, THREE TS, THREE T'S, PSYCHOLOGY"
2020-05-07T18:08:06.000Z,Pre-registration of clinical trials is associated with fewer positive findings,http://theincidentaleconomist.com/wordpress/pre-registration-of-clinical-trials-is-associated-with-fewer-positive-findings/,Bill Gardner,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",Pre-registration of clinical trials is associated with fewer positive findings,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science",Preregistration,"Blog,Open Science,Reproducibility Crisis and Credibility Revolution"
2020-05-08T06:02:18.000Z,Guide Your Students to Become Better Research Consumers,https://www.psychologicalscience.org/observer/teach-your-students-to-be-better-consumers,Beth Morling,"Reading, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional, Adult Education",Blog post going over making undergraduate students better consumers of research,English,I don't see any of these,"Student, Teacher","Applied Science, Education, Life Science, Social Science",Reproducibility and Replicability Knowledge,"Blog,Open Science"
2020-05-08T06:11:30.000Z,Data peeking without p-hacking,http://daniellakens.blogspot.com/2014/06/data-peeking-without-p-hacking.html,Daniel Lakens,"Reading, R Code","College / Upper Division (Undergraduates), Graduate / Professional",Blog post going over data peeking without p-hacking,English,CC BY-NC,"Student, Teacher","Applied Science, Education, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Blog,Code,Open Science,Reproducibility Crisis and Credibility Revolution"
2020-05-08T06:17:55.000Z,What is a p-value?,http://daniellakens.blogspot.com/2014/08/what-is-p-value_21.html,Daniel Lakens,"Reading, R Code","College / Upper Division (Undergraduates), Graduate / Professional","Blog post going over the p value, misnomers and what p < .05 means",English,CC BY-NC,"Student, Teacher, Researcher","Applied Science, Education, Life Science, Math & Statistics, Physical Science, Social Science",Conceptual and Statistical Knowledge,"Blog,Code,Reproducibility Crisis and Credibility Revolution"
2020-05-08T06:25:26.000Z,Always use Welch t-test,http://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html,Daniel Lakens,"Reading, R Code","College / Upper Division (Undergraduates), Graduate / Professional",This is a blogpost describing why we should use Welch t-test instead of Student t-test,English,CC BY-NC,"Student, Teacher, Researcher","Applied Science, Education, Life Science, Math & Statistics, Physical Science, Social Science",Conceptual and Statistical Knowledge,"Blog,Code,Reproducibility Crisis and Credibility Revolution"
2020-05-08T06:38:05.000Z,Spurious Correlations,http://www.tylervigen.com/spurious-correlations,Tyler Vigen,Teaching/Learning Strategy,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education",A website detailing spurious correlations,English,CC BY,"Student, Teacher","Applied Science, Education, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Website
2020-05-08T06:46:19.902Z,Research method syllabi,https://online225.psych.wisc.edu/,Morton-Ann  Gernsbacher,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education",A syllabus about evaluating research methods,English,CC BY-NC,Teacher,"Applied Science, Education, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge",Syllabus
2020-05-08T07:11:28.000Z,"Methods for Reliable, Transparent, and Open Science",https://github.com/rouderj/transparent-science-course,Jeffrey Rouder and Joachim Vandekerckhove,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","A syllabus about the methods for reliable, transparent and open science",English,I don't see any of these,Teacher,"Applied Science, Education, Life Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration",Syllabus
2020-05-08T07:17:17.000Z,"Good Science, Bad Science",http://www.ejwagenmakers.com/GSBS/GSBS.html,Eric-Jan Wagenmakers,Syllabus,"College / Upper Division (Undergraduates), Graduate / Professional",A syllabi about open science: good science and bad science.,English,I don't see any of these,Teacher,"Applied Science, Education, Life Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Preregistration,Replication Research",Syllabus
2020-05-08T08:15:01.701Z,Significance,https://xkcd.com/882/,Randall Munroe,Diagram/Illustration,College / Upper Division (Undergraduates),Visual comics about significance testing,English,CC BY-NC,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Replication Research",Comic
2020-05-08T08:44:12.396Z,DAGitty,https://dagitty.net,"Johannes Texor, Maciej Liśkiewicz and Benito van der Zander","Teaching/Learning Strategy, R code",Graduate / Professional,"DAGitty is a browser-based environment for creating, editing, and analyzing causal diagrams (also known as directed acyclic graphs or causal Bayesian networks). The focus is on the use of causal diagrams for minimizing bias in empirical studies in epidemiology and other disciplines.",English,I don't see any of these,Researcher,"Applied Science, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Website
2020-05-08T09:00:52.928Z,Reply to Uri Simonsohn's Critique of Default Bayesian Tests,http://jeffrouder.blogspot.ca/2015/04/reply-to-uri-simonsohns-critique-of.html,Jeff Rouder,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",A blog that summarises a reply to Uri Simonsohn's Critique of Default Bayesian Tests,English,I don't see any of these,"Student, Teacher, Researcher","Life Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge",Blog
2020-05-08T09:04:29.640Z,Retraction Watch: Michael LaCour archives,http://retractionwatch.com/category/by-author/lacour/,Alison McCook,Reading,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","How easy is it to change people’s minds? In 2014, a Science study suggested that a short conversation could have a lasting impact on people’s opinions about gay marriage – but left readers disappointed when it was retracted only months later, after the first author admitted to falsifying some of the details of the study, including data collection. We found out about the problems with the paper thanks to Joshua Kalla at the University of California, Berkeley and David Broockman at Stanford University, who tried to repeat the remarkable findings. Last week, Kalla and Broockman published a Science paper suggesting what the 2014 paper showed was, in fact, correct – they found that 10-minute conversations about the struggles facing transgender people reduced prejudices against them for months afterwards. We spoke with Kalla and Broockman about the remarkable results from their paper, and the shadow of the earlier retraction.",English,I don't see any of these,"Student, Teacher","Applied Science, Education, Math & Statistics, Physical Science, Social Science",Reproducibility and Replicability Knowledge,Blog
2020-05-08T09:09:04.110Z,Degrees of Freedom Tutorial,http://ron.dotsch.org/degrees-of-freedom/,Ron Dotsch,"Reading, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","A lot of researchers seem to be struggling with their understanding of the statistical concept of degrees of freedom. Most do not really care about why degrees of freedom are important to statistical tests, but just want to know how to calculate and report them. This page will help. For those interested in learning more about degrees of freedom, take a look at the following resources: This chapter in the little handbook of statistical practice Walker, H. W. (1940). Degrees of Freedom. Journal of Educational Psychology, 31(4), 253-269. I couldn’t find any resource on the web that explains calculating degrees of freedom in a simple and clear manner and believe this page will fill that void. It reflects my current understanding of degrees of freedom, based on what I read in textbooks and scattered sources on the web. Feel free to add or comment.",English,I don't see any of these,"Student, Teacher","Applied Science, Education, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-08T09:14:52.298Z,Calculating the overlap of two normal distributions using monte carlo intergration,http://rpsychologist.com/calculating-the-overlap-of-two-normal-distributions-using-monte-carlo-integration,Kristoffer Magnusson ,"Reading, Student Guide, Teaching/Learning Strategy, R code","College / Upper Division (Undergraduates), Graduate / Professional","I read this post over at the blog Cartesian Faith about Probability and Monte Carlo methods. The post describe how to numerically intregate using Monte Carlo methods. I thought the results looked cool so I used the method to calculate the overlap of two normal distributions that are separated by a Cohen’s d of 0.8. You should head over to the original post if you want a more detailed explanation of the method. And I should add that this is not the most efficient way to calculate the overlap of two gaussian distributions, but it is a fun and pretty intuitive way, plus you can make a cool plot of the result. However, I also show how to get the overlap using the cumulative distribution function and using R’s built-in integration function.",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-08T09:17:09.608Z,Visualizing a One-Way ANOVA using D3.js,http://rpsychologist.com/d3-one-way-anova,Kristoffer Magnusson ,"Interactive, Reading, Simulation, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional",A blog post and tutorial on visualizing one-way ANOVA,English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interactive,Tutorial"
2020-05-08T09:33:04.497Z,Interpreting confidence intervals,http://rpsychologist.com/d3/CI/,Kristoffer Magnusson ,"Interactive, Simulation, Student Guide, Teaching/Learning Strategy, R code","College / Upper Division (Undergraduates), Graduate / Professional",A blog about the visualisation of Confidence intervals,English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction,Simulation,Tutorial"
2020-05-08T09:46:20.016Z,Interpreting Correlations: an interactive visualization,http://rpsychologist.com/d3/correlation/,Kristoffer Magnusson,"Interactive, Reading, Simulation, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","Correlation is one of the most widely used tools in statistics. The correlation coefficient summarizes the association between two variables. In this visualization I show a scatter plot of two variables with a given correlation. The variables are samples from the standard normal distribution, which are then transformed to have a given correlation by using Cholesky decomposition. By moving the slider you will see how the shape of the data changes as the association becomes stronger or weaker. You can also look at the Venn diagram to see the amount of shared variance between the variables. It is also possible drag the data points to see how the correlation is influenced by outliers.",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction,Simulation,Tutorial"
2020-05-08T09:48:23.546Z,Understanding Statistical Power and Significance Testing: an interactive visualization,http://rpsychologist.com/d3/NHST/,Kristoffer Magnusson,"Interactive, Simulation, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","Much has been said about significance testing – most of it negative. Methodologists constantly point out that researchers misinterpret p-values. Some say that it is at best a meaningless exercise and at worst an impediment to scientific discoveries. Consequently, I believe it is extremely important that students and researchers correctly interpret statistical tests. This visualization is meant as an aid for students when they are learning about statistical hypothesis testing. The visualization is based on a one-sample Z-test. You can vary the sample size, power, significance level and the effect size using the sliders to see how the sampling distributions change.",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction,Simulation,Tutorial"
2020-05-08T09:49:58.156Z,Distribution of p-values when comparing two groups,http://rpsychologist.com/d3/pdist/,Kristoffer Magnusson,"Interactive, Reading, Simulation, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional",An interactive visualisation of the distribution of p-values when comparing two groups,English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction,Simulation,Tutorial"
2020-05-08T09:56:13.658Z,Using R and lme/lmer to fit different two- and three- level longitudinal models,https://rpsychologist.com/r-guide-longitudinal-lme-lmer,Kristoffer Magnusson,"Reading, R code",Graduate / Professional,"I often get asked how to fit different multilevel models (or individual growth models, hierarchical linear models or linear mixed-models, etc.) in R. In this guide I have compiled some of the more common and/or useful models (at least common in clinical psychology), and how to fit them using nlme::lme() and lme4::lmer(). I will cover the common two-level random intercept-slope model, and three-level models when subjects are clustered due to some higher level grouping (such as therapists), partially nested models were there are clustering in one group but not the other, and different level 1 residual covariances (such as AR(1)). The point of this post is to show how to fit these longitudinal models in R, not to cover the statistical theory behind them, or how to interpret them.",English,CC BY,"Student, Teacher, Researcher","Education, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-09T07:14:14.675Z,The default bayesian test is prejudiced against small effects,http://datacolada.org/2015/04/09/35-the-default-bayesian-test-is-prejudiced-against-small-effects/,Uri Simonsohn,Reading,"College / Upper Division (Undergraduates), Graduate / Professional","When considering any statistical tool I think it is useful to answer the following two practical questions: 1. ""Does it give reasonable answers in realistic circumstances?"" and 2. ""Does it answer a question I am interested in?"" In this post I explain why, for me, when it comes to the default Bayesian test that's starting to pop up in some psychology publications, the answer to both questions is ""no.""",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Blog
2020-05-09T07:20:18.729Z,Power Posing: Reassessing The Evidence Behind The Most Popular TED Talk,http://datacolada.org/2015/05/08/37-power-posing-reassessing-the-evidence-behind-the-most-popular-ted-talk/,Joe Simmons and Uri Simonsohn,Reading,"College / Upper Division (Undergraduates), Graduate / Professional","A recent paper in Psych Science (.pdf) reports a failure to replicate the study that inspired a TED Talk that has been seen 25 million times. [1]  The talk invited viewers to do better in life by assuming high-power poses, just like Wonder Woman's below, but the replication found that power-posing was inconsequential. If an original finding is a false positive then its replication is likely to fail, but a failed replication need not imply that the original was a false positive. In this post we try to figure out why the replication failed.",English,CC BY,Student,"Education, Social Science","Reproducibility and Replicability Knowledge,Replication Research",Blog
2020-05-09T07:25:38.751Z,Falsely Reassuring: Analyses of ALL p-values,http://datacolada.org/2015/08/24/41-falsely-reassuring-analyses-of-all-p-values-2/,Uri Simonsohn,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",A blog post that describes analyses of all p-values,English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Blog
2020-05-09T07:29:09.914Z,Evaluating the R-Index and the P-Curve,http://disjointedthinking.jeffhughes.ca/2015/01/evaluating-r-index-p-curve/,Jeff Hughes,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",This blog evaluates the R-Index and the P-Curve,English,CC BY-NC,"Student, Researcher",Math & Statistics,Conceptual and Statistical Knowledge,Blog
2020-05-09T07:34:37.002Z,The Amazing Significo: why researchers need to understand poker,http://deevybee.blogspot.com/2016/01/the-amazing-significo-why-researchers.html?m=1,Dorothy Bishop,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",A post that describes significance values,English,I don't see any of these,"Student, Teacher, Researcher","Education, Life Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Blog,Reproducibility Crisis and Credibility Revolution"
2020-05-09T07:42:17.421Z,why p = .048 should be rare (and why this feels counterintuitive),http://sometimesimwrong.typepad.com/wrong/2015/06/why-p-048-should-be-rare-and-why-this-feels-counterintuitive.html,Simine Vazire,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",This post discusses the why p value around .048 should be rare,English,I don't see any of these,"Student, Teacher, Researcher",Math & Statistics,Conceptual and Statistical Knowledge,Blog
2020-05-09T07:46:43.203Z,P-curve visualization updated with log x-axis,http://rpsychologist.com/updated-d3-js-visualization-p-curve-distribution,Kristoffer Magnusson,"Interactive, Reading, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","My p-curve tool now lets you show the x-axis on a log₁₀ scale, which makes it a lot easier to look at really small p-values",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction"
2020-05-09T07:50:00.360Z,Bayesian inference,http://rpsychologist.com/new-d3-js-visualization-bayes,Kristoffer Magnusson,"Interactive, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","This blog post is about a new statistical visualization. This time I’ve tried to illustrate the logic of bayesian updating and hypothesis testing, using a Bayesian t-test.",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction"
2020-05-09T07:53:18.382Z,Short R script to plot effect sizes (Cohen's d) and share overlapping are,http://rpsychologist.com/short-r-script-to-plot-effect-sizes-cohens-d-and-shade-overlapping-area,Kristoffer Magnusson,"Reading, R code","College / Upper Division (Undergraduates), Graduate / Professional",This blog describes how to plot effect sizes (Cohen's d) and shade overlapping area with R scripts,English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-09T08:11:29.826Z,A tale of two papers,http://sometimesimwrong.typepad.com/wrong/2015/11/guest-post-a-tale-of-two-papers.html,Michael Inzlicht,Reading,"College / Upper Division (Undergraduates), Graduate / Professional, Adult Education",An abstract about transparency and robustness for two papers,English,I don't see any of these,"Student, Teacher, Librarian, Researcher","Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Blog,Reproducibility Crisis and Credibility Revolution"
2020-05-10T08:13:40.715Z,teaching psych science,http://www.teachpsychscience.org/,"Lewandowski, Jr., Ciarocco and Strohmetz","Activity/Lab, Asssessment",College / Upper Division (Undergraduates),A collection of activities to teach APA writing and statistics,English,I don't see any of these,"Student, Teacher","Math & Statistics, Social Science",Open Data and Materials,
2020-05-10T08:24:38.748Z,The Open Science Training Handbook,https://open-science-training-handbook.gitbook.io/book/open-science-basics,"Sonja Bezjak, Philipp Conzett, Pedro L. Fernandes, Edit Görögh, Kerstin Helbig, Bianca Kramer, Ignasi Labastida, Kyle Niemeyer, Fotis Psomopoulos, Tony Ross-Hellauer, René Schneider, Jon Tennant, Ellen Verbakel","Activity/Lab, Reading",College / Upper Division (Undergraduates),A collection about open science,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Social Science","Reproducible Analyses,Open Data and Materials",
2020-05-10T08:27:51.326Z,Open science toolbox,https://www.osc.uni-muenchen.de/toolbox/index.html,Lutz Heil,Reading,College / Upper Division (Undergraduates),"There is a vast body of helpful tools that can be used in order to foster Open Science practices. For reasons of clarity, this toolbox aims at providing only a selection of links to these resources and tools. Our goal is to give a short overview on possibilities of how to enhance your Open Science practices without consuming too much of your time",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research",Reproducibility Knowledge
2020-05-10T08:31:46.972Z,Teaching resources spreadsheet,https://docs.google.com/spreadsheets/d/1kzJDrj3dtL9WOz_zRMEhgR7xxo9p3pGQOLJMUVkO1A0/edit#gid=0,Courtney Soderberg,Reading,College / Upper Division (Undergraduates),An excel spreadsheet about collection of open science items,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research",Reproducibility Knowledge
2020-05-10T08:34:48.715Z,Teaching reproducible science hack-a-thon from 2018 SIPS,https://osf.io/x7d45/,Lane et al.,"Lesson, Teaching/Learning Strategy",College / Upper Division (Undergraduates),"Participants will develop materials for teaching replicability and reproducible science. Possible materials to be generated include syllabi, specific assignments, or single lectures or lesson plans. We will provide existing teaching materials and structured activities designed to help participants define learning goals, develop teaching resources to facilitate those goals, and to create appropriate learning assessments",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research",Reproducibility Knowledge
2020-05-10T08:49:35.811Z,Introducing JASP,http://blog.efpsa.org/2015/09/01/introducing-jasp-a-free-and-intuitive-statistics-software-that-might-finally-replace-spss/,Jonas Haslbeck,Reading,College / Upper Division (Undergraduates),A blog about JASP to replace SPSS,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Open Data and Materials,"Blog,Reproducibility Knowledge"
2020-05-10T08:54:23.927Z,The Bayesfactor blog,http://bayesfactor.blogspot.com,Richard Morey,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",Blog about Bayesfactor and statistics,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses",Blog
2020-05-10T08:57:00.500Z,"Statistical Modeling, Causal Inference, and Social Science",http://andrewgelman.com/,Andrew Gelman,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",A blog about statistics and open science,English,I don't see any of these,"Student, Teacher",Math & Statistics,"Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Blog,Reproducibility Knowledge"
2020-05-10T09:00:27.501Z,The Bayesian Reproducibility Project,https://alexanderetz.com/2015/08/30/the-bayesian-reproducibility-project/,Alexander Etz,"Data Set, Reading, Simulation, R code","College / Upper Division (Undergraduates), Graduate / Professional",An abstract about bayesian reproducibility project,English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses","Blog,R code"
2020-05-10T09:04:37.815Z,Bad Science blog on the Guardian webpages,https://www.theguardian.com/science/series/badscience,Ben Goldacre,Reading,"College / Upper Division (Undergraduates), Graduate / Professional",Articles about Bad Science,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Social Science",,"Blog,Reproducibility Knowledge,Open Science"
2020-05-11T10:10:21.373Z,GLIMMPSE,https://glimmpse.samplesizeshop.org/#/,Anon,"Reading, Simulation",Graduate / Professional,Welcome to GLIMMPSE. The GLIMMPSE software calculates power and sample size for study designs with normally distributed outcomes. Select one of the options below to begin a power or sample size calculation.,English,I don't see any of these,"Student, Teacher, Researcher",Math & Statistics,"Reproducible Analyses,Preregistration,Replication Research","Reproducibility Knowledge,Power Analysis Tool"
2020-05-11T10:13:17.293Z,PANGEA,https://jakewestfall.shinyapps.io/pangea/,Jake Westfall,"Reading, Simulation","College / Upper Division (Undergraduates), Graduate / Professional","PANGEA is the first power analysis program for general ANOVA designs (e.g., Winer, Brown, & Michels, 1991). PANGEA can handle designs with any number of factors, each with any number of levels; any factor can be treated as fixed or random; and any valid pattern of nesting or crossing of the factors is allowed.",English,I don't see any of these,"Student, Teacher, Researcher",Math & Statistics,"Reproducible Analyses,Preregistration,Replication Research","Reproducibility Knowledge,Power Analysis Tool"
2020-05-11T10:16:17.692Z,G*Power,http://www.gpower.hhu.de/,The G*Power Team,"Reading, Simulation",College / Upper Division (Undergraduates),"G*Power is a tool to compute statistical power analyses for many different t tests, F tests, χ2 tests, z tests and some exact tests. G*Power can also be used to compute effect sizes and to display graphically the results of power analyses","English, German",I don't see any of these,"Student, Teacher, Researcher",Math & Statistics,"Reproducible Analyses,Preregistration,Replication Research","Reproducibility Knowledge,Power Analysis Tool"
2020-05-11T10:21:15.633Z,Stat 545,https://stat545.com/,Jenny Bryan,"Reading, Student Guide, Tutorial","College / Upper Division (Undergraduates), Graduate / Professional, Adult Education","This site is about everything that comes up during data analysis except for statistical modelling and inference. This might strike you as strange, given R’s statistical roots. First, let me assure you we believe that modelling and inference are important. But the world already offers a lot of great resources for doing statistics with R. The design of STAT 545 was motivated by the need to provide more balance in applied statistical training. Data analysts spend a considerable amount of time on project organization, data cleaning and preparation, and communication. These activities can have a profound effect on the quality and credibility of an analysis. Yet these skills are rarely taught, despite how important and necessary they are. STAT 545 aims to address this gap.",English,CC BY-SA,"Student, Teacher",Math & Statistics,"Conceptual and Statistical Knowledge,Open Data and Materials",Tutorial
2020-05-11T10:25:12.200Z,SWIRL,https://swirlstats.com/,Anon,"Activity/Lab, Homework/Assignment, Student Guide, Teaching/Learning Strategy, Tutorial",College / Upper Division (Undergraduates),"Swirl teaches you R programming and data science interactively, at your own pace, and right in the R console!",English,I don't see any of these,Student,"Applied Science, Math & Statistics","Conceptual and Statistical Knowledge,Open Data and Materials",Tutorial
2020-05-11T10:29:40.604Z,The t-distritbution and its normal approximation,http://rpsychologist.com/new-d3-js-visualization-t-distribution,Kristoffer Magnusson ,"Interactive, Simulation, Student Guide, Teaching/Learning Strategy","College / Upper Division (Undergraduates), Graduate / Professional","I just published a new interactive visualization in my series of basic statistical concepts and techniques. This time I am trying to show how the t-distribution and the normal distribution differs, and how they become very similar for larger sample sizes",English,CC BY,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Interaction,Simulation,Tutorial"
2020-05-11T10:36:37.736Z,Chrome Book Data Science,https://jhudatascience.org/chromebookdatascience/,"Jeff Leek,  Ashley Johnson, Shannon Ellis, Aboozar Hadavand, John Muschelli, Sean Kross, Leo Collado-Torres, Leah Jager, Sarah McClymont, Leslie Myint","Full Course, Lesson, Module","College / Upper Division (Undergraduates), Graduate / Professional","Chromebook Data Science (CBDS) is a free, massive open online educational program offered through Leanpub to help anyone who can read, write, and use a computer to move into data science, the number one rated job.",English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Open Data and Materials",Data Science Tool
2020-05-11T10:41:21.439Z,Hack Your Way To Scientific Glory,http://projects.fivethirtyeight.com/p-hacking/,Anon,"Activity/Lab, Interactive, Simulation, Teaching/Learning Strategy",College / Upper Division (Undergraduates),"You’re a social scientist with a hunch: The U.S. economy is affected by whether Republicans or Democrats are in office. Try to show that a connection exists, using real data going back to 1948. For your results to be publishable in an academic journal, you’ll need to prove that they are “statistically significant” by achieving a low enough p-value.",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials","Demo,Reproducibility Knowledge"
2020-05-11T10:45:22.707Z,Teaching Replication in Psychology: A Guide for Teachers and Students,https://osf.io/28r6g/,Bradford Wiggins,"Lecture, Lecture Notes, Syllabus",College / Upper Division (Undergraduates),"This symposium explores the “replication crisis” from the perspective of teachers and students. Presenters will describe the major issues surrounding replication, explain how students can contribute to replication research both in the classroom and in the lab, and offer curricular models that incorporate replication.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Replication Research,OSF Project
2020-05-11T10:56:42.741Z,CREP project,https://osf.io/wfc6u/,Jon Grahe et al.,"Primary Source, Reading, Project",College / Upper Division (Undergraduates),"CREP’s mission is to provide training, support, and professional growth opportunities for students and instructors completing replication projects, while also addressing the need for direct and direct+ replications of highly-cited studies in the field.",English,I don't see any of these,"Student, Teacher, Researchers","Applied Science, Life Science, Social Science",Replication Research,"Reproducibility Knowledge,OSF Project"
2020-05-11T11:01:00.922Z,False Positive Psychology,https://osf.io/4sehf/,Dermot Lynott,"Lecture, Lecture Notes, Reading",College / Upper Division (Undergraduates),A lecture on when analysis goes wrong  A look at false-positive psychology,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Lecture,Reproducibility Knowledge"
2020-05-11T11:05:09.426Z,Project TIER,https://www.projecttier.org/,TIER Team,"Activity/Lab, Asssessment, Homework/Assignment, Lesson Plan, Module, Syllabus",College / Upper Division (Undergraduates),"Project TIER (Teaching Integrity in Empirical Research) promotes the integration of principles and practices related to transparency and replicability in the research training of social scientists. We develop methods and tools for enhancing research transparency that are specially designed to serve the needs of undergraduate and graduate students, and disseminate them to faculty who teach courses on quantitative methods or supervise student research, as well as to students interested in adopting them independently. Our goal is to reach a day in which training in research transparency becomes standard and ubiquitous in the education of social scientists.",English,CC BY-NC,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Framework,Reproducibility Knowledge"
2020-05-11T11:10:12.608Z,Measurement Matters,https://docs.google.com/document/d/11jyoXtO0m2lUywpC04KjLvI5QcBUY4YtwEvw6cg2cMs/edit,Eiko Fried  and Jessica Flake,Reading,College / Upper Division (Undergraduates),"This resource list contains reading material on the topic of measurement in psychological sciences. We hope the list will be a useful tool in helping researchers to improve measurement practices, and inspire debates about measurement in psychology. We initiated the repository originally as accompanying material to our piece in the APS Observer entitled “Measurement Matters”. We consider it to be a preliminary, active, living document, and plan to update it regularly. We also want to acknowledge that the list is the outcome of many different sources, such as the SIPS pre-conference at SPSP 2018. If you have other papers you would like to see included here, please let us know (eikofried@gmail.com & kayflake@gmail.com).
This is not a complete overview of all relevant papers on measurement in psychology, but a selection of useful papers. We don’t agree with all positions put forward, but believe the papers and books provide a healthy balance of viewpoints. We intend this list as a resource for researchers at all levels of measurement expertise, and marked a few papers with * that we consider to be exceptionally suitable introductory papers for beginners. You can find the list on the Open Science Framework at https://osf.io/zrkd4. ",English,I don't see any of these,"Student, Researcher",Social Science,"Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials","Collection,Reproducibility Knowledge"
2020-05-11T11:12:54.746Z,course syllabi for open and reproducible science,https://osf.io/vkhbt/wiki/home/,Ball et al.,"Reading, syllabus",College / Upper Division (Undergraduates),"A collection of course syllabi from any discipline featuring content to examine or improve open and reproducible research practices. Email to join project, access articles, or add other syllabi.",English,CC BY,"Student, Teacher, Researchers","Applied Science, Life Science, Social Science",Conceptual and Statistical Knowledge,"Collection,Reproducibility Knowledge"
2020-05-11T11:20:16.279Z,Understanding Bayes: Visualization of the Bayes Factor,http://alexanderetz.com/2015/08/09/understanding-bayes-visualization-of-bf/,Alexander Etz,"Diagram/Illustration, Reading, Student Guide, Teaching/Learning Strategy, R code",College / Upper Division (Undergraduates),An abstract about Understanding Bayes and visualising Bayes Factor,English,I don't see any of these,"Student, Teacher",Math & Statistics,"Conceptual and Statistical Knowledge,Reproducible Analyses","Blog,R code"
2020-05-11T11:24:04.657Z,Tutorial/R code for creating scree/parallel analysis plots,https://sakaluk.wordpress.com/2016/05/26/11-make-it-pretty-scree-plots-and-parallel-analysis-using-psych-and-ggplot2/,John K Sakaluk,"Reading, R code","College / Upper Division (Undergraduates), Graduate / Professional","With this post, I’m going to be showing how you can use the psych package in conjunction with ggplot2 in order to create a prettier scree plot with parallel analysis–a very useful visualization when conducting exploratory factor analysis.",English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-11T11:26:11.166Z,Tutorial/R code for creating funnel/forest plots,https://sakaluk.wordpress.com/2016/02/16/7-make-it-pretty-plots-for-meta-analysis/,John K. Sakaluk,"Reading, R code","College / Upper Division (Undergraduates), Graduate / Professional","Meta-analyses are often accompanied by two popular forms of data visualization: forest plots and funnel plots. In this post, I’ll show how quick-and-dirty forest and funnel plots can be created with the metafor package. After, I’ll show how we can instead use the ggplot2 package to create forest plots and use the ggplot2 package to create funnel plots, so that we can have pretty plots that are easy to change/stylize, and that can be produced regardless of which meta-analysis package for R that you elect to use.",English,I don't see any of these,"Student, Teacher","Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-11T11:28:42.584Z,Tutorial/R code for creating plots for 2-way interactions,https://sakaluk.wordpress.com/2015/08/27/6-make-it-pretty-plotting-2-way-interactions-with-ggplot2/,John K. Sakaluk,"Reading, R code","College / Upper Division (Undergraduates), Graduate / Professional","ggplot2, as I’ve already made clear, is one of my favourite packages for R. And since that original post about ggplot2 remains one of my most frequently visited, I thought I would proceed with starting a series of posts called “Make It Pretty”, all about sharing ways of visualizing data that I think are attractive/effective/comprehensive. So with this inaugural MIP post, I will be covering how to plot 2-way interactions using ggplot2. 2-way interactions can come in one of three general forms, and I will be providing code for plotting each. This will be a pretty lengthy post (lots of code/explanation), so if you’re only interested in learning how to plot a particular form, just click the the one below. Oh, and each uses an APA format theme that I’ve shared before, but just click here to quickly flip to the code if you need it.",English,I don't see any of these,"Student, Teacher","Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Blog,Tutorial"
2020-05-11T11:45:49.086Z,Replication in Psychology: a historical perspective,https://psyborgs.github.io/projects/replication-in-psychology/,Michael Pettit,Interactive,College / Upper Division (Undergraduates),"The reproducibility of psychological findings has generated much discussion of late. However, the question of replication is not a new one for psychologists. Psychologist have long debated how best to measure their phenomena, how to design their studies, how best to interpret their data, and whether their findings have a broader relevance.",English,I don't see any of these,Student,Social Science,,"Blog,Reproducibility Knowledge"
2020-05-11T11:49:03.272Z,The Hardest Science,https://hardsci.wordpress.com/,Sanjay Srivastava,Reading,College / Upper Division (Undergraduates),"Blogposts about psychology, reproducibility, replication etc.",English,I don't see any of these,"Student, Researcher","Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Blog,Reproducibility Knowledge"
2020-05-14T10:09:38.349Z,P-curve,http://willgervais.com/blog/2014/7/20/my-p-curve,Will Gervais,Reading,College / Upper Division (Undergraduates),An abstract about the p-curve,English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research",Blog
2020-05-14T10:11:39.371Z,Mission: P-curve,http://willgervais.com/blog/2015/3/27/mission-p-curve,Will Gervais,Reading,College / Upper Division (Undergraduates),A blog about the p-curve,English,I don't see any of these,"Student, Teacher",Social Science,"Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research",Blog
2020-05-14T10:14:00.086Z,"Bad Science log, post-Guardian",http://www.badscience.net/,Ben Goldacre,Reading,College / Upper Division (Undergraduates),Blogs about bad science,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research",Blog
2020-05-14T10:16:27.761Z,Bargain Basement Bayes,https://funderstorms.wordpress.com/2015/07/22/bargain-basement-bayes/,David Funder,Reading,College / Upper Division (Undergraduates),A blog about bayesian Statistics,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research",Blog
2020-05-14T10:17:42.725Z,Understanding Bayes: A Look at the Likelihood,http://alexanderetz.com/2015/04/15/understanding-bayes-a-look-at-the-likelihood/,Alexander Etz,Reading,College / Upper Division (Undergraduates),A blog about bayesian statistics,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Blog
2020-05-14T10:21:04.770Z,The Test of Insufficient Variance (TIVA): A New Tool for the Detection of Questionable Research Practices,https://replicationindex.wordpress.com/2014/12/30/the-test-of-insufficient-variance-tiva-a-new-tool-for-the-detection-of-questionable-research-practices/,Ulrich Schimmack ,"Reading, Student Guide",College / Upper Division (Undergraduates),"It has been known for decades that published results tend to be biased (Sterling, 1959). For most of the past decades this inconvenient truth has been ignored. In the past years, there have been many suggestions and initiatives to increase the replicability of reported scientific findings (Asendorpf et al., 2013). One approach is to examine published research results for evidence of questionable research practices (see Schimmack, 2014, for a discussion of existing tests). This blog post introduces a new test of bias in reported research findings, namely the Test of Insufficient Variance (TIVA).",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Blog
2020-05-14T10:22:54.428Z,Replacing p-values with Bayes-Factors: A Miracle Cure for the Replicability Crisis in Psychological Science,https://replicationindex.wordpress.com/2015/04/30/replacing-p-values-with-bayes-factors-a-miracle-cure-for-the-replicability-crisis-in-psychological-science/,Ulrich Schimmack ,"Reading, Student Guide",College / Upper Division (Undergraduates),A blog post about choosing Bayes Factor over p-value,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Blog
2020-05-14T10:25:50.166Z,The New Statistics: Effect Sizes and Confidence Intervals (Workshop Part 3),https://www.youtube.com/watch?v=1P2yV6joYlc,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about effect sizes and confidence intervals,English,I don't see any of these,"Student, Teacher","Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Video
2020-05-14T10:39:09.822Z,SPSP experts - open science,http://spsp.org/resources/multimedia/experts/openscience,Society for Social and Personality Psychology,"Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),"A video about open science, pre-registration etc.",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Video,Reproducibility Knowledge"
2020-05-14T11:00:11.164Z,Nick Brown talk: replicability and reproducibility,https://www.youtube.com/watch?v=tTuZ-IEc0Eg&feature=youtu.be&t=1h50m15s,BPSOfficial/Nick Brown,Video,College / Upper Division (Undergraduates),A video about replicability and reproducibility debate,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Video,Replication Research"
2020-05-14T11:09:05.222Z,Unlikely Results,https://www.youtube.com/watch?v=TosyACdsh-g,The Economist,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),Why most published scientific research is probably false,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Video
2020-05-14T11:13:50.654Z,Why an Entire Field of Psychology Is in Trouble,https://www.youtube.com/watch?v=2MDNvKXdLEM,SciShow/Michael Aranda,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about psychology being trouble,English,I don't see any of these,"Student, Teacher",Social Science,Reproducibility and Replicability Knowledge,Video
2020-05-14T11:21:34.371Z,Cognitive Science StackExchange site (for psychology Q&A),http://cogsci.stackexchange.com,Anon,"Student Guide, Teaching/Learning Strategy",College / Upper Division (Undergraduates),A website about questions on psychology,English,I don't see any of these,"Student, Teacher, Adminstrator, Parent, Librarian","Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research",Website
2020-05-14T11:23:51.707Z,"Resources (tutorials, papers, analysis scripts, utilities) for testing moderation and mediation",http://quantpsy.org/medn.htm,Kristopher J. Preacher,Lesson,College / Upper Division (Undergraduates),A website about moderation and mediation,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Website
2020-05-14T11:32:55.954Z,open stats lab,https://sites.trinity.edu/osl/,Kevin P. McIntyre,"Activity/Lab, Data Set",College / Upper Division (Undergraduates),A website about open statistic labs with data and activities,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Website
2020-05-14T11:38:14.786Z,CrossValidated StackExchange site (for statistics Q&A),http://stats.stackexchange.com,Anon,"Student Guide, Teaching/Learning Strategy, Unit of Study, Website",College / Upper Division (Undergraduates),A website about questions on statistics,English,I don't see any of these,"Student, Teacher, Librarian",Math & Statistics,"Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research",Website
2020-05-14T11:43:57.289Z,Psych 3400,https://crumplab.github.io/psyc3400/,Dr Matt Crump,"Data Set, Full Course, Lecture, Lecture Notes, Lesson, Lesson Plan, Module, Reading, Student Guide, Syllabus, Teaching/Learning Strategy, Tutorial,Textbook",College / Upper Division (Undergraduates),A statistics book and tutorial about statistics,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Statistical Book,Tutorial"
2020-05-14T11:55:45.849Z,Learn Statistics With R,https://learningstatisticswithr.com/,Danielle Navarro,"Textbook,  R Tutorial",College / Upper Division (Undergraduates),"Back in the grimdark pre-Snapchat era of humanity (i.e. early 2011), I started teaching an introductory statistics class for psychology students offered at the University of Adelaide, using the R statistical package as the primary tool. I wrote my own lecture notes for the class, which have now expanded to the point of effectively being a book. The book is freely available, and as of version 0.6 it is released under a creative commons licence (CC BY-SA 4.0)",English,CC BY-SA,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Statistical Book,Tutorial"
2020-05-14T11:59:39.296Z,Learn Stats with Jamovi,https://www.learnstatswithjamovi.com/,Danielle Navarro and David R Foxcraft ,"Data Set, Textbook, Tutorial",College / Upper Division (Undergraduates),A statistical tutorial about using Jamovi ,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,"Statistical Book,Tutorial"
2020-05-14T12:30:42.070Z,Ben Goldacre: Battling Bad Science,https://www.youtube.com/watch?v=h4MhbkWJzKk,TED/Ben Goldacre,Video,College / Upper Division (Undergraduates),A video about battling bad science,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Video
2020-05-14T19:07:09.627Z,"John Ioannidis: ""Reproducible Research: True or False?"" | Talks at Google",https://www.youtube.com/watch?v=GPYzY9I78CI,John Ioannidis,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video detailing whether reproducible research is true or false,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Video
2020-05-15T05:01:55.362Z,The Missing Semester of Your CS Education,https://missing.csail.mit.edu/,MIT,Full Course,College / Upper Division (Undergraduates),Course on computer sciences skills needed for all scientific research,English,CC BY-NC-SA,"Student, Teacher","Applied Science, Career and Technical Education, Life Science, Math & Statistics, Physical Science, Social Science",,Computer Sciences
2020-05-15T12:44:08.383Z,Last Week Tonight with John Oliver: Scientific Studies (HBO),https://www.youtube.com/watch?v=0Rnq1NpHdmw,Last Week Tonight/John Oliver,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about scientific studies,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research",Video
2020-05-15T13:01:06.716Z,The New Statistics: Meta-Analysis and Meta-Analytic Thinking (workshop Part 6),https://www.youtube.com/watch?v=2CBIQDoHCKU,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about meta analysis and meta-analytical thinking,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,Video
2020-05-15T13:07:53.560Z,Transparent and Open Social Science Research  course,https://www.bitss.org/mooc-parent-page/,Berkeley Initiative for Transparency in the Social Sciences,"Full Course, Lecture, Module, Video",College / Upper Division (Undergraduates),"Demand is growing for evidence-based policy making, but there is also growing recognition in the social science community that limited transparency and openness in research have contributed to widespread problems. With this course, you can explore the causes of limited transparency in social science research, as well as tools to make your own work more open and reproducible.",English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Course,Video,Reproducibility Knowledge"
2020-05-15T13:31:19.979Z,"The New Statistics: Confidence Intervals, NHST, and p Values (Workshop Part 1)",https://www.youtube.com/watch?v=iJ4kqk3V8jQ,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),"A video about Confidence Intervals, NHST, and p Values",English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,Video
2020-05-15T14:20:16.230Z,The New Statistics: Research Integrity & the New Statistics (Workshop Part 2),https://www.youtube.com/watch?v=wb0rnZBlcRg,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about Research Integrity & the New Statistics,English,I don't see any of these,"Student, Teacher","Applied Science, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,"Video,Reproducibility Knowledge"
2020-05-15T14:45:33.125Z,The New Statistics: The New Statistics in Action (Workshop Part 4),https://www.youtube.com/watch?v=QYNJMAqZvBQ,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about New statistics in action,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Video
2020-05-15T15:37:55.398Z,"The New Statistics: Planning, Power, and Precision (Workshop Part 5)",https://www.youtube.com/watch?v=Np4lxvS8C-E,Psychological Science/Geoff Cumming,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about power analysis and precision,English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics",Conceptual and Statistical Knowledge,Video
2020-05-15T15:42:02.427Z,Statistical Rethinking Winter 2015,https://www.youtube.com/playlist?list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z,Richard McElreath,"Lecture, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about Statistical Rethinking: A Bayesian Course with R ,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Video,Bayesian"
2020-05-15T15:44:46.399Z,Bradley Efron: Frequentist accuracy of Bayesian estimates,https://www.youtube.com/watch?v=2oKw5HHAWs4,RoyalStatSoc/Bradley Efron,"Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),A video about Frequentist accuracy of Bayesian estimates,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,"Video,Bayesian"
2020-05-15T17:50:57.842Z,Philosophical Psychology 1989,http://meehl.umn.edu/video,P. E. Meehl,"Primary Source, Student Guide, Teaching/Learning Strategy, Video",College / Upper Division (Undergraduates),"P. E. Meehl did first 10 sessions (Winter Quarter, Jan–Mar 1989). In the Spring Quarter, several other department members lectured on various topics. Then PEM did last two sessions (5/25/89 and 6/1/89).",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Course,Video"
2020-05-16T06:36:40.345Z,Faking Science: A True Story of Academic Fraud,https://errorstatistics.files.wordpress.com/2014/12/fakingscience-20141214.pdf,Diederik Stapel/Translated by Nicholas J. L. Brown,Primary Source,College / Upper Division (Undergraduates),A book about Academic Fraud,English,CC BY-NC-ND,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Textbook,Reproducibility Crisis and Credibility Revolution"
2020-05-16T07:21:04.327Z,Science and Pseudoscience BBC Radio Talk,http://www.lse.ac.uk/philosophy/department-history/science-and-pseudoscience-overview-and-transcript/,London School of Economics and Political Science,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),A podcast about science and pseudoscience,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T07:32:40.694Z,Stereotype threat,https://www.wnycstudios.org/story/stereothreat/,"Simon Adler, Amanda Aronczyk and Dan Engber","Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),A podcast about stereotype threat and replication,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Knowledge,Reproducibility Crisis and Credibility Revolution"
2020-05-16T10:33:44.148Z,The Hidden Brain,https://www.npr.org/2016/05/24/477921050/when-great-minds-think-unlike-inside-sciences-replication-crisis,Shankar Vedantam and Maggie Penman,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),A podcast about replication crisis,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Replication Research,"Podcast,Reproducibility Knowledge,Reproducibility Crisis and Credibility Revolution"
2020-05-16T10:35:36.465Z,Neuroscientist Explains,https://www.theguardian.com/science/audio/2018/mar/19/a-neuroscientist-explains-psychologys-replication-crisis-podcast,Daniel Glaser,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),Daniel Glaser apprehensively revisits an article of his that saw some fallout due to a study he cited. But that study was not the only one involved in what is now being called a crisis for psychology and further afield,English,I don't see any of these,Student,"Applied Science, Social Science",Replication Research,"Podcast,Reproducibility Knowledge,Reproducibility Crisis and Credibility Revolution"
2020-05-16T10:37:17.656Z,Detecting fraud in social science,http://rationallyspeakingpodcast.org/show/rs-155-uri-simonsohn-on-detecting-fraud-in-social-science.html,Julia Galef/Uri Simonsohn,"Teaching/Learning Strategy, Unit of Study, Podcast",College / Upper Division (Undergraduates),"He's been called a ""Data vigilante."" In this episode, Prof. Uri Simonsohn describes how he detects fraudulent work in psychology and economics -- what clues tip him off? How big of a problem is fraud relative to other issues like P-hacking? And what solutions are there?",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science",Replication Research,"Podcast,Reproducibility Knowledge,Reproducibility Crisis and Credibility Revolution"
2020-05-16T10:43:38.483Z,The Experiment Experiment,http://www.npr.org/sections/money/2016/01/15/463237871/episode-677-the-experiment-experiment,Planet Money,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),"A few years back, a famous psychologist published a series of studies that found people could predict the future — not all the time, but more often than if they were guessing by chance alone.The paper left psychologists with two options. ""Either we have to conclude that ESP is true,"" says Brian Nosek, a psychologist at the University of Virginia, ""or we have to change our beliefs about the right ways to do science."" Nosek is going with Option B — and not just for psychology experiments. He thinks there's something wrong with the way we're doing science. And he launched a massive project to try to fix it.",English,I don't see any of these,Student,"Applied Science, Social Science",Replication Research,"Podcast,Reproducibility Knowledge,Reproducibility Crisis and Credibility Revolution"
2020-05-16T11:07:57.302Z,Everything Hertz,https://soundcloud.com/everything-hertz,Sound Cloud,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),A podcast about open science and psychology,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T11:19:55.592Z,You are not so smart,https://youarenotsosmart.com/2017/07/19/yanss-100-the-replication-crisis/,David McRaney,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),"Psychology is working on the hardest problems in all of science. Physics, astronomy, geology — those are easy, by comparison. Understanding consciousness, willpower, ideology, social change – there’s a larger-than-Large-Hadron-Collider level of difficulty to each one of these, but since these are more relatable ideas than quarks and bosons and mass coronal ejections — this a science about our minds and selves — it’s easier to create eye-catching headlines and, well, to make podcasts about them. This is the problem. Because the system for distributing the findings of science is based on publication within journals, which themselves are often depend on the interest of the general media, all the biases that come with that system and media consumption in general are now causing the sciences that are most interesting to the public to get tainted by that interest. As you will hear in this episode, one of the most famous and most talked-about phenomena in recent psychological history, ego depletion, hasn’t been doing so well in replication attempts. In the show, journalist Daniel Engber who wrote an article for Slate about the failure to replicate many of the famous ego depletion experiments will detail what this means for the science and the scientists involved. Also, you’ll hear from psychologist Brain Nosek, who says, “Science is wrong about everything, but you can trust it more than anything.” Nosek is director of the Center for Open Science, an organization working to correct what they see as the temporarily wayward path of psychology. Nosek recently lead a project in which 270 scientists sought to replicate 100 different studies in psychology, all published in 2008 — 97 of which claimed to have found significant results — and in the end, two-thirds failed to replicate. Clearly, some sort of course correction is in order. There is now a massive effort underway sort out what is being called the replication crisis. Much of the most headline-producing research in the last 20 years isn’t standing up to attempts to reproduce its findings. Nosek wants to clean up the processes that have lead to this situation, and in this episode, you’ll learn how he and others plan to do so.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T11:34:46.639Z,Open science: many hands make light work,https://www.jisc.ac.uk/podcasts/open-science-many-hands-make-light-work-30-oct-2015,Jisc,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),Open science has been highlighted as one of the priorities of the Dutch presidency of the European Union in 2016. Matthew Dovey discusses the motivations behind the open science movement and why initiatives to support it are more important than ever. Read his original blog.,English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T12:04:50.998Z,Circle of Willis (episode with Sinime Vazire),http://circleofwillispodcast.com/episode-8-simine-vazire,Jim Coan,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),"Welcome to Episode 8, where I talk to SIMINE VAZIRE, Associate Professor of Psychology at the University of California at Davis, about the stability of personality, our ability to know ourselves, and some of the nuances within the prescriptive advice of the Open Science Movement.  Simine wears a number of different hats. In recent years, she’s been at or near the center of ongoing conversations among scientists about the virtues and challenges of open science. As part of this work, she co-founded the Society for the Improvement of Psychological Science (SIPS) and co-hosts a science podcast (with Sanjay Srivastava and Alexa Tullett) called THE BLACK GOAT. Simine is also editor in chief of the journal Social Psychological and Personality Science and a senior editor at Collabra. Interestingly, Simine has also been a part of the conversation about the process of criticism in science. As most listeners well know, criticism is unquestionably essential if science is going to be self-correcting (which is of course the whole point!). One question the field has been grappling with is the point at which criticism crosses over into harassment and bullying—a question at the heart of a recent op-ed Simine wrote for Slate. I have my own thoughts on this question, which I’ll save for another time, but one of the reasons I was so keen to ask Simine to be on Circle of Willis is that I find her approach to grappling with such questions to be equal parts humble, charitable, and firm. She isn’t likely to allow a legitimate criticism to be brushed aside in order to avoid hurting someone's feelings, but neither is she going to participate in (or for that matter tolerate) bullying. I think that in our age of shoot-from-the-hip outrage, that can be a hard path to find, let alone walk, and I genuinely admire her efforts. There are many other things I love about Simine, but as you’ll hear in this episode, at or near the top of the list of her agreeable traits is that she’ll be the first to tell any of you that sometimes she’s wrong. We try to be right while tolerating (and admitting to) our mistakes.  Oh, and — seriously — keep a notepad handy for this episode. Simine is unusually quotable!",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T12:08:21.186Z,P-Hacking and Other Problems in Psychology Research,http://rationallyspeakingpodcast.org/show/rs123-daniel-lakens-on-p-hacking-and-other-problems-in-psych.html,"Julia Galef, Massimo Pigliucci and Benny Pollak","Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),"What's wrong with the social sciences? In this episode, Massimo and Julia are joined by Professor Daniel Lakens from the Eindhoven University of Technology, who studies psychology and blogs about research methods and open science. The three discuss why so many psychology papers can't be trusted, and what solutions might exist for the problem (including how to fix the skewed incentives in the field).",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T12:11:05.263Z,ReproducibiliTea,https://soundcloud.com/reproducibilitea,"Sophia Cruwell, Amy Orben and Sam Parsons","Student Guide, Unit of Study","College / Upper Division (Undergraduates), Graduate / Professional","Serving mugs of Reproducibili☕️: Blends include transparency, openness and robustness + a spoonful of science.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-16T12:47:35.074Z,The Black Goat,http://www.theblackgoatpodcast.com/,"Sanjay Srivastava, Alexa Tullett, and Simine Vazire","Student Guide, Unit of Study, Podcast",College / Upper Division (Undergraduates),"Three psychologists talk about doing science. Hosted by Sanjay Srivastava, Alexa Tullett, and Simine Vazire.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Podcast,Reproducibility Crisis and Credibility Revolution"
2020-05-16T12:49:35.192Z,The Bayes Factor,https://sites.tufts.edu/hilab/podcast/,Prof. JP De Ruiter,"Student Guide, Teaching/Learning Strategy, Podcast",College / Upper Division (Undergraduates),"In this episode JP and Alex interview Zoltan Dienes. They discuss Zoltan's passion for the martial arts, why Bayesian inference could be more Popperian than you might think,  and the easiest way to start using Bayesian statistics in practice.",English,I don't see any of these,"Student, Teacher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Podcast
2020-05-19T16:34:08.333Z,Understanding psychology as a science: An introduction to scientific and statistical inference,https://he.palgrave.com/page/detail/?sf1=barcode&st1=9780230542303,Zoltan Dienes,Textbook,Graduate / Professional,"How can we objectively define categories of truth in scientific thinking? How can we reliably measure the results of research? In this ground-breaking text, Dienes undertakes a comprehensive historical analysis of the dominant schools of thought, key theories and influential thinkers that have progressed the foundational principles and characteristics that typify scientific research methodology today. This book delivers a masterfully simple, ‘though not simplistic’, introduction to the core arguments surrounding Popper, Kuhn and Lakatos, Fisher and Royall, Neyman and Pearson and Bayes. Subsequently, this book clarifies the prevalent misconceptions that surround such theoretical perspectives in psychology today, providing an especially accessible critique for student readers.  ",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:38:10.820Z,Statistical Evidence: A Likelihood Paradigm,https://www.routledge.com/Statistical-Evidence-A-Likelihood-Paradigm/Royall/p/book/9780412044113?utm_source=crcpress.com&utm_medium=referral,Richard Royall,Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","Interpreting statistical data as evidence, Statistical Evidence: A Likelihood Paradigm focuses on the law of likelihood, fundamental to solving many of the problems associated with interpreting data in this way. Statistics has long neglected this principle, resulting in a seriously defective methodology. This book redresses the balance, explaining why science has clung to a defective methodology despite its well-known defects. After examining the strengths and weaknesses of the work of Neyman and Pearson and the Fisher paradigm, the author proposes an alternative paradigm which provides, in the law of likelihood, the explicit concept of evidence missing from the other paradigms. At the same time, this new paradigm retains the elements of objective measurement and control of the frequency of misleading results, features which made the old paradigms so important to science. The likelihood paradigm leads to statistical methods that have a compelling rationale and an elegant simplicity, no longer forcing the reader to choose between frequentist and Bayesian statistics. ",English,I don't see any of these,"Student, Teacher, Researcher",Math & Statistics,Conceptual and Statistical Knowledge,Book
2020-05-19T16:41:14.955Z,Dyadic Data Analysis (Methodology in the Social Sciences),https://www.amazon.com/Dyadic-Analysis-Methodology-Social-Sciences/dp/1572309865?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o09_s00,"David A. Kenny, Deborah A. Kashy, William L. Cook",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional","Interpersonal phenomena such as attachment, conflict, person perception, helping, and influence have traditionally been studied by examining individuals in isolation, which falls short of capturing their truly interpersonal nature. This book offers state-of-the-art solutions to this age-old problem by presenting methodological and data-analytic approaches useful in investigating processes that take place among dyads: couples, coworkers, or parent-child, teacher-student, or doctor-patient pairs, to name just a few. Rich examples from psychology and across the behavioral and social sciences help build the researcher's ability to conceptualize relationship processes; model and test for actor effects, partner effects, and relationship effects; and model the statistical interdependence that can exist between partners. The companion website provides clarifications, elaborations, corrections, and data and files for each chapter.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:43:29.962Z,Exploratory Factor Analysis,https://www.amazon.com/Exploratory-Factor-Analysis-Understanding-Statistics/dp/0199734178?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o00_s00,"Leandre R. Fabrigar, Dueane T. Wegener",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","This book provides a non-mathematical introduction to the underlying theory of Efa and reviews the key decisions that must be made in its implementation. Among the issues discussed are the use of confirmatory versus exploratory factor analysis, the use of principal components analysis versus common factor analysis, procedures for determining the appropriate number of factors, and methods for rotating factor solutions. Explanations and illustrations of the application of different factor analytic procedures are provided for analyses using common statistical packages (Spss and Sas), as well as a free package available on the web (Comprehensive Exploratory Factor Analysis). In addition, practical instructions are provided for conducting a number of useful factor analytic procedures not included in the statistical packages.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:46:40.198Z,"Introduction to Mediation, Moderation, and Conditional Process Analysis, First Edition: A Regression-Based Approach (Methodology in the Social Sciences)",https://www.amazon.com/Introduction-Mediation-Moderation-Conditional-Analysis/dp/1609182308?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o03_s00,Andrew F. Hayes,Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","Explaining the fundamentals of mediation and moderation analysis, this engaging book also shows how to integrate the two using an innovative strategy known as conditional process analysis. Procedures are described for testing hypotheses about the mechanisms by which causal effects operate, the conditions under which they occur, and the moderation of mechanisms. Relying on the principles of ordinary least squares regression, Andrew Hayes carefully explains the estimation and interpretation of direct and indirect effects, probing and visualization of interactions, and testing of questions about moderated mediation. Examples using data from published studies illustrate how to conduct and report the analyses described in the book. Of special value, the book introduces and documents PROCESS, a macro for SPSS and SAS that does all the computations described in the book. The companion website (www.afhayes.com) offers free downloads of PROCESS plus data files for the book's examples.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:48:12.793Z,Introduction to Meta-Analysis,https://www.amazon.com/Introduction-Meta-Analysis-Michael-Borenstein/dp/0470057246?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o02_s00,"Michael Borenstein, Larry V. Hedges, Julian P.T. Higgins, Hannah R. Rothstein",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","This book provides a clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies. Meta-analysis has become a critically important tool in fields as diverse as medicine, pharmacology, epidemiology, education, psychology, business, and ecology.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Physical Science, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:49:10.473Z,Longitudinal Structural Equation Modeling (Methodology in the Social Sciences),https://www.amazon.com/Longitudinal-Structural-Equation-Modeling-Methodology/dp/1462510167?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o03_s00,Todd D. Little,Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","Featuring actual datasets as illustrative examples, this book reveals numerous ways to apply structural equation modeling (SEM) to any repeated-measures study. Initial chapters lay the groundwork for modeling a longitudinal change process, from measurement, design, and specification issues to model evaluation and interpretation. Covering both big-picture ideas and technical ""how-to-do-it"" details, the author deftly walks through when and how to use longitudinal confirmatory factor analysis, longitudinal panel models (including the multiple-group case), multilevel models, growth curve models, and complex factor models, as well as models for mediation and moderation. User-friendly features include equation boxes that clearly explain the elements in every equation, end-of-chapter glossaries, and annotated suggestions for further reading. The companion website (www.guilford.com/little-materials) provides datasets for all of the examples--which include studies of bullying, adolescent students' emotions, and healthy aging--with syntax and output from LISREL, Mplus, and R (lavaan).",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:50:30.775Z,Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling,https://www.amazon.com/Multilevel-Analysis-Introduction-Advanced-Modeling/dp/0761958908?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o07_s00,"Tom A.B. Snijders, Roel Bosker",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","The main methods, techniques and issues for carrying out multilevel modeling and analysis are covered in this book. The book is an applied introduction to the topic, providing a clear conceptual understanding of the issues involved in multilevel analysis and will be a useful reference tool. Information on designing multilevel studies, sampling, testing and model specification and interpretation of models is provided. A comprehensive guide to the software available is included. Multilevel Analysis is the ideal guide for researchers and applied statisticians in the social sciences, including education, but will also interest researchers in economics, and biological, medical and health disciplines.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:51:55.275Z,Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences,https://www.amazon.com/Multiple-Regression-Correlation-Analysis-Behavioral/dp/0805822232?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o03_s00,"Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional","This classic text on multiple regression is noted for its nonmathematical, applied, and data-analytic approach. Readers profit from its verbal-conceptual exposition and frequent use of examples.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:54:01.549Z,R Graphics Cookbook: Practical Recipes for Visualizing Data ,https://www.amazon.com/R-Graphics-Cookbook-Winston-Chang/dp/1449316956?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o03_s00,Winston Chang,Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","This practical guide provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project, and includes a discussion of how and why the recipe works.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Physical Science, Social Science",,"Book,Software,R"
2020-05-19T16:56:17.036Z,Theory Construction and Model-Building Skills: A Practical Guide for Social Scientists,https://www.amazon.com/Theory-Construction-Model-Building-Skills-Methodology/dp/1606233394?ie=UTF8&psc=1&redirect=true&ref_=oh_aui_detailpage_o03_s00,"James Jaccard, Jacob Jacoby",Textbook,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","Meeting a crucial need for graduate students and newly minted researchers, this innovative text provides hands-on tools for generating ideas and translating them into formal theories. It is illustrated with numerous practical examples drawn from multiple social science disciplines and research settings. The authors offer clear guidance for defining constructs, thinking through relationships and processes that link constructs, and deriving new theoretical models (or building on existing ones) based on those relationships. Step by step, they show readers how to use causal analysis, mathematical modeling, simulations, and grounded and emergent approaches to theory construction. A chapter on writing about theories contains invaluable advice on crafting effective papers and grant applications. ",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Physical Science, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T16:57:41.910Z,Latent Variable Modeling with R,https://www.amazon.ca/gp/product/0415832446/ref=oh_aui_detailpage_o01_s00?ie=UTF8&psc=1,"W. Holmes Finch, Brian F. French",Textbook,"Graduate / Professional, Career /Technical","This book demonstrates how to conduct latent variable modeling (LVM) in R by highlighting the features of each model, their specialized uses, examples, sample code and output, and an interpretation of the results. Each chapter features a detailed example including the analysis of the data using R, the relevant theory, the assumptions underlying the model, and other statistical details to help readers better understand the models and interpret the results. Every R command necessary for conducting the analyses is described along with the resulting output which provides readers with a template to follow when they apply the methods to their own data. The basic information pertinent to each model, the newest developments in these areas, and the relevant R code to use them are reviewed. Each chapter also features an introduction, summary, and suggested readings. A glossary of the text’s boldfaced key terms and key R commands serve as helpful resources. The book is accompanied by a website with exercises, an answer key, and the in-text example data sets.",English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Life Science, Math & Statistics, Physical Science, Social Science",Conceptual and Statistical Knowledge,Book
2020-05-19T17:01:58.699Z,Improving your statistical inferences,https://www.coursera.org/learn/statistical-inferences,Daniel Lakens,Full Course,"College / Upper Division (Undergraduates), Graduate / Professional, Career /Technical","This course aims to help you to draw better statistical inferences from empirical research. First, we will discuss how to correctly interpret p-values, effect sizes, confidence intervals, Bayes Factors, and likelihood ratios, and how these statistics answer different questions you might be interested in. Then, you will learn how to design experiments where the false positive rate is controlled, and how to decide upon the sample size for your study, for example in order to achieve high statistical power. Subsequently, you will learn how to interpret evidence in the scientific literature given widespread publication bias, for example by learning about p-curve analysis. Finally, we will talk about how to do philosophy of science, theory construction, and cumulative science, including how to perform replication studies, why and how to pre-register your experiment, and how to share your results following Open Science principles. ",English,I don't see any of these,Student,"Applied Science, Life Science, Math & Statistics, Social Science",,Course
2020-05-22T18:33:13.945Z,Giving Community Psychology Away: A case for open access publishing,https://www.gjcpp.org/en/article.php?issue=33&article=199,"Crystal N. Steltenpohl, Amy J. Anderson, and Katherine M. Daniels","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Amidst increased pressure for transparency in science, researchers and community members are calling for open access to study stimuli and measures, data, and results. These arguments coincidentally align with calls within community psychology to find innovative ways to support communities and increase the prominence of our field. This paper aims to (1) define the current context for community psychologists in open access publishing, (2) illustrate the alignment between open access publishing and community psychology principles, and (3) demonstrate how to engage in open access publishing using community psychology values. Currently, there are several facilitators (e.g. an increasing number of open access journals, the proliferation of blogs, and social media) and barriers (e.g. Article Processing Charges (APCs), predatory journals) to publishing in open access venues. Openly sharing our research findings aligns with our values of (1) citizen participation, (2) social justice, and (3) collaboration and community strengths. Community psychologists desiring to engage in open access publishing can ask journals to waive APCs, publish pre-prints, use blogs and social media to share results, and push for systemic change in a publishing system that disenfranchises researchers, students, and community members.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science",Open Data and Materials,
2020-05-22T18:35:28.851Z,Which is the correct statistical test to use?,http://oralpathol.dlearn.kmu.edu.tw/case/Journal%20reading-intern-08-12/statistical%20use-review-BJOMFS-2008.pdf,Evie McCrum-Gardner,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This paper explains how to select the correct statistical test for a research project, clinical trial, or other investigation. The first step is to decide in what scale of measurement your data are as this will affect your decision—nominal, ordinal, or interval. The next stage is to consider the purpose of the analysis—for example, are you comparing independent or paired groups? Several statistical tests are discussed with an explanation of when it is appropriate to use each one; relevant examples of each are provided. If an incorrect test is used, then invalid results and misleading conclusions may be drawn from the study",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-22T18:38:12.752Z,How (and Whether) to Teach Undergraduates About the Replication Crisis in Psychological Science,https://journals.sagepub.com/doi/abs/10.1177/0098628318762900,"William J. Chopik, Ryan H. Bremner, Andrew M. Defever, Victor N. Keller","Primary Source, Reading, Paper",Graduate / Professional,"Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.",English,I don't see any of these,Teacher,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research",Reproducibility Crisis and Credibility Revolution
2020-05-22T18:40:59.988Z,The preregistration revolution,https://www.pnas.org/content/115/11/2600,"Brian A. Nosek, Charles R. Ebersole , Alexander C. DeHaven , and David T. Mellor","Primary Source, Reading, Paper","College / Upper Division (Undergraduates), Graduate / Professional","Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.",English,I don't see any of these,Student,"Applied Science, Social Science",Preregistration,
2020-05-22T18:43:10.415Z,Teaching replication,https://journals.sagepub.com/doi/10.1177/1745691612460686,Michael C. Frank and Rebecca Saxe,"Primary Source, Reading, Paper",Graduate / Professional,"Replication is held as the gold standard for ensuring the reliability of published scientific literature. But conducting direct replications is expensive, time-consuming, and unrewarded under current publication practices. So who will do them? Our answer is that students in laboratory classes should replicate recent findings as part of their training in experimental methods. In our own courses, we have found that replicating cutting-edge results is exciting and fun, it gives students the opportunity to make real scientific contributions (provided supervision is appropriate), and it provides object lessons about the scientific process, the importance of reporting standards, and the value of openness.",English,I don't see any of these,Teacher,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research",Reproducibility Crisis and Credibility Revolution
2020-05-22T18:55:08.142Z,Bayes Factor,https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/j.1740-9713.2006.00204.x,O'Hagan,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Bayes factors are somewhat essential to Bayesian statistics. Tony O’Hagan explains their basics,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-22T19:01:39.952Z,An exploratory test for an excess of significant findings,https://journals.sagepub.com/doi/abs/10.1177/1740774507079441,John PA Ioannidis and Thomas A Trikalinos,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background The published clinical research literature may be distorted by the pursuit of statistically significant results. Purpose: We aimed to develop a test to explore biases stemming from the pursuit of nominal statistical significance.  Methods The exploratory test evaluates whether there is a relative excess of formally significant findings in the published literature due to any reason (e.g., publication bias, selective analyses and outcome reporting, or fabricated data). The number of expected studies with statistically significant results is estimated and compared against the number of observed significant studies. The main application uses alpha = 0.05, but a range of alpha thresholds is also examined. Different values or prior distributions of the effect size are assumed. Given the typically low power (few studies per research question), the test may be best applied across domains of many meta-analyses that share common characteristics (interventions, outcomes, study populations, research environment). Results:  We evaluated illustratively eight meta-analyses of clinical trials with 50 studies each and 10 meta-analyses of clinical efficacy for neuroleptic agents in schizophrenia; the 10 meta-analyses were also examined as a composite domain. Different results were obtained against commonly used tests of publication bias. We demonstrated a clear or possible excess of significant studies in 6 of 8 large meta analyses and in the wide domain of neuroleptic treatments. Limitations The proposed test is exploratory, may depend on prior assumptions, and should be applied cautiously. Conclusions An excess of significant findings may be documented in some clinical research fields",English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-22T19:04:53.880Z,"Cohen, J. (1992). Statistical power analysis. Current Directions in Psychological Science, 1(3), 98–101.",https://journals.sagepub.com/doi/abs/10.1111/1467-8721.ep10768783,Jacob Cohen,"Primary Source, Reading",College / Upper Division (Undergraduates),A paper about statistical power,English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-22T19:06:59.335Z,Power failure: why small sample size undermines the reliability of neuroscience,https://doi.org/10.1038/nrn3475,Katherine Button et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:09:22.607Z,Questionable Research Practices Revisited,https://doi.org/10.1177/1948550615612150,Klaus Fiedler and Norbert Schwarz,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:13:41.207Z,Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling,https://doi.org/10.1177/0956797611430953,"Leslie K. John, George Loewenstein, Drazen Prelec","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:16:27.793Z,Psychological testing and psychological assessment: A review of evidence and issues.,https://doi.org/10.1037/0003-066X.56.2.128,Meyer et al.,"Primary Source, Reading",College / Upper Division (Undergraduates),"This article summarizes evidence and issues associated with psychological assessment. Data from more than 125 meta-analyses on test validity and 800 samples examining multimethod assessment suggest 4 general conclusions: (a) Psychological test validity is strong and compelling, (b) psychological test validity is comparable to medical test validity, (c) distinct assessment methods provide unique sources of information, and (d) clinicians who rely exclusively on interviews are prone to incomplete understandings. Following principles for optimal nomothetic research, the authors suggest that a multimethod assessment battery provides a structured means for skilled clinicians to maximize the validity of individualized assessments. Future investigations should move beyond an examination of test scales to focus more on the role of psychologists who use tests as helpful tools to furnish patients and referral sources with professional consultatio",English,I don't see any of these,Student,Applied Science,Conceptual and Statistical Knowledge,
2020-05-22T19:20:06.810Z,Curating Research Assets: A Tutorial on the Git Version Control System,https://doi.org/10.1177/2515245918754826,Matti Vuorre and James P. Curley,"Primary Source, Reading",College / Upper Division (Undergraduates),"Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.",English,I don't see any of these,Student,"Applied Science, Social Science",Open Data and Materials,
2020-05-22T19:22:28.222Z,Comparing Published Scientific Journal Articlesto Their Pre-print VersionsMart,https://arxiv.org/abs/1604.05363,"Martin Klein, Peter Broadwell, Sharon E. Farb, Todd Grappone","Primary Source, Reading",College / Upper Division (Undergraduates),"Academic publishers claim that they add value to scholarly communications by coordinating reviews and contributing and enhancing text during publication. These contributions come at a considerable cost: U.S. academic libraries paid $1.7 billion for serial subscriptions in 2008 alone. Library budgets, in contrast, are flat and not able to keep pace with serial price inflation. We have investigated the publishers' value proposition by conducting a comparative study of pre-print papers and their final published counterparts. This comparison had two working assumptions: 1) if the publishers' argument is valid, the text of a pre-print paper should vary measurably from its corresponding final published version, and 2) by applying standard similarity measures, we should be able to detect and quantify such differences. Our analysis revealed that the text contents of the scientific papers generally changed very little from their pre-print to final published versions. These findings contribute empirical indicators to discussions of the added value of commercial publishers and therefore should influence libraries' economic decisions regarding access to scholarly publications.",English,I don't see any of these,Student,"Applied Science, Social Science",Replication Research,Reproducibility Knowledge
2020-05-22T19:24:27.127Z,"The Persistence of Underpowered Studies in Psychological Research: Causes, Consequences, and Remedies",https://doi.org/10.1037/1082-989X.9.2.147,S.E. Maxwell,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The ""curse of multiplicities"" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:25:32.671Z,The earth is round (p < .05).,https://doi.org/10.1037/0003-066X.49.12.997,Jacob Cohen,"Primary Source, Reading",College / Upper Division (Undergraduates),"After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.",English,I don't see any of these,"Student, Teacher",Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-22T19:27:41.535Z,Intro to the special issue,https://onlinelibrary.wiley.com/doi/pdf/10.1080/03640210802473582,"Kevin Glucka, Paul Bellob, Jerome Busemeyer","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about a special issue on cognitive modelling,English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,
2020-05-22T19:29:31.744Z,Facts Are More Important Than Novelty: Replication in the Education Sciences,https://doi.org/10.3102/0013189X14545513,Matthew C. Makel and Jonathan A. Plucker,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Despite increased attention to methodological rigor in education research, the field has focused heavily on experimental design and not on the merit of replicating important results. The present study analyzed the complete publication history of the current top 100 education journals ranked by 5-year impact factor and found that only 0.13% of education articles were replications. Contrary to previous findings in medicine, but similar to psychology, the majority of education replications successfully replicated the original studies. However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. The results emphasize the importance of third-party, direct replications in helping education research improve its ability to shape education policy and practice.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:31:28.489Z,The Empirical March: Making Science Better at Self-Correction,https://doi.org/10.1037/a0035803,Matthew Makel,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychology has been criticized recently for a range of research quality issues. The current article organizes these problems around the actions of the individual researcher and the existing norms of the field. Proposed solutions align the incentives of all those involved in the research process. I recommend moving away from a focus on statistical significance to one of statistical power, renewing an emphasis on prediction and the pre-registration of hypotheses, changing the timing and method of peer-review, and increasing the rate at which replications are conducted and published. These strategies seek to unify incentives toward increased methodological and statistical rigor to more effectively and efficiently reduce bias and error.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-22T19:33:08.534Z,A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science’s Aversion to the Null,https://doi.org/10.1177/1745691612459059,Christopher J. Ferguson and Moritz Heene,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.",English,I don't see any of these,"Student, Teacher","Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T02:19:21.665Z,"Ioannidis, J. P. A. (2012). Why science is not necessarily self-correcting. Perspectives on Psychological Science, 7, 645-654.",https://doi.org/10.1177/1745691612464056,John Ioannidis,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that “it is obvious that progress is made” is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T02:54:57.533Z,The (mis)reporting of statistical results in psychology journals,https://doi.org/10.3758/s13428-011-0089-5,Marjan Bakker & Jelte M. Wicherts,"Primary Source, Reading",College / Upper Division (Undergraduates)," In order to study the prevalence, nature (direction), and causes of reporting errors in psychology, we checked the consistency of reported test statistics, degrees of freedom, and p values in a random sample of high- and low-impact psychology journals. In a second study, we established the generality of reporting errors in a random sample of recent psychological articles. Our results, on the basis of 281 articles, indicate that around 18% of statistical results in the psychological literature are incorrectly reported. Inconsistencies were more common in low-impact journals than in high impact journals. Moreover, around 15% of the articles contained at least one statistical conclusion that proved, upon recalculation, to be incorrect; that is, recalculation rendered the previously significant result insignificant, or vice versa. These errors were often in line with researchers’ expectations. We classified the most common errors and contacted authors to shed light on the origins of the errors.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-23T02:57:51.044Z,Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results,https://doi.org/10.1371/journal.pone.0026828,"Jelte M. Wicherts, Marjan Bakker, Dylan Molenaar","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background: The widespread reluctance to share published research data is often hypothesized to be due to the authors’ fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings: We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions: Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science",Open Data and Materials,
2020-05-23T02:59:43.507Z,Statistical Reporting Errors and Collaboration on Statistical Analyses in Psychological Science,https://doi.org/10.1371/journal.pone.0114876,"Coosje L. S. Veldkamp, 1 , * Michèle B. Nuijten,  Linda Dominguez-Alvarez,  Marcel A. L. M. van Assen, and Jelte M. Wicherts","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistical analysis is error prone. A best practice for researchers using statistics would therefore be to share data among co-authors, allowing double-checking of executed tasks just as co-pilots do in aviation. To document the extent to which this ‘co-piloting’ currently occurs in psychology, we surveyed the authors of 697 articles published in six top psychology journals and asked them whether they had collaborated on four aspects of analyzing data and reporting results, and whether the described data had been shared between the authors. We acquired responses for 49.6% of the articles and found that co-piloting on statistical analysis and reporting results is quite uncommon among psychologists, while data sharing among co-authors seems reasonably but not completely standard. We then used an automated procedure to study the prevalence of statistical reporting errors in the articles in our sample and examined the relationship between reporting errors and co-piloting. Overall, 63% of the articles contained at least one p-value that was inconsistent with the reported test statistic and the accompanying degrees of freedom, and 20% of the articles contained at least one p-value that was inconsistent to such a degree that it may have affected decisions about statistical significance. Overall, the probability that a given p-value was inconsistent was over 10%. Co-piloting was not found to be associated with reporting errors.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Conceptual and Statistical Knowledge,Open Data and Materials",
2020-05-23T03:06:04.290Z,"The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time",https://doi.org/10.1511/2014.111.460,Andrew Gelman and Eric Loken,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Data-dependent analysis—a “garden of forking paths”— explains why many statistically significant comparisons don't hold up. ,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-23T03:12:32.579Z,Response to Comment on “Estimating the reproducibility of psychological science”,https://doi.org/10.1126/science.aad9163,Christopher J. Anderson et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T03:14:55.722Z,Research Practices That Can Prevent an Inflation of False-Positive Rates,https://doi.org/10.1177/1088868313496330,"Kou Murayama, Reinhard Pekrun and Klaus Fiedler","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent studies have indicated that research practices in psychology may be susceptible to factors that increase false-positive rates, raising concerns about the possible prevalence of false-positive findings. The present article discusses several practices that may run counter to the inflation of false-positive rates. Taking these practices into account would lead to a more balanced view on the false-positive issue. Specifically, we argue that an inflation of false-positive rates would diminish, sometimes to a substantial degree, when researchers (a) have explicit a priori theoretical hypotheses, (b) include multiple replication studies in a single paper, and (c) collect additional data based on observed results. We report findings from simulation studies and statistical evidence that support these arguments. Being aware of these preventive factors allows researchers not to overestimate the pervasiveness of false-positives in psychology and to gauge the susceptibility of a paper to possible false positives in practical and fair ways.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T03:17:01.140Z,egative results are disappearing from most disciplines and countries,https://doi.org/10.1007/s11192-011-0494-7,Daniele Fanelli,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T03:19:27.205Z,"Randomization Does Not Help Much, Comparability Does",https://doi.org/10.1371/journal.pone.0132102,Uwe Saint-Mont,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"According to R.A. Fisher, randomization “relieves the experimenter from the anxiety of considering innumerable causes by which the data may be disturbed.” Since, in particular, it is said to control for known and unknown nuisance factors that may considerably challenge the validity of a result, it has become very popular. This contribution challenges the received view. First, looking for quantitative support, we study a number of straightforward, mathematically simple models. They all demonstrate that the optimism surrounding randomization is questionable: In small to medium-sized samples, random allocation of units to treatments typically yields a considerable imbalance between the groups, i.e., confounding due to randomization is the rule rather than the exception. In the second part of this contribution, the reasoning is extended to a number of traditional arguments in favour of randomization. This discussion is rather non-technical, and sometimes touches on the rather fundamental Frequentist/Bayesian debate. However, the result of this analysis turns out to be quite similar: While the contribution of randomization remains doubtful, comparability contributes much to a compelling conclusion. Summing up, classical experimentation based on sound background theory and the systematic construction of exchangeable groups seems to be advisable",English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,
2020-05-23T03:21:37.817Z,"Schmidt, S. (2009). Shall we really do it again? The powerful concept of replication is neglected in the social sciences. Review of General Psychology, 13(2), 90–100.",https://doi.org/10.1037/a0015108,Stefan Schmidt,"Primary Source, Reading",College / Upper Division (Undergraduates),"Replication is one of the most important tools for the verification of facts within the empirical sciences. A detailed examination of the notion of replication reveals that there are many different meanings to this concept and the relevant procedures, but hardly any systematic literature. This paper analyzes the concept of replication from a theoretical point of view. It demonstrates that the theoretical demands are scarcely met in everyday work within the social sciences. Some demands are just not feasible, whereas others are constricted by restrictions relating to publication. A new classification scheme based on a functional approach that distinguishes between different types of replication is proposed. Next, it will be argued that replication addresses the important connection between existing and new knowledge. To do so it has to be applied explicitly and systematically. The paper ends with a description of procedures how this could be done and a set of recommendations how to handle the concept of replication in the future to exploit its potential to the full.",English,I don't see any of these,Student,"Applied Science, Math & Statistics","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-23T03:24:41.687Z,Is Psychology Suffering From a Replication Crisis? What Does “Failure to Replicate” Really Mean?,https://doi.org/10.1037/a0039400,"Scott E. Maxwell, Michael Y. Lau and George S. Howard","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and meta-analysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology’s alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:00:43.880Z,A Unified Framework to Quantify the Credibility of Scientific Findings,https://journals.sagepub.com/doi/10.1177/2515245918787489,"Etienne P. LeBel, Randy J. McCarthy, Brian D. Earp, Malte Elson and Wolf Vanpaemel","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Preregistration,Open Data and Materials,Replication Research","Transparency,Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:02:46.188Z,Tracking replicability as a method of post-publication open evaluation,https://www.frontiersin.org/articles/10.3389/fncom.2012.00008/full,Joshua K. Hartshorne and Adena Schachner,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent reports have suggested that many published results are unreliable. To increase the reliability and accuracy of published papers, multiple changes have been proposed, such as changes in statistical methods. We support such reforms. However, we believe that the incentive structure of scientific publishing must change for such reforms to be successful. Under the current system, the quality of individual scientists is judged on the basis of their number of publications and citations, with journals similarly judged via numbers of citations. Neither of these measures takes into account the replicability of the published findings, as false or controversial results are often particularly widely cited. We propose tracking replications as a means of post-publication evaluation, both to help researchers identify reliable findings and to incentivize the publication of reliable results. Tracking replications requires a database linking published studies that replicate one another. As any such database is limited by the number of replication attempts published, we propose establishing an open-access journal dedicated to publishing replication attempts. Data quality of both the database and the affiliated journal would be ensured through a combination of crowd-sourcing and peer review. As reports in the database are aggregated, ultimately it will be possible to calculate replicability scores, which may be used alongside citation counts to evaluate the quality of work published in individual journals. In this paper, we lay out a detailed description of how this system could be implemented, including mechanisms for compiling the information, ensuring data quality, and incentivizing the research community to participate.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:04:53.667Z,Investigating Variation in Replicability: A “Many Labs” Replication Project,https://psycnet.apa.org/fulltext/2014-20922-002.html,Klein et al.,"Primary Source, Reading, Reading",College / Upper Division (Undergraduates),"Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:06:33.029Z,Replication and Robustness in Developmental Research,https://doi.org/10.1037/a0037996,"Greg J Duncan, Mimi Engel, Amy Claessens, Chantelle J Dowsett","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Replications and robustness checks are key elements of the scientific method and a staple in many disciplines. However, leading journals in developmental psychology rarely include explicit replications of prior research conducted by different investigators, and few require authors to establish in their articles or online appendices that their key results are robust across estimation methods, data sets, and demographic subgroups. This article makes the case for prioritizing both explicit replications and, especially, within-study robustness checks in developmental psychology. It provides evidence on variation in effect sizes in developmental studies and documents strikingly different replication and robustness-checking practices in a sample of journals in developmental psychology and a sister behavioral science-applied economics. Our goal is not to show that any one behavioral science has a monopoly on best practices, but rather to show how journals from a related discipline address vital concerns of replication and generalizability shared by all social and behavioral sciences. We provide recommendations for promoting graduate training in replication and robustness-checking methods and for editorial policies that encourage these practices. Although some of our recommendations may shift the form and substance of developmental research articles, we argue that they would generate considerable scientific benefits for the field.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:08:26.709Z,"Goodman, S. N., Fanelli, D., & Ioannidis, J. P. (2016). What does research reproducibility mean?. Science translational medicine, 8(341), 341ps12-341ps12.",https://stm.sciencemag.org/content/8/341/341ps12/tab-pdf,"Goodman, S. N., Fanelli, D., & Ioannidis, J. P.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:10:37.555Z,"The ASA Statement on p-Values: Context, Process, and Purpose",https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108,Ronald L. Wasserstein & Nicole A. Lazar,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An editorial about p value,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:13:40.761Z,"Sangnier, M., & Zylberberg, Y. (2016). Star wars: The empirics strike back. American Economic Journal: Applied Economics, 8(1), 1-32.",https://doi.org/10.1257/app.20150044,"Sangnier, M., & Zylberberg, Y. ","Primary Source, Reading",College / Upper Division (Undergraduates),"Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing ""significant"" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:16:20.027Z,"Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science",http://web.stanford.edu/group/bps/cgi-bin/wordpress/wp-content/uploads/2015/09/NSF-Robust-Research-Workshop-Report.pdf,"Cacioppo, J. T., Kaplan, R. M., Krosnick, J. A., Olds, J. L., & Dean, H. ","Primary Source, Reading",College / Upper Division (Undergraduates),"A paper about social, behavioural economic perspective on reproducible science",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-24T12:17:30.012Z,"Nuzzo, R. (2014). Statistical errors: P values, the ‘gold standard’ of statistical validity, are not as reliable as many scientists assume. Nature, 506(7487), 150-152.",https://www.nature.com/news/scientific-method-statistical-errors-1.14700,"Nuzzo, R.","Primary Source, Reading",College / Upper Division (Undergraduates),"P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:19:43.126Z,Abandon statistical significance. ,https://doi.org/10.1080/00031305.2018.1527253,"McShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. ","Primary Source, Reading",College / Upper Division (Undergraduates),"We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We  recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:23:32.312Z,Surrogate Science: The Idol of a Universal Method for Scientific Inference,https://doi.org/10.1177/0149206314547522,Gerd Gigerenzer and Julian N. Marewski,"Primary Source, Reading",College / Upper Division (Undergraduates),"The application of statistics to science is not a neutral act. Statistical tools have shaped and were also shaped by its objects. In the social sciences, statistical methods fundamentally changed research practice, making statistical inference its centerpiece. At the same time, textbook writers in the social sciences have transformed rivaling statistical systems into an apparently monolithic method that could be used mechanically. The idol of a universal method for scientific inference has been worshipped since the “inference revolution” of the 1950s. Because no such method has ever been found, surrogates have been created, most notably the quest for significant p values. This form of surrogate science fosters delusions and borderline cheating and has done much harm, creating, for one, a flood of irreproducible results. Proponents of the “Bayesian revolution” should be wary of chasing yet another chimera: an apparently universal inference procedure. A better path would be to promote both an understanding of the various devices in the “statistical toolbox” and informed judgment to select among these.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-24T12:26:19.575Z,The case against statistical significance testing.,https://www.hepgjournals.org/doi/abs/10.17763/haer.48.3.t490261645281841,Ronald P Carver,"Primary Source, Reading",College / Upper Division (Undergraduates),"In recent years the use of traditional statistical methods in educational research has increasingly come under attack. In this article, Ronald P. Carver exposes the fantasies often entertained by researchers about the meaning of statistical significance. The author recommends abandoning all statistical significance testing and suggests other ways of evaluating research results. Carver concludes that we should return to the scientific method of examining data and replicating results rather than relying on statistical significance testing to provide equivalent information.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:27:53.785Z,"Evolution of Reporting P Values in the Biomedical Literature, 1990-2015",https://doi.org/10.1001/jama.2016.1952,"David Chavalarias, Joshua David Wallach, Alvin Ho Ting Li, John P A Ioannidis ","Primary Source, Reading",College / Upper Division (Undergraduates),"Importance: The use and misuse of P values has generated extensive debates. Objective: To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values. Design: Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text. Main outcomes and measures: P values reported. Results: Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the ""best"" (most statistically significant) reported P values were modestly smaller and the ""worst"" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome. Conclusions and relevance: In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990-2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:30:51.357Z,Statistical significance in psychological research.,https://doi.org/10.1037/h0026141,Lykken,"Primary Source, Reading",College / Upper Division (Undergraduates),"MOST THEORIES IN THE AREAS OF PERSONALITY, CLINICAL, AND SOCIAL PSYCHOLOGY PREDICT ONLY THE DIRECTION OF A CORRELATION, GROUP DIFFERENCE, OR TREATMENT EFFECT. SINCE THE NULL HYPOTHESIS IS NEVER STRICTLY TRUE, SUCH PREDICTIONS HAVE ABOUT A 50-50 CHANCE OF BEING CONFIRMED BY EXPERIMENT WHEN THE THEORY IN QUESTION IS FALSE, SINCE THE STATISTICAL SIGNIFICANCE OF THE RESULT IS A FUNCTION OF THE SAMPLE SIZE. CONFIRMATION OF 1 DIRECTIONAL PREDICTION GENERALLY BUILDS LITTLE CONFIDENCE IN THE THEORY BEING TESTED. MOST THEORIES SHOULD BE TESTED BY MULTIPLE CORROBORATION AND MOST EMPIRICAL GENERALIZATIONS BY CONSTRUCTIVE REPLICATION. STATISTICAL SIGNIFICANCE, PERHAPS THE LEAST IMPORTANT ATTRIBUTE OF A GOOD EXPERIMENT, IS NEVER A SUFFICIENT CONDITION FOR CLAIMING THAT (1) A THEORY HAS BEEN USEFULLY CORROBORATED, (2) A MEANINGFUL EMPIRICAL FACT HAS BEEN ESTABLISHED, OR (3) AN EXPERIMENTAL REPORT OUGHT TO BE PUBLISHED",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-24T12:32:09.741Z,Null Hypothesis Significance Testing. On the Survival of a Flawed Method,https://doi.org/10.1037/0003-066x.56.1.16,J Krueger,"Primary Source, Reading",College / Upper Division (Undergraduates),"Null hypothesis significance testing (NHST) is the researcher's workhorse for making inductive inferences. This method has often been challenged, has occasionally been defended, and has persistently been used through most of the history of scientific psychology. This article reviews both the criticisms of NHST and the arguments brought to its defense. The review shows that the criticisms address the logical validity of inferences arising from NHST, whereas the defenses stress the pragmatic value of these inferences. The author suggests that both critics and apologists implicitly rely on Bayesian assumptions. When these assumptions are made explicit, the primary challenge for NHST--and any system of induction--can be confronted. The challenge is to find a solution to the question of replicability.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-24T12:34:37.031Z,The harm done by tests of significance.,https://doi.org/10.1016/S0001-4575(03)00036-8,Ezra Hauer,"Primary Source, Reading",College / Upper Division (Undergraduates),Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.,English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-24T12:37:21.911Z,In Praise of the Null Hypothesis Statistical Test,https://doi.org/10.1037/0003-066X.52.1.15,Richard L. Hagen,"Primary Source, Reading",College / Upper Division (Undergraduates),"Jacob Cohen (see record 1995-12080-001) raised a number of questions about the logic and information value of the null hypothesis statistical test (NHST). Specifically, he suggested that: (1) The NHST does not tell us what we want to know; (2) the null hypothesis is always false; and (3) the NHST lacks logical integrity. It is the author's view that although there may be good reasons to give up the NHST, these particular points made by Cohen are not among those reasons. When addressing these points, the author also attempts to demonstrate the elegance and usefulness of the NHST.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-24T12:39:29.922Z,Effect sizes and p values: What should be reported and what should be replicated? ,https://doi.org/10.1111/j.1469-8986.1996.tb02121.x.,"ANTHONY G. GREENWALD, RICHARD GONZALEZ, RICHARD J. HARRIS and DONALD GUTHRIE","Primary Source, Reading",College / Upper Division (Undergraduates),"Despite publication of many well-argued critiques of null hypothesis testing (NHT), behavioral science researchers continue to rely heavily on this set of practices. Although we agree with most critics' catalogs of NHT's flaws, this article also takes the unusual stance of identifying virtues that may explain why NHT continues to be so extensively used. These virtues include providing results in the form of a dichotomous (yes/no) hypothesis evaluation and providing an index (p value) that has a justifiable mapping onto confidence in repeatability of a null hypothesis rejection. The most-criticized flaws of NHT can be avoided when the importance of a hypothesis, rather than the p value of its test, is used to determine that a finding is worthy of report, and when p approximately equal to .05 is treated as insufficient basis for confidence in the replicability of an isolated non-null finding. Together with many recent critics of NHT, we also urge reporting of important hypothesis tests in enough descriptive detail to permit secondary uses such as meta-analysis.",English,I don't see any of these,Student,"Applied Science, Math & Statistics",Conceptual and Statistical Knowledge,
2020-05-24T12:42:57.049Z,The appropriate use of null hypothesis testing. ,https://doi.org/10.1037/1082-989X.1.4.379,R.W. Frick,"Primary Source, Reading",College / Upper Division (Undergraduates),"The many criticisms of null hypothesis testing suggest when it is not useful and what is should not be used for. This article explores when and why its use is appropriate. Null hypothesis testing is insufficient when size of effect is important, but it is ideal for testing ordinal claims relating the order of conditions, which are common in psychology. Null hypothesis testing also is insufficient for determining beliefs, but it is ideal for demonstrating sufficient evidential strength to support an ordinal claim, with sufficient evidence being 1 criterion for a finding entering the corpus of legitimate findings in psychology. The line between sufficient and insufficient evidence is currently set at p < .05; there is little reason for allowing experimenters to select their own value of alpha. Thus null hypothesis testing is an optimal method for demonstrating sufficient evidence for an ordinal claim.",English,I don't see any of these,Student,"Applied Science, Math & Statistics",Conceptual and Statistical Knowledge,
2020-05-24T12:45:47.042Z,On the origins of the .05 level of statistical significance,https://doi.org/10.1037/0003-066X.37.5.553,MICHAEL COWLES and CAROLINE DAVIS,"Primary Source, Reading",College / Upper Division (Undergraduates),"Examination of the literature in statistics and probability that predates Fisher's Statistical Methods for Research Workers indicates that although Fisher is responsible for the first formal statement of the .05 criterion for statistical significance, the concept goes back much further. The move toward conventional levels for the rejection of the hypothesis of chance dates from the turn of the century. Early statements about statistical significance were given in terms of the probable error. These earlier conventions were adopted and restated by Fisher.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T14:31:43.981Z,"Adjusting for Publication Bias in MetaAnalysis. Perspectives on Psychological Science, 11(5), 730–749. ",http://doi.org/10.1177/1745691616662243,"McShane, B. B., Böckenholt, U., & Hansen, K. T","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis—that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T14:33:46.426Z,"Tijdink, J. K., Verbeke, R., & Smulders, Y. M. (2014). Publication pressure and scientific misconduct in medical scientists. Journal of Empirical Research on Human Research Ethics, 9(5), 64-71.",https://doi.org/10.1177/1556264614552421," Joeri K. Tijdink, Reinout Verbeke, Yvo M. Smulders","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72% rated publication pressure as “too high.” Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:35:55.684Z,Is the replicability crisis overblown? Three arguments examined,https://doi.org/10.1177/1745691612463401,"Pashler, H., & Harris, C. R.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We discuss three arguments voiced by scientists who view the current outpouring of concern about replicability as overblown. The first idea is that the adoption of a low alpha level (e.g., 5%) puts reasonable bounds on the rate at which errors can enter the published literature, making false-positive effects rare enough to be considered a minor issue. This, we point out, rests on statistical misunderstanding: The alpha level imposes no limit on the rate at which errors may arise in the literature (Ioannidis, 2005b). Second, some argue that whereas direct replication attempts are uncommon, conceptual replication attempts are common—providing an even better test of the validity of a phenomenon. We contend that performing conceptual rather than direct replication attempts interacts insidiously with publication bias, opening the door to literatures that appear to confirm the reality of phenomena that in fact do not exist. Finally, we discuss the argument that errors will eventually be pruned out of the literature if the field would just show a bit of patience. We contend that there are no plausible concrete scenarios to back up such forecasts and that what is needed is not patience, but rather systematic reforms in scientific practice.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:37:36.491Z,Publication bias and the canonization of false facts. ,https://doi.org/10.7554/eLife.21451,"Nissen, S. B., Magidson, T., Gross, K., & Bergstrom, C. T.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Science is facing a “replication crisis” in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community’s confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:40:54.418Z,Meta-analyses are no substitute for registered replications: a skeptical perspective on religious priming,https://doi.org/10.3389/fpsyg.2015.01365,"Michiel van Elk, Dora Matzke, Quentin F. Gronau, Maime Guan, Joachim Vandekerckhove and Eric-Jan Wagenmakers","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"According to a recent meta-analysis, religious priming has a positive effect on prosocial behavior (Shariff et al., 2015). We first argue that this meta-analysis suffers from a number of methodological shortcomings that limit the conclusions that can be drawn about the potential benefits of religious priming. Next we present a re-analysis of the religious priming data using two different meta-analytic techniques. A Precision-Effect Testing–Precision-Effect-Estimate with Standard Error (PET-PEESE) meta-analysis suggests that the effect of religious priming is driven solely by publication bias. In contrast, an analysis using Bayesian bias correction suggests the presence of a religious priming effect, even after controlling for publication bias. These contradictory statistical results demonstrate that meta-analytic techniques alone may not be sufficiently robust to firmly establish the presence or absence of an effect. We argue that a conclusive resolution of the debate about the effect of religious priming on prosocial behavior – and about theoretically disputed effects more generally – requires a large-scale, preregistered replication project, which we consider to be the sole remedy for the adverse effects of experimenter bias and publication bias.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:44:04.818Z,Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size,https://doi.org/10.1371/journal.pone.0105825,"Anton Kuhberger, Astrid Fritz, Thomas Scherndl","Primary Source, Reading",College / Upper Division (Undergraduates),"Background: The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias. Methods: We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values. Results: We found a negative correlation of r = −.45 [95% CI: −.53; −.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings. Conclusion: The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:48:00.540Z,"Psychologists Are Open to Change, yet Wary of Rules",https://doi.org/10.1177/1745691612459521,"Heather M. Fuchs, Mirjam Jenny, Susann Fiedler","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychologists must change the way they conduct and report their research—this notion has been the topic of much debate in recent years. One article recently published in Psychological Science proposing six requirements for researchers concerning data collection and reporting practices as well as four guidelines for reviewers aimed at improving the publication process has recently received much attention (Simmons, Nelson, & Simonsohn, 2011). We surveyed 1,292 psychologists to address two questions: Do psychologists support these concrete changes to data collection, reporting, and publication practices, and if not, what are their reasons? Respondents also indicated the percentage of print and online journal space that should be dedicated to novel studies and direct replications as well as the percentage of published psychological research that they believed would be confirmed if direct replications were conducted. We found that psychologists are generally open to change. Five requirements for researchers and three guidelines for reviewers were supported as standards of good practice, whereas one requirement was even supported as a publication condition. Psychologists appear to be less in favor of mandatory conditions of publication than standards of good practice. We conclude that the proposal made by Simmons, Nelson & Simonsohn (2011) is a starting point for such standards.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:49:40.450Z,Publication bias in the social sciences: Unlocking the file drawer,https://doi.org/10.1126/science.1255484,"Annie Franco, Neil Malhotra, Gabor Simonovits","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:52:25.107Z,"Scientists’ Reputations Are Based on Getting It Right, Not Being Right",https://doi.org/10.1371/journal.pbio.1002460,"Charles R. Ebersole ,Jordan R. Axt, Brian A. Nosek","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Replication is vital for increasing precision and accuracy of scientific claims. However, when replications “succeed” or “fail,” they could have reputational consequences for the claim’s originators. Surveys of United States adults (N = 4,786), undergraduates (N = 428), and researchers (N = 313) showed that reputational assessments of scientists were based more on how they pursue knowledge and respond to replication evidence, not whether the initial results were true. When comparing one scientist that produced boring but certain results with another that produced exciting but uncertain results, opinion favored the former despite researchers’ belief in more rewards for the latter. Considering idealized views of scientific practices offers an opportunity to address incentives to reward both innovation and verification.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:54:13.825Z,Bite-Size Science and Its Undesired Side Effects,https://doi.org/10.1177/1745691611429353,"Marco Bertamini, Marcus R. Munafò","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Short and rapid publication of research findings has many advantages. However, there is another side of the coin that needs careful consideration. We argue that the most dangerous aspect of a shift toward “bite-size” publishing is the relationship between study size and publication bias. Findings based on a single study or a study based on a limited sample size are more likely to be false positive, because the false positive rate remains constant, whereas the true positive rate (the power) declines as sample size declines. Pressure on productivity and on novelty value further exacerbates the problem.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T14:56:18.299Z,The prevalence of statistical reporting errors in psychology (1985–2013),https://doi.org/10.3758/s13428-015-0664-2,"Michèle B. Nuijten, Chris H. J. Hartgerink, Marcel A. L. M. van Assen, Sacha Epskamp & Jelte M. Wicherts ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package “statcheck.” statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called “co-pilot model,” and to use statcheck to flag possible inconsistencies in one’s own manuscript or during the review process.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T14:57:56.043Z,The GRIM Test: A Simple Technique Detects Numerous Anomalies in the Reporting of Results in Psychology,https://doi.org/10.1177/1948550616673876,"Nicholas J. L. Brown, James A. J. Heathers","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-26T15:00:11.045Z,"Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking",https://doi.org/10.3389/fpsyg.2016.01832,"Jelte M. Wicherts, Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:15:40.656Z,Do studies of statistical power have an effect on the power of studies?,https://doi.org/10.1037/0033-2909.105.2.309,"Sedlmeier, P. & Gigerenzer, G. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The long-term impact of studies of statistical power is investigated using J. Cohen's (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases. We discuss reasons for the ongoing neglect of power.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:22:16.616Z,Statistical power in operations management research ,https://doi.org/10.1016/0272-6963(95)00020-S,Rohit Verma and John C. Goodale,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This paper discusses the need and importance of statistical power analysis in field-based empirical research in Production and Operations Management (POM) and related disciplines. The concept of statistical power analysis is explained in detail and its relevance in designing and conducting empirical experiments is discussed. Statistical power reflects the degree to which differences in sample data in a statistical test can be detected. A high power is required to reduce the probability of failing to detect an effect when it is present. This paper also examines the relationship between statistical power, significance level, sample size and effect size. A probability tree analysis further explains the importance of statistical power by showing the relationship between Type II errors and the probability of making wrong decisions in statistical analysis. A power analysis of 28 articles (524 statistical tests) in the Journal of Operations Management and in Decision Sciences shows that 60% of empirical studies do not have high power levels. This means that several of these tests will have a low degree of repeatability. This and other similar issues involving statistical power will become increasingly important as empirical studies in POM study relatively smaller effects.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:24:15.724Z,Statistical Power of Psychological Research: What Have We Gained in 20 Years?,https://doi.org/10.1037//0022-006x.58.5.646.,J S Rossi,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Power was calculated for 6,155 statistical tests in 221 journal articles published in the 1982 volumes of the Journal of Abnormal Psychology, Journal of Consulting and Clinical Psychology, and Journal of Personality and Social Psychology. Power to detect small, medium, and large effects was .17, .57, and .83, respectively. 20 years after Cohen (1962) conducted the first power survey, the power of psychological research is still low. The implications of these results concerning the proliferation of Type I errors in the published literature, the failure of replication studies, and the interpretation of null (negative) results are emphasized. An example is given of the use of power analysis to help interpret null results by setting probable upper bounds on the magnitudes of effects. Limitations of statistical power analyses, suggestions for future research, sources of computational information, and recommendations for improving power are discussed.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:30:33.536Z,A survey of the statistical power of research in behavioral ecology and animal behavior.,https://doi.org/10.1093/beheco/14.3.438,"Jennions, M. D., & Møller, A. P.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We estimated the statistical power of the first and last statistical test presented in 697 papers from 10 behavioral journals. First tests had significantly greater statistical power and reported more significant results (smaller p values) than did last tests. This trend was consistent across journals, taxa, and the type of statistical test used. On average, statistical power was 13–16% to detect a small effect and 40–47% to detect a medium effect. This is far lower than the general recommendation of a power of 80%. By this criterion, only 2–3%, 13–21%, and 37–50% of the tests examined had the requisite power to detect a small, medium, or large effect, respectively. Neither p values nor statistical power varied significantly across the 10 journals or 11 taxa. However, mean p values of first and last tests were significantly correlated across journals (⁠r =.67, n = 10, p =.034⁠), with a similar trend for mean power (⁠ r =.63, n = 10, p =.051⁠). There is therefore some evidence that power and p values are repeatable among journals. Mean p values or power of first and last tests were, however, uncorrelated across taxa. Finally, there was a significant correlation between power and reported p value for both first (⁠ r =.13, n = 684, p =.001⁠) and last tests (⁠ r =.16, n = 654, p <.0001⁠). If true effect sizes are unrelated to study sample sizes, the average true effect size must be nonzero for this pattern to emerge. This suggests that failure to observe significant relationships is partly owing to small sample sizes, as power increases with sample size.",English,I don't see any of these,Student,"Life Science, Math & Statistics",Conceptual and Statistical Knowledge,
2020-05-26T15:33:31.079Z,The statistical power of abnormal-social psychological research: A review. ,https://doi.org/10.1037/h0045186,Jacob Cohen,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An article about statistical power of abnormal and social psychology,English,I don't see any of these,Student,"Life Science, Social Science",Conceptual and Statistical Knowledge,
2020-05-26T15:37:04.113Z,Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review,https://doi.org/10.1037/0021-9010.90.1.94.,"Herman Aguinis, James C Beaty, Robert J Boik and Charles A Pierce","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The authors conducted a 30-year review (1969-1998) of the size of moderating effects of categorical variables as assessed using multiple regression. The median observed effect size (f(2)) is only .002, but 72% of the moderator tests reviewed had power of .80 or greater to detect a targeted effect conventionally defined as small. Results suggest the need to minimize the influence of artifacts that produce a downward bias in the observed effect size and put into question the use of conventional definitions of moderating effect sizes. As long as an effect has a meaningful impact, the authors advise researchers to conduct a power analysis and plan future research designs on the basis of smaller and more realistic targeted effect sizes.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:42:09.693Z,The ironic effect of significant results on the credibility of multiple-study articles.,https://doi.org/10.1037/a0029487,Schimmack U,"Primary Source, Reading, Reading",College / Upper Division (Undergraduates),"Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsignificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:44:20.900Z,Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa,http://dx.doi.org/10.1080/01621459.1959.10501497,Theodore D. Sterling,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an ""error of the first kind""-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-26T15:52:37.569Z,The new statistics: Why and how.,https://doi.org/10.1038/s41562-017-0189-z,Geoff Cumming,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-26T15:58:40.309Z,Why most published research findings are false,https://doi.org/10.1371/journal.pmed.0020124,John Ioannidis,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-27T16:00:43.499Z,The file drawer problem and tolerance for null results.,https://doi.org/10.1037/0033-2909.86.3.638,R.Rosenthal,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the ""file drawer problem"" is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. ",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:02:03.205Z,Theory-Testing in Psychology and Physics: A Methodological Paradox,https://doi.org/10.1086/288135,Paul E.Meehl,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by ""success"" is very weak, and becomes weaker with increased precision. ""Statistical significance"" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental ""cuteness"" and a free reliance upon ad hoc explanations to avoid refutation.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:03:47.989Z,Cargo cult science. ,http://www.sitpor.org/wp-content/uploads/2016/04/CargoCult.pdf,Richard Feynman,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"some remarks on science, pseudoscience, and learning how to not fool yourself. Caltech's 1974 commencement address",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:04:57.881Z,The existence of publication bias and risk factors for its occurrence.,https://doi.org/10.1001/jama.1990.03440100097014,Kay Dickersin,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication bias is the tendency on the parts of investigators, reviewers, and editors to submit or accept manuscripts for publication based on the direction or strength of the study findings. Much of what has been learned about publication bias comes from the social sciences, less from the field of medicine. In medicine, three studies have provided direct evidence for this bias. Prevention of publication bias is important both from the scientific perspective (complete dissemination of knowledge) and from the perspective of those who combine results from a number of similar studies (meta-analysis). If treatment decisions are based on the published literature, then the literature must include all available data that is of acceptable quality. Currently, obtaining information regarding all studies undertaken in a given field is difficult, even impossible. Registration of clinical trials, and perhaps other types of studies, is the direction in which the scientific community should move.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:10:49.878Z,Increasing transparency through a multiverse analysis.,https://doi.org/10.1177/1745691616658637,"Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:12:42.502Z,A 21 word solution.,http://dx.doi.org/10.2139/ssrn.2160588,"Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri, A ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"One year after publishing ""False-Positive Psychology,"" we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:14:55.320Z,Psychology's Replication Crisis and the Grant Culture: Righting the Ship,https://doi.org/10.1177/1745691616687745,Scott O Lilienfeld,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The past several years have been a time for soul searching in psychology, as we have gradually come to grips with the reality that some of our cherished findings are less robust than we had assumed. Nevertheless, the replication crisis highlights the operation of psychological science at its best, as it reflects our growing humility. At the same time, institutional variables, especially the growing emphasis on external funding as an expectation or de facto requirement for faculty tenure and promotion, pose largely unappreciated hazards for psychological science, including (a) incentives for engaging in questionable research practices, (b) a single-minded focus on programmatic research, (c) intellectual hyperspecialization, (d) disincentives for conducting direct replications, (e) stifling of creativity and intellectual risk taking, (f) researchers promising more than they can deliver, and (g) diminished time for thinking deeply. Preregistration should assist with (a), but will do little about (b) through (g). Psychology is beginning to right the ship, but it will need to confront the increasingly deleterious impact of the grant culture on scientific inquiry.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:20:19.195Z,The empirical benefits of conceptual rigor: Systematic articulation of conceptual hypotheses can reduce the risk of non-replicable results (and facilitate novel discoveries too),https://doi.org/10.1016/j.jesp.2015.09.006,Mark Schaller,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Most discussions of rigor and replication focus on empirical practices (methods used to collect and analyze data). Typically overlooked is the role of conceptual practices: the methods scientists use to arrive at and articulate research hypotheses in the first place. This article discusses how the conceptualization of research hypotheses has implications for methodological decision-making and, consequently, for the replicability of results. The article identifies three ways in which empirical findings may be non-replicable, and shows how all three kinds of non-replicability are more likely to emerge when scientists take an informal conceptual approach, in which personal predictions are equated with scientific hypotheses. The risk of non-replicability may be reduced if scientists adopt more formal conceptual practices, characterized by the rigorous use of “if–then” logic to articulate hypotheses, and to systematically diagnose the plausibility, size, and context-dependence of hypothesized effects. The article identifies benefits that are likely to arise from more rigorous and systematic conceptual practices, and identifies ways in which their use can be encouraged to be more normative within the scholarly culture of the psychological sciences.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:22:44.790Z,Psychology's renaissance,https://doi.org/10.1146/annurev-psych-122216-011836,"Leif D. Nelson, Joseph Simmons, and Uri Simonsohn","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In 2010–2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists’ concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:26:34.404Z,Research Practices That Can Prevent an Inflation of False-Positive Rates,https://doi.org/10.1177/1088868313496330,"Kou Murayama, Reinhard Pekrun, Klaus Fiedler","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent studies have indicated that research practices in psychology may be susceptible to factors that increase false-positive rates, raising concerns about the possible prevalence of false-positive findings. The present article discusses several practices that may run counter to the inflation of false-positive rates. Taking these practices into account would lead to a more balanced view on the false-positive issue. Specifically, we argue that an inflation of false-positive rates would diminish, sometimes to a substantial degree, when researchers (a) have explicit a priori theoretical hypotheses, (b) include multiple replication studies in a single paper, and (c) collect additional data based on observed results. We report findings from simulation studies and statistical evidence that support these arguments. Being aware of these preventive factors allows researchers not to overestimate the pervasiveness of false-positives in psychology and to gauge the susceptibility of a paper to possible false-positives in practical and fair ways",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:27:58.120Z,"Let’s Put Our Money Where Our Mouth Is: If Authors Are to Change Their Ways, Reviewers (and Editors) Must Change With Them",https://doi.org/10.1177/1745691614528215,Jon K. Maner,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A number of scholars recently have argued for fundamental changes in the way psychological scientists conduct and report research. The behavior of researchers is influenced partially by incentive structures built into the manuscript evaluation system, and change in researcher practices will necessitate a change in the way journal reviewers evaluate manuscripts. This article outlines specific recommendations for reviewers that are designed to facilitate open data reporting and to encourage researchers to disseminate the most generative and replicable studies. These recommendations include changing the way reviewers respond to imperfections in empirical data, focusing less on individual tests of statistical significance and more on meta-analyses, being more open to null findings and failures to replicate previous research, and attending carefully to the theoretical contribution of a manuscript in addition to its methodological rigor. The article also calls for greater training and guidance for reviewers so that they can evaluate research in a manner that encourages open reporting and ultimately strengthens our science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:30:04.307Z,Sailing From the Seas of Chaos Into the Corridor of Stability: Practical Recommendations to Increase the Informational Value of Studies,https://doi.org/10.1177/1745691614528520,Daniel Lakens and Ellen R K Evers,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent events have led psychologists to acknowledge that the inherent uncertainty encapsulated in an inductive science is amplified by problematic research practices. In this article, we provide a practical introduction to recently developed statistical tools that can be used to deal with these uncertainties when performing and evaluating research. In Part 1, we discuss the importance of accurate and stable effect size estimates as well as how to design studies to reach a corridor of stability around effect size estimates. In Part 2, we explain how, given uncertain effect size estimates, well-powered studies can be designed with sequential analyses. In Part 3, we (a) explain what p values convey about the likelihood that an effect is true, (b) illustrate how the v statistic can be used to evaluate the accuracy of individual studies, and (c) show how the evidential value of multiple studies can be examined with a p-curve analysis. We end by discussing the consequences of incorporating our recommendations in terms of a reduced quantity, but increased quality, of the research output. We hope that the practical recommendations discussed in this article will provide researchers with the tools to make important steps toward a psychological science that allows researchers to differentiate among all possible truths on the basis of their likelihood.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-27T16:32:51.626Z,How to Make More Published Research True,https://doi.org/10.1371/journal.pmed.1001747,John Ioannidis,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An essay about How to Make More Published Research True,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T16:35:24.398Z,Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals,http://dx.doi.org/10.1561/100.00008024,Alan Gerber and Neil Malhotra,"Primary Source, Reading",College / Upper Division (Undergraduates),"We examine the APSR and the AJPS for the presence of publication bias due to reliance on the 0.05 significance level. Our analysis employs a broad interpretation of publication bias, which we define as the outcome that occurs when, for whatever reason, publication practices lead to bias in the published parameter estimates. We examine the effect of the 0.05 significance level on the pattern of published findings using a ""caliper"" test, a novel method for comparing studies with heterogeneous effects, and find that we can reject the hypothesis of no publication bias at the 1 in 32 billion level. Our findings therefore raise the possibility that the results reported in the leading political science journals may be misleading due to publication bias. We also discuss some of the reasons for publication bias and propose reforms to reduce its impact on research.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-27T16:43:31.803Z,Improving the Dependability of Research in Personality and Social Psychology: Recommendations for Research and Educational Practice,https://doi.org/10.1177/1088868313507536,Funder et al. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In this article, the Society for Personality and Social Psychology (SPSP) Task Force on Publication and Research Practices offers a brief statistical primer and recommendations for improving the dependability of research. Recommendations for research practice include (a) describing and addressing the choice of N (sample size) and consequent issues of statistical power, (b) reporting effect sizes and 95% confidence intervals (CIs), (c) avoiding “questionable research practices” that can inflate the probability of Type I error, (d) making available research materials necessary to replicate reported results, (e) adhering to SPSP’s data sharing policy, (f) encouraging publication of high-quality replication studies, and (g) maintaining flexibility and openness to alternative standards and methods. Recommendations for educational practice include (a) encouraging a culture of “getting it right,” (b) teaching and encouraging transparency of data reporting, (c) improving methodological instruction, and (d) modeling sound science and supporting junior researchers who seek to “get it right.”",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-05-27T17:05:25.015Z,Academic Research in the 21st Century: Maintaining Scientific Integrity in a Climate of Perverse Incentives and Hypercompetition,https://doi.org/10.1089/ees.2016.0223,Marc A. Edwards and Siddhartha Roy,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics to measure performance, and a changing business model for higher education itself. Furthermore, decreased discretionary funding at the federal and state level is creating a hypercompetitive environment between government agencies (e.g., EPA, NIH, CDC), for scientists in these agencies, and for academics seeking funding from all sources—the combination of perverse incentives and decreased funding increases pressures that can lead to unethical behavior. If a critical mass of scientists become untrustworthy, a tipping point is possible in which the scientific enterprise itself becomes inherently corrupt and public trust is lost, risking a new dark age with devastating consequences to humanity. Academia and federal agencies should better support science as a public good, and incentivize altruistic and ethical outcomes, while de-emphasizing output.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T17:12:08.421Z,"Open sharing of data on close relationships and other sensitive social psychological topics: Challenges, tools, and future directions",https://doi.org/10.1177/2515245917744281,"Joel, S., Eastwick, P., & Finkel, E. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This article reports on an adversarial (but friendly) collaboration examining the issues that lie at the intersection of confidentiality and open-data practices. We describe the process we followed to share our data for a speed-dating article we recently published in Psychological Science (Joel, Eastwick, & Finkel, 2017) and provide a summary of the issues we considered and addressed along the way. As we drafted the present article, the third author became unsure, in retrospect, about some of the procedures we had followed, especially if our approach were to be perceived as a model for open-data decisions in other, more typical cases involving nonindependent data. This article addresses these concerns, but also identifies areas of consensus. All three authors agree that there remains an unmet need for guidelines and other resources to help researchers address the challenges of sharing data that cover sensitive topics, particularly nonindependent data collected from pairs and groups (e.g., romantic couples, work teams, therapy groups). We conclude with a discussion of new tools that could be developed to help scholars who have collected such data to increase the transparency of their research while simultaneously protecting the confidentiality of the participants.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T17:14:57.354Z,Registered Reports: A New Publishing Initiative at Cortex,https://doi.org/10.1016/j.cortex.2012.12.016,Christopher Chambers,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An article about registered Reports: A new publishing initiative at Cortex,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T17:17:33.853Z,On the reproducibility of psychological science,https://doi.org/10.1080/01621459.2016.1240079,"Valen E. Johnson, Richard D. Payne,Tianying Wang,Alex Asher &Soutrik Mandal","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97% of the original studies, statistical significance was achieved in only 36% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested non-null effects. The resulting model suggests that more than 90% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-27T17:18:52.283Z,Estimating the reproducibility of psychological science,https://doi.org/10.1126/science.aac4716,Open Science Collaboration. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:01:48.975Z,The frequency of excess success for articles in Psychological Science.,https://doi.org/10.3758/s13423-014-0601-x,Gregory Francis,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent controversies have questioned the quality of scientific practice in the field of psychology, but these concerns are often based on anecdotes and seemingly isolated cases. To gain a broader perspective, this article applies an objective test for excess success to a large set of articles published in the journal Psychological Science between 2009 and 2012. When empirical studies succeed at a rate much higher than is appropriate for the estimated effects and sample sizes, readers should suspect that unsuccessful findings have been suppressed, the experiments or analyses were improper, or the theory does not properly account for the data. In total, problems appeared for 82 % (36 out of 44) of the articles in Psychological Science that had four or more experiments and could be analyzed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:04:25.650Z,SCIENTIFIC APOPHENIA IN STRATEGIC MANAGEMENT RESEARCH: SIGNIFICANCE TESTS & MISTAKEN INFERENCE,https://doi.org/10.1002/smj.2459,BRENT GOLDFARB and ANDREW A. KING,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This article uses distributional matching and posterior predictive checks to estimate the extent of false and inflated findings in empirical research on strategic management. Based on a sample of 300 papers in top outlets for research on strategic management, we estimate that if each study were repeated, 24–40 percent of significant coefficients would become insignificant at the five percent level. Our best guess is that for about half of these, the true coefficient is very close to 0. The remaining coefficients are likely directionally correct but inflated in magnitude. We offer several practical individual and field level suggestions for reducing scientific apophenia, that is, our tendency to find and publish evidence of order where none exists.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:06:08.688Z,Many Labs 3: Evaluating participant pool quality across the academic semester via replication.,https://doi.org/10.1016/j.jesp.2015.10.012,Charles R. Ebersole et al. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences-conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:10:35.558Z,Is there a credibility crisis in strategic management research? Evidence on the reproducibility of study findings,https://doi.org/10.1177/1476127017701076,"Donald D Bergh, Barton M Sharp, Herman Aguinis and Ming Li","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:13:41.155Z,"1,500 scientists lift the lid on reproducibility",https://doi.org/10.1038/533452a,Monya Baker,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Survey sheds light on the ‘crisis’ rocking research.,English,I don't see any of these,Student,"Applied Science, Social Science",Reproducibility and Replicability Knowledge,"Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:17:09.858Z,Effect size guidelines for individual differences researchers,https://doi.org/10.1016/j.paid.2016.06.069,Gilles E.Gignac and Eva T.Szodorai,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Individual differences researchers very commonly report Pearson correlations between their variables of interest. Cohen (1988) provided guidelines for the purposes of interpreting the magnitude of a correlation, as well as estimating power. Specifically, r = 0.10, r = 0.30, and r = 0.50 were recommended to be considered small, medium, and large in magnitude, respectively. However, Cohen's effect size guidelines were based principally upon an essentially qualitative impression, rather than a systematic, quantitative analysis of data. Consequently, the purpose of this investigation was to develop a large sample of previously published meta-analytically derived correlations which would allow for an evaluation of Cohen's guidelines from an empirical perspective. Based on 708 meta-analytically derived correlations, the 25th, 50th, and 75th percentiles corresponded to correlations of 0.11, 0.19, and 0.29, respectively. Based on the results, it is suggested that Cohen's correlation guidelines are too exigent, as < 3% of correlations in the literature were found to be as large as r = 0.50. Consequently, in the absence of any other information, individual differences researchers are recommended to consider correlations of 0.10, 0.20, and 0.30 as relatively small, typical, and relatively large, in the context of a power analysis, as well as the interpretation of statistical results from a normative perspective.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:18:50.167Z,"Experiments with More Than One Random Factor: Designs, Analytic Models, and Statistical Power.",https://doi.org/10.1146/annurev-psych-122414-033702,"Judd, C. M., Westfall, J., & Kenny, D. A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Traditional methods of analyzing data from psychological experiments are based on the assumption that there is a single random factor (normally participants) to which generalization is sought. However, many studies involve at least two random factors (e.g., participants and the targets to which they respond, such as words, pictures, or individuals). The application of traditional analytic methods to the data from such studies can result in serious bias in testing experimental effects. In this review, we develop a comprehensive typology of designs involving two random factors, which may be either crossed or nested, and one fixed factor, condition. We present appropriate linear mixed models for all designs and develop effect size measures. We provide the tools for power estimation for all designs. We then discuss issues of design choice, highlighting power and feasibility considerations. Our goal is to encourage appropriate analytic methods that produce replicable results for studies involving new samples of both participants and targets.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:21:15.116Z,Performing high‐powered studies efficiently with sequential analyses,https://doi.org/10.1002/ejsp.2023,Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre‐registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null‐hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large‐scale medical trials, provide an efficient way to perform high‐powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. ",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:22:58.221Z,Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation,https://doi.org/10.1146/annurev.psych.59.103006.093735,"Scott E. Maxwell, Ken Kelley and Joseph R. Rausch","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:24:20.399Z,An assessment of the magnitude of effect sizes: Evidence from 30 years of meta-analysis in management. ,https://doi.org/10.1177/1548051815614321,"Paterson, T. A., Harms, P. D., Steel, P., & Credé, M","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This study compiles information from more than 250 meta-analyses conducted over the past 30 years to assess the magnitude of reported effect sizes in the organizational behavior (OB)/human resources (HR) literatures. Our analysis revealed an average uncorrected effect of r = .227 and an average corrected effect of ρ = .278 (SDρ = .140). Based on the distribution of effect sizes we report, Cohen’s effect size benchmarks are not appropriate for use in OB/HR research as they overestimate the actual breakpoints between small, medium, and large effects. We also assessed the average statistical power reported in meta-analytic conclusions and found substantial evidence that the majority of primary studies in the management literature are statistically underpowered. Finally, we investigated the impact of the file drawer problem in meta-analyses and our findings indicate that the file drawer problem is not a significant concern for meta-analysts. We conclude by discussing various implications of this study for OB/HR researchers.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:32:15.023Z,Small telescopes Detectability and the evaluation of replication results. ,https://doi.org/10.1177/0956797614567341,Uri Simonsohn,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating “unsuccessful” replication attempts (i.e., studies yielding p > .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) “protecting” true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:35:28.546Z,At what sample size do correlations stabilize? ,https://doi.org/10.1016/j.jrp.2013.05.009,Felix D. Schonbrodt and Marco Perugini,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Sample correlations converge to the population value with increasing sample size, but the estimates are often inaccurate in small samples. In this report we use Monte-Carlo simulations to determine the critical sample size from which on the magnitude of a correlation can be expected to be stable. The necessary sample size to achieve stable estimates for correlations depends on the effect size, the width of the corridor of stability (i.e., a corridor around the true value where deviations are tolerated), and the requested confidence that the trajectory does not leave this corridor any more. Results indicate that in typical scenarios the sample size should approach 250 for stable estimates.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:37:27.140Z,Effect Size Estimation in Neuroimaging. ,https://doi.org/10.1001/jamapsychiatry.2016.3356,"Marianne C. Reddan, Martin A. Lindquist and Tor D. Wager","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A central goal of translational neuroimaging is to establish robust links between brain measures and clinical outcomes. Success hinges on the development of brain biomarkers with large effect sizes. With large enough effects, a measure may be diagnostic of outcomes at the individual patient level. Surprisingly, however, standard brain-mapping analyses are not designed to estimate or optimize the effect sizes of brain-outcome relationships, and estimates are often biased. Here, we review these issues and how to estimate effect sizes in neuroimaging research.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T13:40:20.741Z,Scanning the horizon: towards transparent and reproducible neuroimaging research,https://doi.org/10.1038/nrn.2016.167,"Russell A. Poldrack, Chris I. Baker, Joke Durnez, Krzysztof J. Gorgolewski, Paul M. Matthews, Marcus R. Munafò, Thomas E. Nichols, Jean-Baptiste Poline, Edward Vul & Tal Yarkoni ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions that are drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors and a lack of direct replication apply to many fields, but perhaps particularly to functional MRI. Here, we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful and reliable answers to neuroscientific questions.",English,I don't see any of these,Student,Life Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T13:42:57.798Z,Just Post It: The Lesson From Two Cases of Fabricated Data Detected by Statistics Alone,https://doi.org/10.1177/0956797613480366,Uri Simonsohn,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively through statistical analysis of reported means and standard deviations. Analyses of the raw data behind these published results provided invaluable confirmation of the initial suspicions, ruling out benign explanations (e.g., reporting errors, unusual distributions), identifying additional signs of fabrication, and also ruling out one of the suspected fraud’s explanations for his anomalous results. If journals, granting agencies, universities, or other entities overseeing research promoted or required data posting, it seems inevitable that fraud would be reduced",English,I don't see any of these,Student,Applied Science,"Reproducible Analyses,Open Data and Materials",
2020-05-28T13:45:26.039Z,Problems in using p-curve analysis and text-mining to detect rate of p-hacking.,https://doi.org/10.7287/peerj.preprints.1266v3,"Bishop, D. V, & Thompson, P. A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background. The p-curve is a plot of the distribution of p-values reported in a set of scientific studies. Comparisons between ranges of p-values have been used to evaluate fields of research in terms of the extent to which studies have genuine evidential value, and the extent to which they suffer from bias in the selection of variables and analyses for publication, p-hacking. Methods. p-hacking can take various forms. Here we used R code to simulate the use of ghost variables, where an experimenter gathers data on several dependent variables but reports only those with statistically significant effects. We also examined a text-mined dataset used by Head et al. (2015) and assessed its suitability for investigating p-hacking. Results. We show that when there is ghost p-hacking, the shape of the p-curve depends on whether dependent variables are intercorrelated. For uncorrelated variables, simulated p-hacked data do not give the ""p-hacking bump"" just below .05 that is regarded as evidence of p-hacking, though there is a negative skew when simulated variables are inter-correlated. The way p-curves vary according to features of underlying data poses problems when automated text mining is used to detect p-values in heterogeneous sets of published papers. Conclusions. The absence of a bump in the p-curve is not indicative of lack of p-hacking. Furthermore, while studies with evidential value will usually generate a right-skewed p-curve, we cannot treat a right-skewed p-curve as an indicator of the extent of evidential value, unless we have a model specific to the type of p-values entered into the analysis. We conclude that it is not feasible to use the p-curve to estimate the extent of p-hacking and evidential value unless there is considerable control over the type of data entered into the analysis. In particular, p-hacking with ghost variables is likely to be missed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-28T14:48:51.882Z,Accurary of effect size estimates from published psychological research ,https://doi.org/10.2466/l'MS.106.2.645-649,"ANDREW BRAND,  MICHAEL T. BRADLEY, LISA A. BEST, AND GEORGE STOICA","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Monte-Carlo simulation was used to model the biasing of effect sizes in published studies. The findings from the simulation indicate that, when a predominant bias to publish studies with statistically significant results is coupled with inadequate statistical power, there will be an overestimation of effect sizes. The consequences such an effect size overestimation will then have on meta-analyses and power analyses are highlighted and discussed along with measures which can be taken to reduce the problem. ",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-28T14:52:10.841Z,The poor availability of syntaxes of structural equation modeling,https://doi.org/10.1080/08989621.2017.1396214,Jelte M. Wicherts and Elise A. V. Crompvoets,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The syntax or codes used to fit Structural Equation Models (SEMs) convey valuable information on model specifications and the manner in which SEMs are estimated. We requested SEM syntaxes from a random sample of 229 articles (published in 1998–2013) that ran SEMs using LISREL, AMOS, or Mplus. After exchanging over 500 emails, we ended up obtaining a meagre 57 syntaxes used in these articles (24.9% of syntaxes we requested). Results considering the 129 (corresponding) authors who replied to our request showed that the odds of the syntax being lost increased by 21% per year passed since publication of the article, while the odds of actually obtaining a syntax dropped by 13% per year. So SEM syntaxes that are crucial for reproducibility and for correcting errors in the running and reporting of SEMs are often unavailable and get lost rapidly. The preferred solution is mandatory sharing of SEM syntaxes alongside articles or in data repositories.",English,I don't see any of these,Student,Math & Statistics,"Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials",
2020-05-28T14:55:21.462Z,Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. ,http://doi.org/10.1177/0956797617723724,"Anderson, S. F., Kelley, K., & Maxwell, S. E.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-29T18:15:05.773Z,Promoting Transparency in Social Science Research,https://doi.org/10.1126/science.1245317,"Miguel, E., C. Camerer, K. Casey, J. Cohen, K. M. Esterling, A. Gerber, R. Glennerster, D. P. Green, M. Humphreys, G. Imbens, D. Laitin, T. Madon, L. Nelson, B. A. Nosek, M. Petersen, R. Sedlmayr, J. P. Simmons, U. Simonsohn, M. Van der Laan. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Social scientists should adopt higher transparency standards to improve the quality and credibility of research.,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:17:17.072Z,Point of View: How open science helps researchers succeed,https://doi.org/10.7554/eLife.16800,Erin C McKiernan et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:19:49.274Z,Ten Simple Rules for the Care and Feeding of Scientific Data,https://doi.org/10.1371/journal.pcbi.1003542,"Goodman, Alyssa, et al.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about ten Simple Rules for the Care and Feeding of Scientific Data,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:22:24.009Z,Replication and the Manufacture of Scientific Inferences: A Formal Approach,https://doi.org/10.1093/isp/ekv011,Fernando Martel García,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The field of replication studies remains a controversial, misunderstood, and unappreciated piñata of 18 replication typologies spanning 79 replication types. To help bring order to the chaos, I contribute a theory of manufactured inferences. The theory is built on three pillars: (1) replication causal diagrams (or r-dags for short), (2) a formal conceptualization of study procedures, and (3) the use of Bayesian inference to update our beliefs about the natural phenomenon under investigation and the operating characteristics of the study procedures used to study it. I use this theory to motivate a formal typology of replication types, explaining how they are done and for what purpose. Finally, I discuss some implications of this theory, including the importance of an analytical approach to robustness and generalizability replications, the need to avoid conceptual replications, the possibility of legitimate (unplanned) specification searches, the limitations of meta-analysis, and the false dichotomy between so-called successful and failed replications.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:24:21.628Z,Enhancing transparency of the research process to increase accuracy of findings: A guide for relationship researchers,https://doi.org/10.1111/pere.12053,LORNE CAMPBELL  TIMOTHY J. LOVING  ETIENNE P. LEBEL,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:26:21.205Z,An agenda for purely confirmatory research. ,https://doi.org/10.1177/1745691612463078,"Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Mass, H. L. J., & Kievit, R. A.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology's academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result-a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label ""confirmatory,"" and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled ""exploratory."" We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:28:25.497Z,The value of direct replication.,https://doi.org/10.1177/1745691613514755,Daniel Simons,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Reproducibility is the cornerstone of science. If an effect is reliable, any competent researcher should be able to obtain it when using the same procedures with adequate statistical power. Two of the articles in this special section question the value of direct replication by other laboratories. In this commentary, I discuss the problematic implications of some of their assumptions and argue that direct replication by multiple laboratories is the only way to verify the reliability of an effect.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-29T18:30:35.236Z,Practical Solutions for Sharing Data and Materials From Psychological Research,https://doi.org/10.1177/2515245917746500,"Rick O. Gilmore, Joy Lorenzo Kennedy, and Karen E. Adolph","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Widespread sharing of data and materials (including displays and text- and video-based descriptions of experimental procedures) will improve the reproducibility of psychological science and accelerate the pace of discovery. In this article, we discuss some of the challenges to open sharing and offer practical solutions for researchers who wish to share more of the products—and process—of their research. Many of these solutions were devised by the Databrary.org data library for storing and sharing video, audio, and other forms of sensitive or personally identifiable data. We also discuss ways in which researchers can make shared data and materials easier for others to find and reuse. Widely adopted, these solutions and practices will increase transparency and speed progress in psychological science.",English,I don't see any of these,Student,"Applied Science, Social Science",Reproducible Analyses,Open Science
2020-05-31T18:15:05.189Z,The Generalizability of Survey Experiments,https://doi.org/10.1017/XPS.2015.19,Kevin J. Mullinix et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Survey experiments have become a central methodology across the social sciences. Researchers can combine experiments’ causal power with the generalizability of population-based samples. Yet, due to the expense of population-based samples, much research relies on convenience samples (e.g. students, online opt-in samples). The emergence of affordable, but non-representative online samples has reinvigorated debates about the external validity of experiments. We conduct two studies of how experimental treatment effects obtained from convenience samples compare to effects produced by population samples. In Study 1, we compare effect estimates from four different types of convenience samples and a population-based sample. In Study 2, we analyze treatment effects obtained from 20 experiments implemented on a population-based sample and Amazon’s Mechanical Turk (MTurk). The results reveal considerable similarity between many treatment effects obtained from convenience and nationally representative population-based samples. While the results thus bolster confidence in the utility of convenience samples, we conclude with guidance for the use of a multitude of samples for advancing scientific knowledge.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:17:52.489Z,On the reproducibility of meta-analyses: six practical recommendations,https://doi.org/10.1186/s40359-016-0126-3,Daniel Lakens et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background: Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. Discussion: The present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including quotes from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel ‘registered report’ formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. Summary: The recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of “future-proofing” meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:19:30.056Z,"Effect Size Estimates: Current Use, Calculations, and Interpretation",https://doi.org/10.1037/a0024338,Catherine O. Fritz et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The Publication Manual of the American Psychological Association (American Psychological Association, 2001, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial eta squared was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen’s d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-31T18:21:50.623Z,False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant,https://doi.org/10.1177/0956797611417632,Joseph P. Simmons,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:23:45.787Z,"Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), 241-301.",https://doi.org/10.1037/1082-989x.5.2.241,"Nickerson, R. S. (2000). ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-31T18:26:32.886Z,The rules of the game called psychological science,https://doi.org/10.1177/1745691612459060," Marjan Bakker, Annette van Dijk, Jelte M. Wicherts","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p < .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:27:55.576Z,Why psychologists must change the way they analyze their data: The case of psi: Comment on Bem (2011).,https://doi.org/10.1037/a0022790,"Wagenmakers, E.-J., Wetzels, R., Borsboom, D., & van der Maas, H. L. J.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Does psi exist? D. J. Bem (2011) conducted 9 studies with over 1,000 participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, we show that the data analysis was partly exploratory and that one-sided p values may overstate the statistical evidence against the null hypothesis. We reanalyze Bem's data with a default Bayesian t test and show that the evidence for psi is weak to nonexistent. We argue that in order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. We conclude that Bem's p values do not indicate evidence in favor of precognition; instead, they indicate that experimental psychologists need to change the way they conduct their experiments and analyze their data",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:29:22.863Z,"Welcoming Quality in Non-Significance and Replication Work, but Moving Beyond the p-Value: Announcing New Editorial Policies for Quantitative Research in JOAA",https://doi.org/10.1177/1932202X14532177," Matthew T. McBee, Michael S. Matthews","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The self-correcting nature of psychological and educational science has been seriously questioned. Recent special issues of Perspectives on Psychological Science and Psychology of Aesthetics, Creativity, and the Arts have roundly condemned current organizational models of research and dissemination and have criticized the perverse incentive structure that tempts researchers into generating and publishing false positive findings. At the same time, replications are rarely attempted, allowing untruths to persist in the literature unchallenged. In this article, the editors of the Journal of Advanced Academics consider this situation and announce new policies for quantitative submissions. They are (a) an explicit call for replication studies; (b) new instructions directing reviewers to base their evaluation of a study’s merit on the quality of the research design, execution, and written description, rather than on the statistical significance of its results; and (c) an invitation to omit statistical hypothesis tests in favor of reporting effect sizes and their confidence limits.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:31:35.747Z,Replications in Psychology Research: How Often Do They Really Occur?,https://doi.org/10.1177/1745691612460688,"Matthew C. Makel, Jonathan A. Plucker, and Boyd Hegarty","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention is the role of replication in psychological science. Using the complete publication history of the 100 psychology journals with the highest 5-year impact factors, the current article provides an overview of replications in psychological research since 1900. This investigation revealed that roughly 1.6% of all psychology publications used the term replication in text. A more thorough analysis of 500 randomly selected articles revealed that only 68% of articles using the term replication were actual replications, resulting in an overall replication rate of 1.07%. Contrary to previous findings in other fields, this study found that the majority of replications in psychology journals reported similar findings to their original studies (i.e., they were successful replications). However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. Moreover, despite numerous systemic biases, the rate at which replications are being published has increased in recent decades.",English,I don't see any of these,Student,Applied Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:33:50.069Z,Analytic Review as a Solution to the Misreporting of Statistical Results in Psychological Science,https://doi.org/10.1177/1745691614549257,"John Sakaluk, Alexander Williams, Monica Biernat","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In this article, we propose analytic review (AR) as a solution to the problem of misreporting statistical results in psychological science. AR requires authors submitting manuscripts for publication to also submit the data file and syntax used during analyses. Regular reviewers or statistical experts then review reported analyses in order to verify that the analyses reported were actually conducted and that the statistical values are accurately reported. We begin by describing the problem of misreporting in psychology and introduce the basic AR process. We then highlight both primary and secondary benefits of adopting AR and describe different permutations of the AR system, each of which has its own strengths and limitations. We conclude by attempting to dispel three anticipated concerns about AR: that it will increase the workload placed on scholars, that it will infringe on the traditional peer-review process, and that it will hurt the image of the discipline of psychology. Although implementing AR will add one more step to the bureaucratic publication process, we believe it can be implemented in an efficient manner that would greatly assist in decreasing the frequency and impact of misreporting while also providing secondary benefits in other domains of scientific integrity.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:35:05.650Z,The poor availability of psychological research data for reanalysis.,https://doi.org/10.1037/0003-066X.61.7.726,"Wicherts, Jelte M.,Borsboom, Denny,Kats, Judith,Molenaar, Dylan","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The origin of the present comment lies in a failed attempt to obtain, through e-mailed requests, data reported in 141 empirical articles recently published by the American Psychological Association (APA). Our original aim was to reanalyze these data sets to assess the robustness of the research findings to outliers. We never got that far. In June 2005, we contacted the corresponding author of every article that appeared in the last two 2004 issues of four major APA journals. Because their articles had been published in APA journals, we were certain that all of the authors had signed the APA Certification of Compliance With APA Ethical Principles, which includes the principle on sharing data for reanalysis. Unfortunately, 6 months later, after writing more than 400 e-mails--and sending some corresponding authors detailed descriptions of our study aims, approvals of our ethical committee, signed assurances not to share data with others, and even our full resumes-we ended up with a meager 38 positive reactions and the actual data sets from 64 studies (25.7% of the total number of 249 data sets). This means that 73% of the authors did not share their data.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:36:46.879Z,"Lakens, D. (2015). On the challenges of drawing conclusions from p-values just below 0.05. PeerJ, 3, e1142.",https://doi.org/10.7717/peerj.1142,Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In recent years, researchers have attempted to provide an indication of the prevalence of inflated Type 1 error rates by analyzing the distribution of p-values in the published literature. De Winter & Dodou (2015) analyzed the distribution (and its change over time) of a large number of p-values automatically extracted from abstracts in the scientific literature. They concluded there is a ‘surge of p-values between 0.041–0.049 in recent decades’ which ‘suggests (but does not prove) questionable research practices have increased over the past 25 years.’ I show the changes in the ratio of fractions of p-values between 0.041–0.049 over the years are better explained by assuming the average power has decreased over time. Furthermore, I propose that their observation that p-values just below 0.05 increase more strongly than p-values above 0.05 can be explained by an increase in publication bias (or the file drawer effect) over the years (cf. Fanelli, 2012; Pautasso, 2010, which has led to a relative decrease of ‘marginally significant’ p-values in abstracts in the literature (instead of an increase in p-values just below 0.05). I explain why researchers analyzing large numbers of p-values need to relate their assumptions to a model of p-value distributions that takes into account the average power of the performed studies, the ratio of true positives to false positives in the literature, the effects of publication bias, and the Type 1 error rate (and possible mechanisms through which it has inflated). Finally, I discuss why publication bias and underpowered studies might be a bigger problem for science than inflated Type 1 error rates, and explain the challenges when attempting to draw conclusions about inflated Type 1 error rates from a large heterogeneous set of p-values.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-05-31T18:38:37.236Z,"The Baby Factory: Difficult Research Objects, Disciplinary Standards, and the Production of Statistical Significance",https://doi.org/10.1177/2378023115625071,David Peterson,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Science studies scholars have shown that the management of natural complexity in lab settings is accomplished through a mixture of technological standardization and tacit knowledge by lab workers. Yet these strategies are not available to researchers who study difficult research objects. Using 16 months of ethnographic data from three laboratories that conduct experiments on infants and toddlers, the author shows how psychologists produce statistically significant results under challenging circumstances by using strategies that enable them to bridge the distance between an uncontrollable research object and a professional culture that prizes methodological rigor. This research raises important questions regarding the value of restrictive evidential cultures in challenging research environments.",English,CC BY-NC,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:40:20.303Z,A Short (Personal) Future History of Revolution 2.0,https://doi.org/10.1177/1745691615609918,Barbara A. Spellman,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Crisis of replicability is one term that psychological scientists use for the current introspective phase we are in—I argue instead that we are going through a revolution analogous to a political revolution. Revolution 2.0 is an uprising focused on how we should be doing science now (i.e., in a 2.0 world). The precipitating events of the revolution have already been well-documented: failures to replicate, questionable research practices, fraud, etc. And the fact that none of these events is new to our field has also been well-documented. I suggest four interconnected reasons as to why this time is different: changing technology, changing demographics of researchers, limited resources, and misaligned incentives. I then describe two reasons why the revolution is more likely to catch on this time: technology (as part of the solution) and the fact that these concerns cut across social and life sciences—that is, we are not alone. Neither side in the revolution has behaved well, and each has characterized the other in extreme terms (although, of course, each has had a few extreme actors). Some suggested reforms are already taking hold (e.g., journals asking for more transparency in methods and analysis decisions; journals publishing replications) but the feared tyrannical requirements have, of course, not taken root (e.g., few journals require open data; there is no ban on exploratory analyses). Still, we have not yet made needed advances in the ways in which we accumulate, connect, and extract conclusions from our aggregated research. However, we are now ready to move forward by adopting incremental changes and by acknowledging the multiplicity of goals within psychological science.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:44:43.283Z,Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System ,https://doi.org/10.1007/BF01173636,Michael J. Mahoney,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:51:25.106Z,A Bayesian Perspective on the Reproducibility Project: Psychology,https://doi.org/10.1371/journal.pone.0149794,Alexander Etz and Joachim Vandekerckhove,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor < 10). The majority of the studies (64%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:53:58.247Z,A Reliability-Generalization Study of Journal Peer Reviews: A Multilevel Meta-Analysis of Inter-Rater Reliability and Its Determinants,https://doi.org/10.1371/journal.pone.0014331,"Lutz Bornmann,  Rüdiger Mutz and Hans-Dieter Daniel ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background: This paper presents the first meta-analysis for the inter-rater reliability (IRR) of journal peer reviews. IRR is defined as the extent to which two or more independent reviews of the same scientific document agree. Methodology/Principal Findings: Altogether, 70 reliability coefficients (Cohen's Kappa, intra-class correlation [ICC], and Pearson product-moment correlation [r]) from 48 studies were taken into account in the meta-analysis. The studies were based on a total of 19,443 manuscripts; on average, each study had a sample size of 311 manuscripts (minimum: 28, maximum: 1983). The results of the meta-analysis confirmed the findings of the narrative literature reviews published to date: The level of IRR (mean ICC/r2 = .34, mean Cohen's Kappa = .17) was low. To explain the study-to-study variation of the IRR coefficients, meta-regression analyses were calculated using seven covariates. Two covariates that emerged in the meta-regression analyses as statistically significant to gain an approximate homogeneity of the intra-class correlations indicated that, firstly, the more manuscripts that a study is based on, the smaller the reported IRR coefficients are. Secondly, if the information of the rating system for reviewers was reported in a study, then this was associated with a smaller IRR coefficient than if the information was not conveyed. Conclusions/Significance: Studies that report a high level of IRR are to be considered less credible than those with a low level of IRR. According to our meta-analysis the IRR of peer assessments is quite limited and needs improvement (e.g., reader system).",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:57:05.866Z,Editorial Bias Against Replication Research,https://search.proquest.com/docview/1292299467?pq-origsite=gscholar&imgSeq=1,"Neuliep, James W. and Rick Crandall","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"why aren't more replications published? While there are many possible reasons, one simple one could be that journals prefer not to publish them. As authors learn that replications are not likely to be accepted by journals, they would carry out and submit less replications, thus further reducing the possibility of replications being published. To test basic facts in this area, this study measured journal editors' attitudes toward publishing replication studies. We found a strong bias against publishing replications.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-05-31T18:59:38.336Z,Falsifiability Is Not Optional,https://doi.org/10.1037/pspi0000106,"LeBel, E. P., Berger, D., Campbell, L., & Loving, T. J. (2017). ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Finkel, Eastwick, and Reis (2016; FER2016) argued the post-2011 methodological reform movement has focused narrowly on replicability, neglecting other essential goals of research. We agree multiple scientific goals are essential, but argue, however, a more fine-grained language, conceptualization, and approach to replication is needed to accomplish these goals. Replication is the general empirical mechanism for testing and falsifying theory. Sufficiently methodologically similar replications, also known as direct replications, test the basic existence of phenomena and ensure cumulative progress is possible a priori. In contrast, increasingly methodologically dissimilar replications, also known as conceptual replications, test the relevance of auxiliary hypotheses (e.g., manipulation and measurement issues, contextual factors) required to productively investigate validity and generalizability. Without prioritizing replicability, a field is not empirically falsifiable. We also disagree with FER2016’s position that “bigger samples are generally better, but . . . that very large samples could have the downside of commandeering resources that would have been better invested in other studies” (abstract). We identify problematic assumptions involved in FER2016’s modifications of our original research-economic model, and present an improved model that quantifies when (and whether) it is reasonable to worry that increasing statistical power will engender potential trade-offs. Sufficiently powering studies (i.e., >80%) maximizes both research efficiency and confidence in the literature (research quality). Given that we are in agreement with FER2016 on all key open science points, we are eager to start seeing the accelerated rate of cumulative knowledge development of social psychological phenomena such a sufficiently transparent, powered, and falsifiable approach will generate.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-01T18:54:50.364Z,Reconceptualizing replication as a sequence of different studies: A replication typology,https://doi.org/10.1016/j.jesp.2015.09.009,"Joachim Hüffmeier, Jens Mazei and  Thomas Schultze","Primary Source, Reading",College / Upper Division (Undergraduates),"In contrast to the truncated view that replications have only a little to offer beyond what is already known, we suggest a broader understanding of replications: We argue that replications are better conceptualized as a process of conducting consecutive studies that increasingly consider alternative explanations, critical contingencies, and real-world relevance. To reflect this understanding, we collected and summarized the existing literature on replications and combined it into a comprehensive overall typology that simplifies and restructures existing approaches. The resulting typology depicts how multiple, hierarchically structured replication studies guide the integration of laboratory and field research and advance theory. It can be applied to (a) evaluate a theory's current status, (b) guide researchers' decisions, (c) analyze and argue for the necessity of certain types of replication studies, and (d) assess the added value of a replication study at a given state of knowledge. We conclude with practical recommendations for different protagonists in the field (e.g., authors, reviewers, editors, and funding agencies). Together, our comprehensive typology and the related recommendations will contribute to an enhanced replication culture in social psychology and to a stronger real-world impact of the discipline.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-01T18:56:40.604Z,Internal conceptual replications do not increase independent replication success,https://doi.org/10.3758/s13423-016-1030-9,Richard Kunert,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recently, many psychological effects have been surprisingly difficult to reproduce. This article asks why, and investigates whether conceptually replicating an effect in the original publication is related to the success of independent, direct replications. Two prominent accounts of low reproducibility make different predictions in this respect. One account suggests that psychological phenomena are dependent on unknown contexts that are not reproduced in independent replication attempts. By this account, internal replications indicate that a finding is more robust and, thus, that it is easier to independently replicate it. An alternative account suggests that researchers employ questionable research practices (QRPs), which increase false positive rates. By this account, the success of internal replications may just be the result of QRPs and, thus, internal replications are not predictive of independent replication success. The data of a large reproducibility project support the QRP account: replicating an effect in the original publication is not related to independent replication success. Additional analyses reveal that internally replicated and internally unreplicated effects are not very different in terms of variables associated with replication success. Moreover, social psychological effects in particular appear to lack any benefit from internal replications. Overall, these results indicate that, in this dataset at least, the influence of QRPs is at the heart of failures to replicate psychological findings, especially in social psychology. Variable, unknown contexts appear to play only a relatively minor role. I recommend practical solutions for how QRPs can be avoided.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-01T18:59:09.866Z,The Reputational Consequences of Failed Replications and Wrongness Admission among Scientists,https://doi.org/10.1371/journal.pone.0143723,Adam K. Fetterman and Kai Sassenberg,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Scientists are dedicating more attention to replication efforts. While the scientific utility of replications is unquestionable, the impact of failed replication efforts and the discussions surrounding them deserve more attention. Specifically, the debates about failed replications on social media have led to worry, in some scientists, regarding reputation. In order to gain data-informed insights into these issues, we collected data from 281 published scientists. We assessed whether scientists overestimate the negative reputational effects of a failed replication in a scenario-based study. Second, we assessed the reputational consequences of admitting wrongness (versus not) as an original scientist of an effect that has failed to replicate. Our data suggests that scientists overestimate the negative reputational impact of a hypothetical failed replication effort. We also show that admitting wrongness about a non-replicated finding is less harmful to one’s reputation than not admitting. Finally, we discovered a hint of evidence that feelings about the replication movement can be affected by whether replication efforts are aimed one’s own work versus the work of another. Given these findings, we then present potential ways forward in these discussions.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-01T19:00:34.939Z,The Replication Recipe: What makes for a convincing replication? ,https://doi.org/10.1016/j.jesp.2013.10.005,Brandt et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-01T19:02:03.113Z,One cheer for null hypothesis significance testing,https://doi.org/10.1037/1082-989X.4.2.212,H. Wainer,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Null hypothesis testing as a tool in research is defended. Six examples are offered of situations in which, if all the researcher could do was ""reject H₀ at α = .05"" the scientific contribution would still be substantial. The examples are drawn from physics, cosmology, psychology, geophysics, career counseling and theology. ",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:18:55.005Z,Statistical significance testing and cumulative knowledge in psychology: implications for training researchers.,https://doi.org/10.1037/14805-019,F.L. Schmidt,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Data analysis methods in psychology still emphasize statistical significance testing, despite numerous articles demonstrating its severe deficiencies. It is now possible to use meta-analysis to show that reliance on significance testing retards the development of cumulative knowledge. But reform of teaching and practice will also require that researchers learn that the benefits that they believe flow from use of significance testing are illusory. Teachers must revamp their courses to bring students to understand that (a) reliance on significance testing retards the growth of cumulative research knowledge; (b) benefits widely believed to flow from significance testing do not in fact exist; and (c) significance testing methods must be replaced with point estimates and confidence intervals in individual studies and with meta-analyses in the integration of multiple studies. This reform is essential to the future progress of cumulative knowledge in psychological research.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:20:02.575Z,Reporting Effect Sizes in Original Psychological Research: A Discussion and Tutorial,https://doi.org/10.1037/met0000126,Jolynn Pek and David B Flora,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistical practice in psychological science is undergoing reform which is reflected in part by strong recommendations for reporting and interpreting effect sizes and their confidence intervals. We present principles and recommendations for research reporting and emphasize the variety of ways effect sizes can be reported. Additionally, we emphasize interpreting and reporting unstandardized effect sizes because of common misconceptions regarding standardized effect sizes which we elucidate. Effect sizes should directly answer their motivating research questions, be comprehensible to the average reader, and be based on meaningful metrics of their constituent variables. We illustrate our recommendations with empirical examples involving a One-way ANOVA, a categorical variable analysis, an interaction effect in linear regression, and a simple mediation model, emphasizing the interpretation of effect sizes.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:22:07.215Z,Statistical Significance and the Dichotomization of Evidence,https://doi.org/10.1080/01621459.2017.1289846,Blakeley B. McShane & David Gal,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In light of recent concerns about reproducibility and replicability, the ASA issued a Statement on Statistical Significance and p-values aimed at those who are not primarily statisticians. While the ASA Statement notes that statistical significance and p-values are “commonly misused and misinterpreted,” it does not discuss and document broader implications of these errors for the interpretation of evidence. In this article, we review research on how applied researchers who are not primarily statisticians misuse and misinterpret p-values in practice and how this can lead to errors in the interpretation of evidence. We also present new data showing, perhaps surprisingly, that researchers who are primarily statisticians are also prone to misuse and misinterpret p-values thus resulting in similar errors. In particular, we show that statisticians tend to interpret evidence dichotomously based on whether or not a p-value crosses the conventional 0.05 threshold for statistical significance. We discuss implications and offer recommendations.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:24:46.505Z,Justify your alpha,https://doi.org/10.1038/s41562-018-0311-x,Daniel Lakens et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:26:53.204Z,Enabling Open-Science Initiatives in Clinical Psychology and Psychiatry Without Sacrificing Patients’ Privacy: Current Practices and Future Challenges,https://doi.org/10.1177/2515245917749652,"Walsh, C., Xia, W., Li, M., Denny, J., Harris, P., & Malin, B","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The psychological and psychiatric communities are generating data on an ever-increasing scale. To ensure that society reaps the greatest utility in research and clinical care from such rich resources, there is significant interest in wide-scale, open data sharing to foster scientific endeavors. However, it is imperative that such open-science initiatives ensure that data-privacy concerns are adequately addressed. In this article, we focus on these issues in clinical research. We review the privacy risks and then discuss how they can be mitigated through appropriate governance mechanisms that are both social (e.g., the application of data-use agreements) and technological (e.g., de-identification of structured data and unstructured narratives). We also discuss the benefits and drawbacks of these mechanisms, particularly as regards data fidelity. Our focus is on de-identification methods that meet regulatory requirements, such as the Privacy Rule of the Health Insurance Portability and Accountability Act of 1996. To illustrate their potential, we show how the principles we discuss have been applied in a large-scale clinical database and distributed research networks. We close this article with a discussion of challenges in supporting data privacy as open-science initiatives grow in their scale and complexity.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T19:29:18.431Z,Significance tests have their place,https://doi.org/10.1111/j.1467-9280.1997.tb00535.x,Richard J. Harris,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Null-hypothesis significance tests (NHST), properly used, tell us whether we have sufficient evidence to be confident of the sign of the population effect–but only if we abandon two-valued logic in favor of Kaiser's (1960) three-alternative hypothesis tests Confidence intervals provide a useful addition to NHSTs, and can be used to provide the same sign-determination function as NHST However, when so used, confidence intervals are subject to exactly the same Type I, II, and III error rates as NHST In addition, NHSTs provide two pieces of information about our data–maximum probability of a Type III error and probability of a successful exact replication–that confidence intervals do not The proposed alternative to NHST is just as susceptible to misinterpretation as is NHST The problem of bias due to censoring of data collection or publication can be handled by providing archives for all methodologically sound data sets, but reserving interpretations and conclusions for statistically significant results",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:31:18.321Z,Consequences of prejudice against the null hypothesis.,https://doi.org/10.1037/h0076157,Anthony G. Greenwald ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Examined the consequences of prejudice against accepting the null hypothesis through (a) a mathematical model intended to stimulate the research-publication process and (b) case studies of apparent erroneous rejections of the null hypothesis in published psychological research. The input parameters for the model characterize investigators' probabilities of selecting a problem for which the null hypothesis is true, of reporting, following up on, or abandoning research when data do or do not reject the null hypothesis, and they characterize editors' probabilities of publishing manuscripts concluding in favor of or against the null hypothesis. With estimates of the input parameters based on a questionnaire survey of 75 social psychologists, the model output indicates a dysfunctional research-publication system. Particularly, the model indicates that there may be relatively few publications on problems for which the null hypothesis is (at least to a reasonable approximation) true, and of these, a high proportion will erroneously reject the null hypothesis. The case studies provide additional support for this conclusion. It is concluded that research traditions and customs of discrimination against accepting the null hypothesis may be very detrimental to research progress",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:33:25.880Z,"Statistical tests, p values, confidence intervals, and power: a guide to misinterpretations. ",http://doi.org/10.1007/s10654-016-0149-3,"Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:35:51.218Z,Using OSF to Share Data: A Step-by-Step Guide,https://doi.org/10.1177/2515245918757689,Courtney K. Soderberg,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T19:38:16.466Z,P Values and Statistical Practice,https://doi.org/10.1097/EDE.0b013e31827886f7,Andrew Gelman,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An article about  P Values and Statistical Practice,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:40:23.810Z,Redefine statistical signifcance,https://doi.org/10.1038/s41562-017-0189-z,Daniel J. Benjamin,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),We propose to change the default P-value threshold for statistical signifcance from 0.05 to 0.005 for claims of new discoveries.,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:42:56.653Z,Construct validation in social and personality research: Current practice and recommendations. ,http://doi.org/10.1177/1948550617693063,"Flake, J. K., Pek, J., & Hehman, E. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The verity of results about a psychological construct hinges on the validity of its measurement, making construct validation a fundamental methodology to the scientific process. We reviewed a representative sample of articles published in the Journal of Personality and Social Psychology for construct validity evidence. We report that latent variable measurement, in which responses to items are used to represent a construct, is pervasive in social and personality research. However, the field does not appear to be engaged in best practices for ongoing construct validation. We found that validity evidence of existing and author-developed scales was lacking, with coefficient a often being the only psychometric evidence reported. We provide a discussion of why the construct validation framework is important for social and personality researchers and recommendations for improving practice.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T19:44:06.983Z,The natural selection of bad science.,https://doi.org/10.1098/rsos.160384,"Smaldino, P. E., & McElreath, R.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T19:45:58.086Z,The Chrysalis Effect: How Ugly Initial Results Metamorphosize Into Beautiful Articles,http://doi.org/10.1177/0149206314527133,"Ernest Hugh O’Boyle, Jr., George Christopher Banks, Erik Gonzalez-Mulé","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T19:48:03.156Z,Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture.,https://doi.org/10.1242/dmm.015123,"Matosin, N., Frank, E., Engel, M., Lum, J. S., & Newell, K. A.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Science is often romanticised as a flawless system of knowledge building, where scientists work together to systematically find answers. In reality, this is not always the case. Dissemination of results are straightforward when the findings are positive, but what happens when you obtain results that support the null hypothesis, or do not fit with the current scientific thinking? In this Editorial, we discuss the issues surrounding publication bias and the difficulty in communicating negative results. Negative findings are a valuable component of the scientific literature because they force us to critically evaluate and validate our current thinking, and fundamentally move us towards unabridged science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:03:57.599Z,Comparison of Registered and Published Primary Outcomes in Randomized Controlled Trials,https://doi.org/10.1001/jama.2009.1242,"Sylvain Mathieu, Isabelle Boutron, David Moher, Douglas G Altman, Philippe Ravaud","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Context: As of 2005, the International Committee of Medical Journal Editors required investigators to register their trials prior to participant enrollment as a precondition for publishing the trial's findings in member journals. Objective: To assess the proportion of registered trials with results recently published in journals with high impact factors; to compare the primary outcomes specified in trial registries with those reported in the published articles; and to determine whether primary outcome reporting bias favored significant outcomes. Data sources and study selection: MEDLINE via PubMed was searched for reports of randomized controlled trials (RCTs) in 3 medical areas (cardiology, rheumatology, and gastroenterology) indexed in 2008 in the 10 general medical journals and specialty journals with the highest impact factors. Data extraction: For each included article, we obtained the trial registration information using a standardized data extraction form. Results: Of the 323 included trials, 147 (45.5%) were adequately registered (ie, registered before the end of the trial, with the primary outcome clearly specified). Trial registration was lacking for 89 published reports (27.6%), 45 trials (13.9%) were registered after the completion of the study, 39 (12%) were registered with no or an unclear description of the primary outcome, and 3 (0.9%) were registered after the completion of the study and had an unclear description of the primary outcome. Among articles with trials adequately registered, 31% (46 of 147) showed some evidence of discrepancies between the outcomes registered and the outcomes published. The influence of these discrepancies could be assessed in only half of them and in these statistically significant results were favored in 82.6% (19 of 23). Conclusion: Comparison of the primary outcomes of RCTs registered with their subsequent publication indicated that selective outcome reporting is prevalent.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science",Preregistration,
2020-06-03T20:06:39.357Z,Underreporting in Psychology Experiments: Evidence from a Study Registry. ,https://doi.org/10.1177/1948550615598377,"Annie Franco , Neil Malhotra , and Gabor Simonovits","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Many scholars have raised concerns about the credibility of empirical findings in psychology, arguing that the proportion of false positives reported in the published literature dramatically exceeds the rate implied by standard significance levels. A major contributor of false positives is the practice of reporting a subset of the potentially relevant statistical analyses pertaining to a research project. This study is the first to provide direct evidence of selective underreporting in psychology experiments. To overcome the problem that the complete experimental design and full set of measured variables are not accessible for most published research, we identify a population of published psychology experiments from a competitive grant program for which questionnaires and data are made publicly available because of an institutional rule. We find that about 40% of studies fail to fully report all experimental conditions and about 70% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:08:20.805Z,A vast graveyard of undead theories publication bias and psychological science’s aversion to the null. ,https://doi.org/10.1177/1745691612459059,"Christopher J. Ferguson, Moritz Heene","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:11:45.559Z,Data: Sharing Is Caring,https://doi.org/10.1177/2515245918758319,Margaret C. Levenstein and Jared A. Lyle,"Primary Source, Reading",College / Upper Division (Undergraduates),Data sharing promotes scientific progress by permitting replication of prior scientific analyses and by increasing the return on the human and financial investments made in data collection. The costs of data sharing can be reduced through the implementation of best practices in data management across the research life cycle; this article provides specific guidance on these practices. The benefits of data sharing will be reaped when researchers who share their data are rewarded with citations and recognition of the intellectual value inherent in producing new scientific data.,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducible Analyses,Open Data and Materials",
2020-06-03T20:14:19.516Z,Measurement error and the replication crisis. ,http://science.sciencemag.org/content/355/6325/584,"Loken, E., & Gelman, A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),The assumption that measurement error always reduces effect sizes is false,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:16:57.085Z,Meta‐regression approximations to reduce publication selection bias.,https://doi.org/10.1002/jrsm.1095,"TD Stanley, H Doucouliagos","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations tothe conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in mostcases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulationsalso demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regressionintercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accom-modate systematic heterogeneity along with complex and differential publication selection bias that is relatedto moderator variables. By p roviding an intuitive reason for these approximations, we can also explain why theEgger regression works so well and when it does not. These meta-regression methods are applied to severalpolicy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, theminimum wage, and nicotine replacement therapy.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:19:21.703Z,False-Positive Citations,https://doi.org/10.1177/1745691617698146,"Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We describe why we wrote “False-Positive Psychology,” analyze how it has been cited, and explain why the integrity of experimental psychology hinges on the full disclosure of methods, the sharing of materials and data, and, especially, the preregistration of analyses.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:20:48.282Z,Erroneous analyses of interactions in neuroscience: a problem of significance. ,https://doi.org/10.1038/nn.2886,"Nieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. An additional analysis suggests that incorrect analyses of interactions are even more common in cellular and molecular neuroscience. We discuss scenarios in which the erroneous procedure is particularly beguiling.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:22:21.975Z,HARKing: Hypothesizing after the results are known. ,https://doi.org/10.1207/s15327957pspr0203_4.,"Kerr, N. L. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:23:31.650Z,The Extent and Consequences of P-Hacking in Science. ,http://doi.org/10.1371/journal.pbio.1002106,"Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ""p-hacking,"" occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:25:13.564Z,On the plurality of (methodological) worlds: estimating the analytic flexibility of fMRI experiments,https://doi.org/10.3389/fnins.2012.00149,"Carp, J.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"How likely are published findings in the functional neuroimaging literature to be false? According to a recent mathematical model, the potential for false positives increases with the flexibility of analysis methods. Functional MRI (fMRI) experiments can be analyzed using a large number of commonly used tools, with little consensus on how, when, or whether to apply each one. This situation may lead to substantial variability in analysis outcomes. Thus, the present study sought to estimate the flexibility of neuroimaging analysis by submitting a single event-related fMRI experiment to a large number of unique analysis procedures. Ten analysis steps for which multiple strategies appear in the literature were identified, and two to four strategies were enumerated for each step. Considering all possible combinations of these strategies yielded 6,912 unique analysis pipelines. Activation maps from each pipeline were corrected for multiple comparisons using five thresholding approaches, yielding 34,560 significance maps. While some outcomes were relatively consistent across pipelines, others showed substantial methods-related variability in activation strength, location, and extent. Some analysis decisions contributed to this variability more than others, and different decisions were associated with distinct patterns of variability across the brain. Qualitative outcomes also varied with analysis parameters: many contrasts yielded significant activation under some pipelines but not others. Altogether, these results reveal considerable flexibility in the analysis of fMRI experiments. This observation, when combined with mathematical simulations linking analytic flexibility with elevated false positive rates, suggests that false positive results may be more prevalent than expected in the literature. This risk of inflated false positive rates may be mitigated by constraining the flexibility of analytic choices or by abstaining from selective analysis reporting.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:28:07.675Z,Power-up: A reanalysis of ‘power failure’ in neuroscience using mixture modeling.,https://doi.org/10.1523/JNEUROSCI.3592-16.2017,"Camilla L. Nord, Vincent Valton, John Wood and Jonathan P. Roiser","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recently, evidence for endemically low statistical power has cast neuroscience findings into doubt. If low statistical power plagues neuroscience, then this reduces confidence in the reported effects. However, if statistical power is not uniformly low, then such blanket mistrust might not be warranted. Here, we provide a different perspective on this issue, analyzing data from an influential study reporting a median power of 21% across 49 meta-analyses (Button et al., 2013). We demonstrate, using Gaussian mixture modeling, that the sample of 730 studies included in that analysis comprises several subcomponents so the use of a single summary statistic is insufficient to characterize the nature of the distribution. We find that statistical power is extremely low for studies included in meta-analyses that reported a null result and that it varies substantially across subfields of neuroscience, with particularly low power in candidate gene association studies. Therefore, whereas power in neuroscience remains a critical issue, the notion that studies are systematically underpowered is not the full story: low power is far from a universal problem.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:29:52.405Z,Sample Size in Psychological Research Over the Past 30 Years,https://doi.org/10.2466/03.11,"Jacob M Marszalek, Carolyn Barber, Julie Kohlhart, Cooper B Holmes","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The American Psychological Association (APA) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading APA journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:31:44.017Z,The N-Pact Factor: Evaluating the Quality of Empirical Journals with Respect to Sample Size and Statistical Power,https://doi.org/10.1371/journal.pone.0109019,"R. Chris Fraley ,Simine Vazire ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their N-pact Factors (NF)—the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:33:54.957Z,A systematic review of statistical power in software engineering experiments. ,https://doi.org/10.1016/j.infsof.2005.08.009,"Dybå, T., Kampenes, V. B., & Sjøberg, D. I. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study’s findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:36:15.353Z,Statistical Power and the Testing of Null Hypotheses: A Review of Contemporary Management Research and Recommendations for Future Studies,https://doi.org/10.1177/1094428104263676,"Luke H. Cashen, Scott W. Geiger","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The purpose of this study is to determine how well contemporary management research fares on the issue of statistical power with regard to studies specifically predicting null relationships between phenomena of interest. This power assessment differs from traditional power studies because it focuses solely on studies that offered and tested null hypotheses. A sample of studies containing hypothesized null relationships was taken from five mainstream management journals over the 1990 to 1999 time period. Results of the power assessment suggest that management researchers’ abilities to affirm null hypotheses are low. On average, the power assessment revealed that for those studies that found nonsignificance of results and consequently affirmed their null hypotheses, the actual Type II error rate was nearly 15 times greater than what is advocated in the literature when failing to reject a false null hypothesis. Recommendations for researchers proposing and testing formal null hypotheses are also discussed",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:37:47.649Z,Statistical Power Problems with Moderated Multiple Regression in Management Research ,https://doi.org/10.1177/014920639502100607,Herman Aguinis ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Due to the increasing importance of moderating (i.e., interaction) effects, the use of moderated multiple regression (MMR) has become pervasive in numerous management specialties such as organizational behavior, human resources management, and strategy, to name a few. Despite its popularity, recent research on the MMR approach to moderator variable detection has identified several factors that reduce statistical power below acceptable levels and, consequently, lead researchers to erroneously dismiss theoretical models that include moderated relationships. The present article (1) briefly describes MMR, (2) reviews factors that affect the statistical power of hypothesis tests conducted using this technique, (3) proposes solutions to low power situations, and (4) discusses areas and problems related to MMR that are in need of further investigation.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-03T20:40:01.843Z,WHY SUMMARIES OF RESEARCH ON PSYCHOLOGICAL THEORIES ARE OFTEN UNINTERPRETABLE,https://doi.org/10.2466/pr0.1990.66.1.195,Paul Meehl,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:42:54.957Z,Choosing prediction over explanation in psychology: Lessons from machine learning.,http://doi.org/10.1017/CBO9781107415324.004,"Yarkoni, T., & Westfall, J. ","Primary Source, Reading",College / Upper Division (Undergraduates),"Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:44:44.076Z,"Many hands make tight work: Crowdsourcing research can balance discussions, validate findings and better inform policy",https://doi.org/10.1038/526189a,"Silberzahn, R., & Uhlmann, E.L. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Crowdsourcing research can balance discussions, validate findings and better inform policy, say Raphael Silberzahn and Eric L. Uhlmann.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:46:54.231Z,The pipeline project: Pre-publication independent replications of a single laboratory's research pipeline,https://doi.org/10.1016/j.jesp.2015.10.001,Martin Schweinsberg et al.,"Primary Source, Reading",College / Upper Division (Undergraduates),"This crowdsourced project introduces a collaborative approach to improving the reproducibility of scientific research, in which findings are replicated in qualified independent laboratories before (rather than after) they are published. Our goal is to establish a non-adversarial replication process with highly informative final results. To illustrate the Pre-Publication Independent Replication (PPIR) approach, 25 research groups conducted replications of all ten moral judgment effects which the last author and his collaborators had “in the pipeline” as of August 2014. Six findings replicated according to all replication criteria, one finding replicated but with a significantly smaller effect size than the original, one finding replicated consistently in the original culture but not outside of it, and two findings failed to find support. In total, 40% of the original findings failed at least one major replication criterion. Potential ways to implement and incentivize pre-publication independent replication on a large scale are discussed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:48:47.976Z,A manifesto for reproducible science,https://doi.org/10.0138/s41562-016-0021.,"Munafo, M. R., et al. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:50:36.149Z,Let's Publish Fewer Papers,https://doi.org/10.1080/1047840X.2012.705245,"Leif D. Nelson , Joseph P. Simmons & Uri Simonsohn ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about publishing few papers,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:52:02.806Z,Scientific utopia II. Restructuring incentives and practices to promote truth over publishability.,https://doi.org/10.1177/1745691612459058,"Nosek, B. A., Spies, J. R., & Motyl, M. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-03T20:53:58.304Z,Tracking replicability as a method of post-publication open evaluation. ,https://doi.org/10.3389/fncom.2012.00008,"Hartshorne, J. K., & Schachner, A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recent reports have suggested that many published results are unreliable. To increase the reliability and accuracy of published papers, multiple changes have been proposed, such as changes in statistical methods. We support such reforms. However, we believe that the incentive structure of scientific publishing must change for such reforms to be successful. Under the current system, the quality of individual scientists is judged on the basis of their number of publications and citations, with journals similarly judged via numbers of citations. Neither of these measures takes into account the replicability of the published findings, as false or controversial results are often particularly widely cited. We propose tracking replications as a means of post-publication evaluation, both to help researchers identify reliable findings and to incentivize the publication of reliable results. Tracking replications requires a database linking published studies that replicate one another. As any such database is limited by the number of replication attempts published, we propose establishing an open-access journal dedicated to publishing replication attempts. Data quality of both the database and the affiliated journal would be ensured through a combination of crowd-sourcing and peer review. As reports in the database are aggregated, ultimately it will be possible to calculate replicability scores, which may be used alongside citation counts to evaluate the quality of work published in individual journals. In this paper, we lay out a detailed description of how this system could be implemented, including mechanisms for compiling the information, ensuring data quality, and incentivizing the research community to participate.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T18:56:25.621Z,Too true to be bad: When sets of studies with significant and nonsignificant findings are probably true,https://doi.org/10.1177/1948550617693058,"Lakens, D., & Etz, A. J.","Primary Source, Reading",College / Upper Division (Undergraduates),"Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or “too good to be true”) that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or “too true to be bad.” As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T18:58:56.621Z,Benefits of Open and High-Powered Research Outweigh Costs,https://doi.org/10.1037/pspi0000049,"Etienne P. LeBel, Lorne Campbell and Timothy J. Loving","Primary Source, Reading",College / Upper Division (Undergraduates),"Several researchers recently outlined unacknowledged costs of open science practices, arguing these costs may outweigh benefits and stifle discovery of novel findings. We scrutinize these researchers’ (a) statistical concern that heightened stringency with respect to false-positives will increase false-negatives and (b) metascientific concern that larger samples and executing direct replications engender opportunity costs that will decrease the rate of making novel discoveries. We argue their statistical concern is unwarranted given open science proponents recommend such practices to reduce the inflated Type I error rate from .35 down to .05 and simultaneously call for high-powered research to reduce the inflated Type II error rate. Regarding their metaconcern, we demonstrate that incurring some costs is required to increase the rate (and frequency) of making true discoveries because distinguishing true from false hypotheses requires a low Type I error rate, high statistical power, and independent direct replications. We also examine pragmatic concerns raised regarding adopting open science practices for relationship science (preregistration, open materials, open data, direct replications, sample size); while acknowledging these concerns, we argue they are overstated given available solutions. We conclude benefits of open science practices outweigh costs for both individual researchers and the collective field in the long run, but that short term costs may exist for researchers because of the currently dysfunctional academic incentive structure. Our analysis implies our field’s incentive structure needs to change whereby better alignment exists between researcher’s career interests and the field’s cumulative progress. We delineate recent proposals aimed at such incentive structure realignment.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:00:43.218Z,Inappropriate fiddling with statistical analyses to obtain a desirable p-value: tests to detect its presence in published literature. ,https://doi.org/10.1371/journal.pone.0046363,Gary L. Gadbury and David B. Allison,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Much has been written regarding p-values below certain thresholds (most notably 0.05) denoting statistical significance and the tendency of such p-values to be more readily publishable in peer-reviewed journals. Intuition suggests that there may be a tendency to manipulate statistical analyses to push a ‘‘near significant p-value’’ to a level that is considered significant. This article presents a method for detecting the presence of such manipulation (herein called ‘‘fiddling’’) in a distribution of p-values from independent studies. Simulations are used to illustrate the properties of the method. The results suggest that the method has low type I error and that power approaches acceptable levels as the number of p-values being studied approaches 1000.,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:03:17.742Z,It’s Time to Broaden the Replicability Conversation: Thoughts for and From Clinical Psychological Science,https://doi.org/10.1177/1745691617690042,Jennifer L. Tackett et al. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. Nevertheless, several other major domains of psychological science—including clinical science—have remained insulated from this discussion. The goals of this article are to (a) examine why clinical psychology and allied fields, such as counseling and school psychology, have not been central participants in the replicability conversation; (b) review concerns and recommendations that are less (or more) applicable to or appropriate for research in clinical psychology and allied fields; and (c) generate take-home messages for scholars and consumers of the literature in clinical psychology and allied fields, as well as reviewers, editors, and colleagues from other areas of psychological science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:05:26.743Z,"The State of Social and Personality Science: Rotten to the Core, Not So Bad, Getting Better, or Getting Worse?",https://doi.org/10.1037/pspa0000084,Motyl et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field’s research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003–2004 and 2013–2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers’ intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003–2004 may not be as bad as many feared, and (d) research published in 2013–2014 shows some improvement over research published in 2003–2004, a result that suggests the field is evolving in a positive direction.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:08:03.089Z,"A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building. ",https://doi.org/10.1111/infa.12182,Frank et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The ideal of scientific progress is that we accumulate measurements and integrate these into theory, but recent discussion of replicability issues has cast doubt on whether psychological research conforms to this model. Developmental research—especially with infant participants—also has discipline-specific replicability challenges, including small samples and limited measurement methods. Inspired by collaborative replication efforts in cognitive and social psychology, we describe a proposal for assessing and promoting replicability in infancy research: large-scale, multi-laboratory replication efforts aiming for a more precise understanding of key developmental phenomena. The ManyBabies project, our instantiation of this proposal, will not only help us estimate how robust and replicable these phenomena are, but also gain new theoretical insights into how they vary across ages, linguistic communities, and measurement methods. This project has the potential for a variety of positive outcomes, including less-biased estimates of theoretically important effects, estimates of variability that can be used for later study planning, and a series of best-practices blueprints for future infancy research.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:10:16.207Z,Using science and psychology to improve the dissemination and evaluation of scientific work,https://doi.org/10.3389/fncom.2014.00082,Brett T. Buttliere,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Here I outline some of what science can tell us about the problems in psychological publishing and how to best address those problems. First, the motivation behind questionable research practices is examined (the desire to get ahead or, at least, not fall behind). Next, behavior modification strategies are discussed, pointing out that reward works better than punishment. Humans are utility seekers and the implementation of current change initiatives is hindered by high initial buy-in costs and insufficient expected utility. Open science tools interested in improving science should team up, to increase utility while lowering the cost and risk associated with engagement. The best way to realign individual and group motives will probably be to create one, centralized, easy to use, platform, with a profile, a feed of targeted science stories based upon previous system interaction, a sophisticated (public) discussion section, and impact metrics which use the associated data. These measures encourage high quality review and other prosocial activities while inhibiting self-serving behavior. Some advantages of centrally digitizing communications are outlined, including ways the data could be used to improve the peer review process. Most generally, it seems that decisions about change design and implementation should be theory and data driven.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:12:25.920Z,WHAT YOU SEE IS WHAT YOU GET? ENHANCING METHODOLOGICAL TRANSPARENCY IN MANAGEMENT RESEARCH,https://doi.org/10.5465/annals.2016.0011,"Herman Aguinis, Ravi S. Ramani and Nawaf Alabduljader","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We review the literature on evidence-based best practices on how to enhance methodological transparency, which is the degree of detail and disclosure about the specific steps, decisions, and judgment calls made during a scientific study. We conceptualize lack of transparency as a “research performance problem” because it masks fraudulent acts, serious errors, and questionable research practices, and therefore precludes inferential and results reproducibility. Our recommendations for authors provide guidance on how to increase transparency at each stage of the research process: (1) theory, (2) design, (3) measurement, (4) analysis, and (5) reporting of results. We also offer recommendations for journal editors, reviewers, and publishers on how to motivate authors to be more transparent. We group these recommendations into the following categories: (1) manuscript submission forms requiring authors to certify they have taken actions to enhance transparency, (2) manuscript evaluation forms including additional items to encourage reviewers to assess the degree of transparency, and (3) review process improvements to enhance transparency. Taken together, our recommendations provide a resource for doctoral education and training; researchers conducting empirical studies; journal editors and reviewers evaluating submissions; and journals, publishers, and professional organizations interested in enhancing the credibility and trustworthiness of research.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:14:15.524Z,An Introduction to Registered Replication Reports at Perspectives on Psychological Science,https://doi.org/10.1177/1745691614543974,"Daniel J. Simons, Alex O. Holcombe, Barbara A. Spellman","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An article about an Introduction to Registered Replication Reports at Perspectives on Psychological Science,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:18:52.809Z,Registered reports : a method to increase the credibility of published results,https://doi.org/10.1027/1864-9335/a000192,Brian Nosek and Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),Registered reports a Method to Increase the Credibility of Published Results,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:21:22.409Z,Raise standards for preclinical cancer research,https://doi.org/10.1038/483531a,C. Glenn Begley and Lee M. Ellis ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.",English,I don't see any of these,Student,Life Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:27:40.678Z,Detecting and avoiding likely false-positive findings: A practical guide,https://doi.org/10.1111/brv.12315,"Wolfgang Forstmeier, Eric-Jan Wagenmakers and Timothy H. Parker","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Recently there has been a growing concern that many published research ﬁndings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a signiﬁcant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difﬁcult to falsify. In order to pinpoint the sources of errorand possible solutions, we review current scientiﬁc practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive ﬁndings is expected to increase  with(i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher ﬂexibility,and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points(clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Speciﬁcally, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis,and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research ﬁndings of one’s own and of others for the beneﬁt of the scientiﬁc community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientiﬁc rigour",English,I don't see any of these,Student,Life Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-04T19:29:29.055Z,Easy preregistration will beneft any research,https://doi.org/10.1038/s41562-018-0294-7,David T. Mellor and Brian Nosek,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),An article about easy preregistration will benefit any research.,English,I don't see any of these,Student,"Applied Science, Social Science",Preregistration,
2020-06-04T19:31:13.021Z,Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli,https://doi.org/10.1037/xge0000014,"Jacob Westfall, David A Kenny, Charles M Judd","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:33:08.859Z,Safeguard Power as a Protection Against Imprecise Power Estimates,https://doi.org/10.1177/1745691614528519,"Marco Perugini, Marcello Gallucci, and Giulio Costantini","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:34:14.528Z,You Cannot Step Into the Same River Twice: When Power Analyses Are Optimistic,https://doi.org/10.1177/1745691614548513.,"Blakeley B McShane, Ulf Böckenholt","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistical power depends on the size of the effect of interest. However, effect sizes are rarely fixed in psychological research: Study design choices, such as the operationalization of the dependent variable or the treatment manipulation, the social context, the subject pool, or the time of day, typically cause systematic variation in the effect size. Ignoring this between-study variation, as standard power formulae do, results in assessments of power that are too optimistic. Consequently, when researchers attempting replication set sample sizes using these formulae, their studies will be underpowered and will thus fail at a greater than expected rate. We illustrate this with both hypothetical examples and data on several well-studied phenomena in psychology. We provide formulae that account for between-study variation and suggest that researchers set sample sizes with respect to our generally more conservative formulae. Our formulae generalize to settings in which there are multiple effects of interest. We also introduce an easy-to-use website that implements our approach to setting sample sizes. Finally, we conclude with recommendations for quantifying between-study variation.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:37:23.364Z,Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs,https://doi.org/10.3389/fpsyg.2013.00863,Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.",English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,
2020-06-04T19:39:07.878Z,Power Analysis and Effect Size in Mixed Effects Models: A Tutorial. ,https://doi.org/10.5334/joc.10,"Brysbaert, M. and Stevens, M.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In psychology, attempts to replicate published findings are less successful than expected. For properly powered studies replication rate should be around 80%, whereas in practice less than 40% of the studies selected from different areas of psychology can be replicated. Researchers in cognitive psychology are hindered in estimating the power of their studies, because the designs they use present a sample of stimulus materials to a sample of participants, a situation not covered by most power formulas. To remedy the situation, we review the literature related to the topic and introduce recent software packages, which we apply to the data of two masked priming studies with high power. We checked how we could estimate the power of each study and how much they could be reduced to remain powerful enough. On the basis of this analysis, we recommend that a properly powered reaction time experiment with repeated measures has at least 1,600 word observations per condition (e.g., 40 participants, 40 stimuli). This is considerably more than current practice. We also show that researchers must include the number of observations in meta-analyses because the effect sizes currently reported depend on the number of stimuli presented to the participants. Our analyses can easily be applied to new datasets gathered.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:40:47.120Z,A Power Primer,https://doi.org/10.1037/0033-2909.112.1.155,Jacob Cohen,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product–moment correlation, (3) the difference between independent rs, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation. ",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-04T19:42:36.518Z,A Powerful Nudge? Presenting Calculable Consequences of Underpowered Research Shifts Incentives Toward Adequately Powered Designs,https://doi.org/10.1177/1948550615584199,"Will M. Gervais , Jennifer A. Jewell , Maxine B. Najle , and Ben K. L. Ng","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"If psychologists have recognized the pitfalls of underpowered research for decades, why does it persist? Incentives, perhaps: underpowered research benefits researchers individually (increased productivity), but harms science collectively (inflated Type I error rates and effect size estimates but low replication rates). Yet, researchers can selectively reward power at various scientific bottlenecks (e.g., peer review, hiring, funding, and promotion). We designed a stylized thought experiment to evaluate the degree to which researchers consider power and productivity in hiring decisions. Accomplished psychologists chose between a low sample size candidate and a high sample size candidate who were otherwise identical. We manipulated the degree to which participants received information about (1) productivity, (2) sample size, and (3) directly calculable Type I error and replication rates. Participants were intolerant of the negative consequences of low-power research, yet merely indifferent regarding the practices that logically produce those consequences, unless those consequences were made quite explicit.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T18:16:31.342Z,Correlational Effect Size Benchmarks,https://doi.org/10.1037/a0038047,"Bosco, F. A., Aguinis, H., Singh, K., Field, J. G., & Pierce, C. A.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions. ",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-05T18:18:35.045Z,When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias,https://doi.org/10.1016/j.jesp.2017.09.004,Casper Albers and Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index (η2, ω2 and ε2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of η2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-05T18:23:03.111Z,Are We Wasting a Good Crisis? The Availability of Psychological Research Data after the Storm,https://doi.org/http://doi.org/10.1525/collabra.13,"Wolf Vanpaemel , Maarten Vermorgen , Leen Deriemaecker  and Gert Storms","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"To study the availability of psychological research data, we requested data from 394 papers, published in all issues of four APA journals in 2012. We found that 38% of the researchers sent their data immediately or after reminders. These findings are in line with estimates of the willingness to share data in psychology from the recent or remote past. Although the recent crisis of confidence that shook psychology has highlighted the importance of open research practices, and technical developments have greatly facilitated data sharing, our findings make clear that psychology is nowhere close to being an open science",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T18:25:25.640Z,"The what, why, and how of born-open data",https://doi.org/10.3758/s13428-015-0630-z,Jeffrey N. Rouder,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Although many researchers agree that scientific data should be open to scrutiny to ferret out poor analyses and outright fraud, most raw data sets are not available on demand. There are many reasons researchers do not open their data, and one is technical. It is often time consuming to prepare and archive data. In response, my laboratory has automated the process such that our data are archived the night they are created without any human approval or action. All data are versioned, logged, time stamped, and uploaded including aborted runs and data from pilot subjects. The archive is GitHub, github.com, the world’s largest collection of open-source materials. Data archived in this manner are called born open. In this paper, I discuss the benefits of born-open data and provide a brief technical overview of the process. I also address some of the common concerns about opening data before publication.",English,I don't see any of these,Student,"Applied Science, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials",
2020-06-05T18:28:19.538Z,Data reuse and the open data citation advantage,https://doi.org/10.7717/peerj.175,Heather A. Piwowar and Todd J. Vision,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background. Attribution to the original contributor upon reuse of published data is important both as a reward for data creators and to document the provenance of research findings. Previous studies have found that papers with publicly available datasets receive a higher number of citations than similar studies without available data. However, few previous analyses have had the statistical power to control for the many variables known to predict citation rate, which has led to uncertain estimates of the “citation benefit”. Furthermore, little is known about patterns in data reuse over time and across datasets. Method and Results. Here, we look at citation rates while controlling for many known citation predictors and investigate the variability of data reuse. In a multivariate regression on 10,555 studies that created gene expression microarray data, we found that studies that made data available in a public repository received 9% (95% confidence interval: 5% to 13%) more citations than similar studies for which the data was not made available. Date of publication, journal impact factor, open access status, number of authors, first and last author publication history, corresponding author country, institution citation history, and study topic were included as covariates. The citation benefit varied with date of dataset deposition: a citation benefit was most clear for papers published in 2004 and 2005, at about 30%. Authors published most papers using their own datasets within two years of their first publication on the dataset, whereas data reuse papers published by third-party investigators continued to accumulate for at least six years. To study patterns of data reuse directly, we compiled 9,724 instances of third party data reuse via mention of GEO or ArrayExpress accession numbers in the full text of papers. The level of third-party data use was high: for 100 datasets deposited in year 0, we estimated that 40 papers in PubMed reused a dataset by year 2, 100 by year 4, and more than 150 data reuse papers had been published by year 5. Data reuse was distributed across a broad base of datasets: a very conservative estimate found that 20% of the datasets deposited between 2003 and 2007 had been reused at least once by third parties. Conclusion. After accounting for other factors affecting citation rate, we find a robust citation benefit from open data, although a smaller one than previously reported. We conclude there is a direct effect of third-party data reuse that persists for years beyond the time when researchers have published most of the papers reusing their own data. Other factors that may also contribute to the citation benefit are considered. We further conclude that, at least for gene expression microarray data, a substantial fraction of archived datasets are reused, and that the intensity of dataset reuse has been steadily increasing since 2003.",English,CC BY,Student,"Applied Science, Social Science","Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials",
2020-06-05T19:05:30.819Z,Scientific Utopia: I. Opening Scientific Communication,https://doi.org/10.1080/1047840X.2012.692215,Brian A. Nosek & Yoav Bar-Anan,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Existing norms for scientific communication are rooted in anachronistic practices of bygone eras making them needlessly inefficient. We outline a path that moves away from the existing model of scientific communication to improve the efficiency in meeting the purpose of public science—knowledge accumulation. We call for six changes: (a) full embrace of digital communication; (b) open access to all published research; (c) disentangling publication from evaluation; (d) breaking the “one article, one journal” model with a grading system for evaluation and diversified dissemination outlets; (e) publishing peer review; and (f) allowing open, continuous peer review. We address conceptual and practical barriers to change and provide examples showing how the suggested practices are being used already. The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:09:54.895Z,A Software Tool for Removing Patient Identifying Information from Clinical Documents,https://doi.org/10.1197/jamia.M2702,"Friedlin, F. J., & McDonald, C. J.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We created a software tool that accurately removes all patient identifying information from various kinds of clinical data documents, including laboratory and narrative reports. We created the Medical De-identification System (MeDS), a software tool that de-identifies clinical documents, and performed 2 evaluations. Our first evaluation used 2,400 Health Level Seven (HL7) messages from 10 different HL7 message producers. After modifying the software based on the results of this first evaluation, we performed a second evaluation using 7,190 pathology report HL7 messages. We compared the results of MeDS de-identification process to a gold standard of human review to find identifying strings. For both evaluations, we calculated the number of successful scrubs, missed identifiers, and over-scrubs committed by MeDS and evaluated the readability and interpretability of the scrubbed messages. We categorized all missed identifiers into 3 groups: (1) complete HIPAA-specified identifiers, (2) HIPAA-specified identifier fragments, (3) non-HIPAA–specified identifiers (such as provider names and addresses). In the results of the first-pass evaluation, MeDS scrubbed 11,273 (99.06%) of the 11,380 HIPAA-specified identifiers and 38,095 (98.26%) of the 38,768 non-HIPAA–specified identifiers. In our second evaluation (status postmodification to the software), MeDS scrubbed 79,993 (99.47%) of the 80,418 HIPAA-specified identifiers and 12,689 (96.93%) of the 13,091 non-HIPAA–specified identifiers. Approximately 95% of scrubbed messages were both readable and interpretable. We conclude that MeDS successfully de-identified a wide range of medical documents from numerous sources and creates scrubbed reports that retain their interpretability, thereby maintaining their usefulness for research.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducible Analyses,Open Data and Materials",
2020-06-05T19:12:53.512Z,Making replication mainstream,https://doi.org/10.1017/S0140525X17001972,"Rolf A. Zwaan, Alexander Etz Richard E. Lucas and M. Brent Donnellan","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A finding is elevated from single observation to scientific evidence when the procedures that were used to obtain it can be reproduced and the finding itself can be replicated. Recent replication attempts show that some high profile results – most notably in psychology, but in many other disciplines as well – cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:13:59.055Z,Expectations for Replications: Are Yours Realistic?,https://doi.org/10.1177/1745691614528518,"David J Stanley 1, Jeffrey R Spence","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Failures to replicate published psychological research findings have contributed to a ""crisis of confidence."" Several reasons for these failures have been proposed, the most notable being questionable research practices and data fraud. We examine replication from a different perspective and illustrate that current intuitive expectations for replication are unreasonable. We used computer simulations to create thousands of ideal replications, with the same participants, wherein the only difference across replications was random measurement error. In the first set of simulations, study results differed substantially across replications as a result of measurement error alone. This raises questions about how researchers should interpret failed replication attempts, given the large impact that even modest amounts of measurement error can have on observed associations. In the second set of simulations, we illustrated the difficulties that researchers face when trying to interpret and replicate a published finding. We also assessed the relative importance of both sampling error and measurement error in producing variability in replications. Conventionally, replication attempts are viewed through the lens of verifying or falsifying published findings. We suggest that this is a flawed perspective and that researchers should adjust their expectations concerning replications and shift to a meta-analytic mind-set.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:16:04.774Z,‘‘Positive’’ Results Increase Down the Hierarchy of the Sciences,https://doi.org/10.1371/journal.pone.0010068,Daniele Fanelli,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:18:22.188Z,Effect of open peer review on quality of reviews and on reviewers'recommendations: a randomised trial,https://doi.org/10.1136/bmj.318.7175.23,Susan van Rooyen et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Objectives: To examine the effect on peer review of asking reviewers to have their identity revealed to the authors of the paper. Design: Randomised trial. Consecutive eligible papers were sent to two reviewers who were randomised to have their identity revealed to the authors or to remain anonymous. Editors and authors were blind to the intervention. Main outcome measures: The quality of the reviews was independently rated by two editors and the corresponding author using a validated instrument. Additional outcomes were the time taken to complete the review and the recommendation regarding publication. A questionnaire survey was undertaken of the authors of a cohort of manuscripts submitted for publication to find out their views on open peer review. Results: Two editors' assessments were obtained for 113out of 125 manuscripts, and the corresponding author's assessment was obtained for 105.Reviewers randomised to be asked to be identified were 12% (95% confidence interval 0.2% to 24%) more likely to decline to review than reviewers randomised to remain anonymous (35% v 23%). There was no significant difference in quality (scored on a scale of 1to 5) between anonymous reviewers (3.06(SD 0.72)) and identified reviewers (3.09(0.68)) (P=0.68, 95% confidence interval for difference −align=baseline>0.19 to 0.12), and no significant difference in the recommendation regarding publication or time taken to review the paper. The editors' quality score for reviews (3.05(SD 0.70)) was significantly higher than that of authors (2.90(0.87))(P<0.005, 95%confidence interval for difference − align=baseline>0.26 to − align=baseline>0.03). Most authors were in favour of open peer review.Conclusions: Asking reviewers to consent to being identified to the author had no important effect on the quality of the review, the recommendation regarding publication, or the time taken to review, but it significantly increased the likelihood of reviewers declining to review.",English,I don't see any of these,Student,"Applied Science, Social Science",,"Transparency,Open Science"
2020-06-05T19:19:33.825Z,"Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology.",https://doi.org/10.1037/0022-006X.46.4.806,"Meehl, P. E","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Theories in ""soft"" areas of psychology (e.g., clinical, counseling, social, personality, school, and community) lack the cumulative character of scientific knowledge because they tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest. Even though intrinsic subject matter difficulties (20 are listed) contribute to this, the excessive reliance on significance testing is partly responsible (Ronald A. Fisher). Karl Popper's approach, with modifications, would be prophylactic. Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of ""significant differences"" are little more than complex, causally uninterpretable outcomes of statistical power functions. Multiple paths to estimating numerical point values (""consistency tests"") are better, even if approximate with rough tolerances; and lacking this, ranges, orderings, 2nd-order differences, curve peaks and valleys, and function forms should be used. Such methods are usual in developed sciences that seldom report statistical significance. Consistency tests of a conjectural taxometric model yielded 94% success with no false negatives. ",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:20:42.716Z,Why Most Discovered True Associations Are Inflated,https://doi.org/10.1097/EDE.0b013e31818131e7,John P A Ioannidis,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated-for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:23:03.318Z,Statistical Procedures and the Justification of Knowledge in Psychological Science ,https://doi.org/10.1037/0003-066X.44.10.1276,"Rosnow, R.L., & Rosenthal, R. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Justification, in the vernacular language of philosophy of science, refers to the evaluation, defense, and confirmation of claims of truth. In this article, we examine some aspects of the rhetoric of justification, which in part draws on statistical data analysis to shore up facts and inductive inferences. There are a number of problems of methodological spirit and substance that in the past have been resistant to attempts to correct them. The major problems are discussed, and readers are reminded of ways to clear away these obstacles to justification.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:25:50.192Z,"Replication, Communication, and the Population Dynamics of Scientific Discovery",https://doi.org/10.1371/journal.pone.0136088,"Richard McElreath, Paul E. Smaldino","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts—suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:27:33.056Z,"Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency",https://doi.org/10.1371/journal.pbio.1002456,Mallory C. Kidwell et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3% of Psychological Science articles reported open data. After badges, 23% reported open data, with an accelerating trend; 39% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:29:27.056Z,Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time,https://doi.org/10.1371/journal.pone.0132382,"Kaplan RM, Irvin VL ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Background: We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. Methods: We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs >$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results: 17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions: The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.",English,I don't see any of these,Student,Life Science,"Reproducibility and Replicability Knowledge,Preregistration,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science,Transparency"
2020-06-05T19:31:32.930Z,Rewarding Replications: A Sure and Simple Way to Improve Psychological Science,https://doi.org/10.1177/1745691612462586,"Sander L Koole, Daniël Lakens","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Although replications are vital to scientific progress, psychologists rarely engage in systematic replication efforts. In this article, we consider psychologists’ narrative approach to scientific publications as an underlying reason for this neglect and propose an incentive structure for replications within psychology. First, researchers need accessible outlets for publishing replications. To accomplish this, psychology journals could publish replication reports in files that are electronically linked to reports of the original research. Second, replications should get cited. This can be achieved by cociting replications along with original research reports. Third, replications should become a valued collaborative effort. This can be realized by incorporating replications in teaching programs and by stimulating adversarial collaborations. The proposed incentive structure for replications can be developed in a relatively simple and cost-effective manner. By promoting replications, this incentive structure may greatly enhance the dependability of psychology’s knowledge base.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-05T19:33:05.179Z,Statistical methods in psychology journals: Guidelines and explanations.,https://doi.org/10.1037/0003-066X.54.8.594,"Wilkinson, L., & Task Force on Statistical Inference, American Psychological Association, Science Directorate.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In the light of continuing debate over the applications of significance testing in psychology journals and following the publication of J. Cohen's (1994) article, the Board of Scientific Affairs (BSA) of the American Psychological Association (APA) convened a committee called the Task Force on Statistical Interference (TFSI) whose charge was ""to elucidate some of the controversial issues surrounding applications of statistics including significance testing and its alternatives; alternative underlying models and data transformation; and newer methods made possible by powerful computers"" (BSA, personal communication, February 28, 1996). After extensive discussion, the BSA recommended that publishing an article in American Psychologist, as a way to initiate discussion in the field about changes in current practices of data analysis and reporting may be appropriate. This report follows that request. Following each guideline are comments, explanations, or elaborations assembled by L. Wilkinson for the task force and under its review. The report is concerned with the use of statistical methods only and is not meant as an assessment of research methods in general. The title and format of the report are adapted from an article by J. C. Bailar and F. Mosteller (1988).",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-05T19:34:31.478Z,Is the call to abandon p-values the red herring of the replicability crisis? ,https://doi.org/10.3389/fpsyg.2015.00245,Victoria Savalei and Elizabeth Dunn,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about Is the call to abandon p-values the red herring of the replicability crisis?,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-06T18:53:30.916Z,Conservative tests under satisficing models of publication bias.,https://doi.org/10.1371/journal.pone.0149590,"Justin McCrary ,Garret Christensen ,Daniele Fanelli","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication bias leads consumers of research to observe a selected sample of statistical estimates calculated by producers of research. We calculate critical values for statistical significance that could help to adjust after the fact for the distortions created by this selection effect, assuming that the only source of publication bias is file drawer bias. These adjusted critical values are easy to calculate and differ from unadjusted critical values by approximately 50%—rather than rejecting a null hypothesis when the t-ratio exceeds 2, the analysis suggests rejecting a null hypothesis when the t-ratio exceeds 3. Samples of published social science research indicate that on average, across research fields, approximately 30% of published t-statistics fall between the standard and adjusted cutoffs.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T18:56:09.990Z," Hail the impossible: p-values, evidence, and likelihood. ",https://doi.org/10.1111/j.1467-9450.2010.00852.x,"Johansson, T.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Significance testing based on p-values is standard in psychological research and teaching. Typically, research articles and textbooks present and use p as a measure of statistical evidence against the null hypothesis (the Fisherian interpretation), although using concepts and tools based on a completely different usage of p as a tool for controlling long-term decision errors (the Neyman–Pearson interpretation). There are four major problems with using p as a measure of evidence and these problems are often overlooked in the domain of psychology. First, p is uniformly distributed under the null hypothesis and can therefore never indicate evidence for the null. Second, p is conditioned solely on the null hypothesis and is therefore unsuited to quantify evidence, because evidence is always relative in the sense of being evidence for or against a hypothesis relative to another hypothesis. Third, p designates probability of obtaining evidence (given the null), rather than strength of evidence. Fourth, p depends on unobserved data and subjective intentions and therefore implies, given the evidential interpretation, that the evidential strength of observed data depends on things that did not happen and subjective intentions. In sum, using p in the Fisherian sense as a measure of statistical evidence is deeply problematic, both statistically and conceptually, while the Neyman–Pearson interpretation is not about evidence at all. In contrast, the likelihood ratio escapes the above problems and is recommended as a tool for psychologists to represent the statistical evidence conveyed by obtained data relative to two hypotheses",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-06T18:57:55.560Z,The fickle P value generates irreproducible results,https://doi.org/10.1038/nmeth.3288,"Lewis G Halsey, Douglas Curran-Everett, Sarah L Vowler & Gordon B Drummond","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-06T18:59:43.953Z,Mindless statistics,https://doi.org/10.1016/j.socec.2004.09.033,Gerd Gigerenzer,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:01:12.709Z,Things I have learned (so far). ,https://doi.org/10.1037/10109-028,Jacob Cohen,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates)," This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles ""less is more"" (fewer variables, more highly targeted issues, sharp rounding off), ""simple is better"" (graphic representation, unit weighting for linear composites), and ""some things you learn aren't so."" I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:03:16.404Z,On the Surprising Longevity of Flogged Horses: Why There Is a Case for the Significance Test,https://doi.org/10.1111/j.1467-9280.1997.tb00536.x,Robert P. Abelson,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Criticisms of null-hypothesis significance tests (NHSTs) are reviewed. Used as formal, two-valued decision procedures, they often generate misleading conclusions. However, critics who argue that NHSTs are totally meaningless because the null hypothesis is virtually always false are overstating their case. Critics also neglect the whole class of valuable significance tests that assess goodness of fit of models to data. Even as applied to simple mean differences, NHSTs can be rhetorically useful in defending research against criticisms that random factors adequately explain the results, or that the direction of mean difference was not demonstrated convincingly. Principled argument and counterargument produce the lore, or communal understanding, in a field, which in turn helps guide new research. Alternative procedures--confidence intervals, effect sizes, and meta-analysis--are discussed. Although these alternatives are not totally free from criticism either, they deserve more frequent use, without an unwise ban on NHSTs.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:05:13.758Z,Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods,https://doi.org/10.1177/2515245919847196,"Evan C. Carter, Felix D. Schönbrodt, Will M. Gervais, and Joseph Hilgard","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:06:47.916Z,"Short, Sweet, and Problematic? The Rise of the Short Report in Psychological Science",https://doi.org/10.1177/1745691611427304,"Alison Ledgerwood, Jeffrey W. Sherman","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Our field has witnessed a rapid increase in the appeal and prevalence of the short report format over the last two decades. In this article, we discuss both the benefits and drawbacks of the trend toward shorter and faster publications. Although the short report format can help us cope with ever-increasing time constraints; ease the burden on hiring, promotion, and tenure committees; speed the publication of our findings; and promote the dissemination of research beyond the borders of our discipline, it can also exacerbate problems with publication bias and selective reporting, decrease theoretical integration within our science, and risk overemphasizing colorful effects relative to basic processes. In the face of these challenges, we believe it is essential to find ways to preserve the advantages of the short-and-fast approach while minimizing its disadvantages and while acknowledging the complementary and critical importance of longer articles in advancing the field.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:09:00.928Z,Science or art? How aesthetic standards grease the way through the publication bottleneck but undermine science.,https://doi.org/10.1177/1745691612457576,"Giner-Sorolla, R. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The current crisis in psychological research involves issues of fraud, replication, publication bias, and false positive results. I argue that this crisis follows the failure of widely adopted solutions to psychology’s similar crisis of the 1970s. The untouched root cause is an information-economic one: Too many studies divided by too few publication outlets equals a bottleneck. Articles cannot pass through just by showing theoretical meaning and methodological rigor; their results must appear to support the hypothesis perfectly. Consequently, psychologists must master the art of presenting perfect-looking results just to survive in the profession. This favors aesthetic criteria of presentation in a way that harms science’s search for truth. Shallow standards of statistical perfection distort analyses and undermine the accuracy of cumulative data; narrative expectations encourage dishonesty about the relationship between results and hypotheses; criteria of novelty suppress replication attempts. Concerns about truth in research are emerging in other sciences and may eventually descend on our heads in the form of difficult and insensitive regulations. I suggest a more palatable solution: to open the bottleneck, putting structures in place to reward broader forms of information sharing beyond the exquisite art of present-day journal publication.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:12:14.005Z,Defining and distinguishing validity: Interpretations of score meaning and justifications of test use.,https://doi.org/10.1037/a0026975,G.J. Cizek,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The concept of validity has suffered because the term has been used to refer to 2 incompatible concerns: the degree of support for specified interpretations of test scores (i.e., intended score meaning) and the degree of support for specified applications (i.e., intended test uses). This article has 3 purposes: (a) to provide a brief summary of current validity theory, (b) to illustrate the incompatibility of incorporating score meaning and score use into a single concept, and (c) to propose and describe a framework that both accommodates and differentiates validation of test score inferences and justification of test use.",English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:14:34.538Z,"Peer-review practices of psychological journals: The fate of published articles, submitted again. ",https://doi.org/https://doi.org/10.1017/S0140525X00011183,"Peters, D. P., & Ceci, S. J. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A growing interest in and concern about the adequacy and fairness of modern peer-review practices in publication and funding are apparent across a wide range of scientific disciplines. Although questions about reliability, accountability, reviewer bias, and competence have been raised, there has been very little direct research on these variables. The present investigation was an attempt to study the peer-review process directly, in the natural setting of actual journal referee evaluations of submitted manuscripts. As test materials we selected 12 already published research articles by investigators from prestigious and highly productive American psychology departments, one article from each of 12 highly regarded and widely read American psychology journals with high rejection rates (80%) and nonblind refereeing practices. With fictitious names and institutions substituted for the original ones (e.g., Tri-Valley Center for Human Potential), the altered manuscripts were formally resubmitted to the journals that had originally refereed and published them 18 to 32 months earlier. Of the sample of 38 editors and reviewers, only three (8%) detected the resubmissions. This result allowed nine of the 12 articles to continue through the review process to receive an actual evaluation: eight of the nine were rejected. Sixteen of the 18 referees (89%) recommended against publication and the editors concurred. The grounds for rejection were in many cases described as “serious methodological flaws.” A number of possible interpretations of these data are reviewed and evaluated.",English,I don't see any of these,Student,"Applied Science, Social Science",,"Open Science,Open Review"
2020-06-06T19:16:27.806Z,HARKing: How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results?,https://doi.org/10.1007/s10869-017-9524-7,Kevin R. Murphy & Herman Aguinis ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The practice of hypothesizing after results are known (HARKing) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of HARKing behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When HARKing involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, HARKing has only a small effect on estimates of the population effect size. When HARKing involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, HARKing produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of HARKing behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of HARKing in future research.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:18:18.085Z,"Better P-curves: Making P-curve Analysis More Robust to Errors, Fraud, and Ambitious P-hacking, a Reply to Ulrich and Miller (2015)",https://doi.org/10.1037/xge0000104.,"Uri Simonsohn, Joseph P Simmons, Leif D Nelson","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"When studies examine true effects, they generate right-skewed p-curves, distributions of statistically significant results with more low (.01 s) than high (.04 s) p values. What else can cause a right-skewed p-curve? First, we consider the possibility that researchers report only the smallest significant p value (as conjectured by Ulrich & Miller, 2015), concluding that it is a very uncommon problem. We then consider more common problems, including (a) p-curvers selecting the wrong p values, (b) fake data, (c) honest errors, and (d) ambitiously p-hacked (beyond p < .05) results. We evaluate the impact of these common problems on the validity of p-curve analysis, and provide practical solutions that substantially increase its robustness.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:20:29.153Z,Big Correlations in Little Studies: Inflated fMRI Correlations Reflect Low Statistical Power-Commentary on Vul Et Al. (2009),https://doi.org/10.1111/j.1745-6924.2009.01127.x,Tal Yarkoni,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Vul, Harris, Winkielman, and Pashler (2009), (this issue) argue that correlations in many cognitive neuroscience studies are grossly inflated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.'s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inflated correlations in whole-brain fMRI analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.'s conclusions, the simulations presented suggest that the level of inflation may be even worse than Vul et al.'s empirical analysis would suggest.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:22:48.150Z,"Statistical Power, Sample Size, and Their Reporting in Randomized Controlled Trials",https://doi.org/10.1001/jama.1994.03520020048013,"David Moher, MSc; Corinne S. Dulberg, PhD, MPH; George A. Wells, PhD","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Objective.  —To describe the pattern over time in the level of statistical power and the reporting of sample size calculations in published randomized controlled trials (RCTs) with negative results. Design.  —Our study was a descriptive survey. Power to detect 25% and 50% relative differences was calculated for the subset of trials with negative results in which a simple two-group parallel design was used. Criteria were developed both to classify trial results as positive or negative and to identify the primary outcomes. Power calculations were based on results from the primary outcomes reported in the trials. Population.  —We reviewed all 383 RCTs published in JAMA, Lancet, and the New England Journal of Medicine in 1975, 1980, 1985, and 1990. Results.  —Twenty-seven percent of the 383 RCTs (n=102) were classified as having negative results. The number of published RCTs more than doubled from 1975 to 1990, with the proportion of trials with negative results remaining fairly stable. Of the simple two-group parallel design trials having negative results with dichotomous or continuous primary outcomes (n=70), only 16% and 36% had sufficient statistical power (80%) to detect a 25% or 50% relative difference, respectively. These percentages did not consistently increase overtime. Overall, only 32% of the trials with negative results reported sample size calculations, but the percentage doing so has improved over time from 0% in 1975 to 43% in 1990. Only 20 of the 102 reports made any statement related to the clinical significance of the observed differences. Conclusions.  —Most trials with negative results did not have large enough sample sizes to detect a 25% or a 50% relative difference. This result has not changed over time. Few trials discussed whether the observed differences were clinically important. There are important reasons to change this practice. The reporting of statistical power and sample size also needs to be improved",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-06T19:24:43.970Z,P-curve: A key to the file-drawer.,https://doi.org/10.1037/a0033242,"Simonsohn, Uri,Nelson, Leif D.,Simmons, Joseph P.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work,” readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps < .05). Because only true effects are expected to generate right-skewed p-curves—containing more low (.01s) than high (.04s) significant p values—only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. ",English,I don't see any of these,Student,"Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:27:39.827Z,The relation between statistical power and inference in fMRI.,https://doi.org/10.1371/journal.pone.0184923,"Cremers, H. R., Wager, T. D., & Yarkoni, T. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Statistically underpowered studies can result in experimental failure even when all other experimental considerations have been addressed impeccably. In fMRI the combination of a large number of dependent variables, a relatively small number of observations (subjects), and a need to correct for multiple comparisons can decrease statistical power dramatically. This problem has been clearly addressed yet remains controversial—especially in regards to the expected effect sizes in fMRI, and especially for between-subjects effects such as group comparisons and brain-behavior correlations. We aimed to clarify the power problem by considering and contrasting two simulated scenarios of such possible brain-behavior correlations: weak diffuse effects and strong localized effects. Sampling from these scenarios shows that, particularly in the weak diffuse scenario, common sample sizes (n = 20–30) display extremely low statistical power, poorly represent the actual effects in the full sample, and show large variation on subsequent replications. Empirical data from the Human Connectome Project resembles the weak diffuse scenario much more than the localized strong scenario, which underscores the extent of the power problem for many studies. Possible solutions to the power problem include increasing the sample size, using less stringent thresholds, or focusing on a region-of-interest. However, these approaches are not always feasible and some have major drawbacks. The most prominent solutions that may help address the power problem include model-based (multivariate) prediction methods and meta-analyses with related synthesis-oriented approaches.",English,I don't see any of these,Student,"Life Science, Math & Statistics","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:30:50.930Z,Investigation and its discontents: Some constraints on progress in psychological research. ,https://doi.org/10.1037/0003-066X.35.5.399,Paul L. Wachtel,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Examines several prominent trends in the conduct of psychological research and considers how they may limit progress in the field. Failure to appreciate important differences in temperament among researchers, and differences in the particular talents researchers bring to their work have prevented the development in psychology of a vigorous tradition of fruitful theoretical inquiry. Misplaced emphasis on quantitative ""productivity,"" a problem for all disciplines, is shown to have particularly unfortunate results in psychology. Problems associated with the distorting effects of seeking grant support are shown to interact with the first two difficulties. Finally, the distorting effects of certain kinds of experimental studies are discussed, together with their implications for progress in this field.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:33:27.577Z,Chaos in the brickyard,https://doi.org/10.1126/science.142.3590.339,"Forscher, B. K.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about open science and novelty,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-06T19:36:22.578Z,"Psychology, Science, and Knowledge Construction: Broadening Perspectives from the Replication Crisis",https://doi.org/10.1146/annurev-psych-122216-011845,Patrick E. Shrout and Joseph L. Rodgers,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T18:34:55.042Z,Strong inference,https://doi.org/10.1126/science.146.3642.347,John R Platt,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),certain systematic methods of scientific thinking may produce much more rapid progress than others.,English,I don't see any of these,Student,"Applied Science, Life Science, Physical Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T18:36:25.638Z,Best practices in data analysis and sharing in neuroimaging using MRI,https://doi.org/10.1038/nn.4500,Thomas E. Nichols et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Given concerns about the reproducibility of scientific findings, neuroimaging must define best practices for data analysis, results reporting, and algorithm and data sharing to promote transparency, reliability and collaboration. We describe insights from developing a set of recommendations on behalf of the Organization for Human Brain Mapping and identify barriers that impede these practices, including how the discipline must change to fully exploit the potential of the world’s neuroimaging data.",English,I don't see any of these,Student,Life Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T18:37:56.242Z,Optimizing Research Payoff,https://doi.org/10.1177/1745691616649170,Jeff Miller and Rolf Ulrich,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In this article, we present a model for determining how total research payoff depends on researchers’ choices of sample sizes, α levels, and other parameters of the research process. The model can be used to quantify various tradeoffs inherent in the research process and thus to balance competing goals, such as (a) maximizing both the number of studies carried out and also the statistical power of each study, (b) minimizing the rates of both false positive and false negative findings, and (c) maximizing both replicability and research efficiency. Given certain necessary information about a research area, the model can be used to determine the optimal values of sample size, statistical power, rate of false positives, rate of false negatives, and replicability, such that overall research payoff is maximized. More specifically, the model shows how the optimal values of these quantities depend upon the size and frequency of true effects within the area, as well as the individual payoffs associated with particular study outcomes. The model is particularly relevant within current discussions of how to optimize the productivity of scientific research, because it shows which aspects of a research area must be considered and how these aspects combine to determine total research payoff.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T18:39:54.479Z,Psychology’s Replication Crisis and the Grant Culture: Righting the Ship,https://doi.org/10.1177/1745691616687745,Scott O.  Lilienfeld,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The past several years have been a time for soul searching in psychology, as we have gradually come to grips with the reality that some of our cherished findings are less robust than we had assumed. Nevertheless, the replication crisis highlights the operation of psychological science at its best, as it reflects our growing humility. At the same time, institutional variables, especially the growing emphasis on external funding as an expectation or de facto requirement for faculty tenure and promotion, pose largely unappreciated hazards for psychological science, including (a) incentives for engaging in questionable research practices, (b) a single-minded focus on programmatic research, (c) intellectual hyperspecialization, (d) disincentives for conducting direct replications, (e) stifling of creativity and intellectual risk taking, (f) researchers promising more than they can deliver, and (g) diminished time for thinking deeply. Preregistration should assist with (a), but will do little about (b) through (g). Psychology is beginning to right the ship, but it will need to confront the increasingly deleterious impact of the grant culture on scientific inquiry",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T18:43:00.336Z,Ten Simple Rules for Effective Statistical Practice,https://doi.org/10.1371/journal.pcbi.1004961,Robert E.Kass et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A paper about Ten Simple Rules for Effective Statistical Practice,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T18:45:22.803Z,Mini Meta-Analysis of Your Own Studies: Some Arguments on Why and a Primer on How,https://doi.org/10.1111/spc3.12267,"Jin X. Goh, Judith A. Hall and Robert Rosenthal","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We outline the need to, and provide a guide on how to, conduct a meta-analysis on one’s own studies within a manuscript. Although conducting a “mini meta” within one’s manuscript has been argued for in the past, this practice is still relatively rare and adoption is slow. We believe two deterrents are responsible. First, researchers may not think that it is legitimate to do a meta-analysis on a small number of studies. Second, researchers may think a meta-analysis is too complicated to do without expert knowledge or guidance. We dispel these two misconceptions by (1) offering arguments on why researchers should be encouraged to do mini metas, (2) citing previous articles that have conducted such analyses to good effect, and (3) providing a user-friendly guide on calculating some meta-analytic procedures that are appropriate when there are only a few studies. We provide formulas for calculating effect sizes and converting effect sizes from one metric to another (e.g., from Cohen’s d to r), as well as annotated Excel spreadsheets and a step-by-step guide on how to conduct a simple meta-analysis. A series of related studies can be strengthened and better understood if accompanied by a mini meta-analysis.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T18:48:21.209Z,Data Sharing in Psychology: A Survey on Barriers and Preconditions,https://doi.org/10.1177/2515245917751886,Houtkoop et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Despite its potential to accelerate academic progress in psychological science, public data sharing remains relatively uncommon. In order to discover the perceived barriers to public data sharing and possible means for lowering them, we conducted a survey, which elicited responses from 600 authors of articles in psychology. The results confirmed that data are shared only infrequently. Perceived barriers included respondents’ belief that sharing is not a common practice in their fields, their preference to share data only upon request, their perception that sharing requires extra work, and their lack of training in sharing data. Our survey suggests that strong encouragement from institutions, journals, and funders will be particularly effective in overcoming these barriers, in combination with educational materials that demonstrate where and how data can be shared effectively.",English,I don't see any of these,Student,Math & Statistics,"Reproducible Analyses,Open Data and Materials",
2020-06-07T19:01:57.951Z,Prediction Interval: What to Expect When You’re Expecting … A Replication,https://doi.org/10.1371/journal.pone.0162874,Jeffrey R.Spence and David J. Stanley,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A challenge when interpreting replications is determining whether the results of a replication “successfully” replicate the original study. Looking for consistency between two studies is challenging because individual studies are susceptible to many sources of error that can cause study results to deviate from each other and the population effect in unpredictable directions and magnitudes. In the current paper, we derive methods to compute a prediction interval, a range of results that can be expected in a replication due to chance (i.e., sampling error), for means and commonly used indexes of effect size: correlations and d-values. The prediction interval is calculable based on objective study characteristics (i.e., effect size of the original study and sample sizes of the original study and planned replication) even when sample sizes across studies are unequal. The prediction interval provides an a priori method for assessing if the difference between an original and replication result is consistent with what can be expected due to sample error alone. We provide open-source software tools that allow researchers, reviewers, replicators, and editors to easily calculate prediction intervals.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T19:04:41.241Z,Replicating Studies in Which Samples of Participants Respond to Samples of Stimuli,https://doi.org/10.1177/1745691614564879,"Jacob Westfall, Charles M. Judd, David A. Kenny","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In a direct replication, the typical goal is to reproduce a prior experimental result with a new but comparable sample of participants in a high-powered replication study. Often in psychology, the research to be replicated involves a sample of participants responding to a sample of stimuli. In replicating such studies, we argue that the same criteria should be used in sampling stimuli as are used in sampling participants. Namely, a new but comparable sample of stimuli should be used to ensure that the original results are not due to idiosyncrasies of the original stimulus sample, and the stimulus sample must often be enlarged to ensure high statistical power. In support of the latter point, we discuss the fact that in experiments involving samples of stimuli, statistical power typically does not approach 1 as the number of participants goes to infinity. As an example of the importance of sampling new stimuli, we discuss the bygone literature on the risky shift phenomenon, which was almost entirely based on a single stimulus sample that was later discovered to be highly unrepresentative. We discuss the use of both resampled and expanded stimulus sets, that is, stimulus samples that include the original stimuli plus new stimuli.",English,I don't see any of these,Student,"Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research",Reproducibility Crisis and Credibility Revolution
2020-06-07T19:08:02.926Z,Privacy and Data-Based Research,https://doi.org/10.1257/jep.28.2.75,"Heffetz, Ori, and Katrina Liggett.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"What can we, as users of microdata, formally guarantee to the individuals (or firms) in our dataset, regarding their privacy? We retell a few stories, well-known in data-privacy circles, of failed anonymization attempts in publicly released datasets. We then provide a mostly informal introduction to several ideas from the literature on differential privacy, an active literature in computer science that studies formal approaches to preserving the privacy of individuals in statistical databases. We apply some of its insights to situations routinely faced by applied economists, emphasizing big-data contexts.",English,I don't see any of these,Student,Social Science,"Reproducible Analyses,Open Data and Materials",
2020-06-07T19:09:59.434Z,The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review. ,https://doi.org/10.1098/rsos.150547,Richard Morey et al. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and meta-analysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism—reviewers withholding comprehensive review—to achieve the goal of creating the expectation of open practices as a matter of scientific principle.",English,I don't see any of these,Student,"Applied Science, Social Science",,"Transparency,Peer-Review"
2020-06-07T19:14:30.269Z,Promoting an open research culture,https://doi.org/10.1126/science.aab2374,Nosek et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Author guidelines for journals could help to promote transparency, openness, and reproducibility",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T19:16:02.933Z,Researchers’ Intuitions About Power in Psychological Research,https://doi.org/10.1177/0956797616647519,Bakker et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Many psychology studies are statistically underpowered. In part, this may be because many researchers rely on intuition, rules of thumb, and prior practice (along with practical considerations) to determine the number of subjects to test. In Study 1, we surveyed 291 published research psychologists and found large discrepancies between their reports of their preferred amount of power and the actual power of their studies (calculated from their reported typical cell size, typical effect size, and acceptable alpha). Furthermore, in Study 2, 89% of the 214 respondents overestimated the power of specific research designs with a small expected effect size, and 95% underestimated the sample size needed to obtain .80 power for detecting a small effect. Neither researchers’ experience nor their knowledge predicted the bias in their self-reported power intuitions. Because many respondents reported that they based their sample sizes on rules of thumb or common practice in the field, we recommend that researchers conduct and report formal power analyses for their studies.",English,I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-07T19:17:39.866Z,Using prediction markets to estimate the reproducibility of scientific research,https://doi.org/10.1073/pnas.1516179112,Anna Drebner et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants’ individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9%) and that a “statistically significant” finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T19:25:08.972Z,Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors,https://doi.org/10.1177/1745691614551642,Andrew Gelman1 and John Carlin,"Primary Source, Reading",College / Upper Division (Undergraduates),"Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-07T19:27:16.108Z,Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals,https://doi.org/10.1037/1082-989X.11.4.363,"Ken Kelley 1, Joseph R Rausch","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Methods for planning sample size (SS) for the standardized mean difference so that a narrow confidence interval (CI) can be obtained via the accuracy in parameter estimation (AIPE) approach are developed. One method plans SS so that the expected width of the CI is sufficiently narrow. A modification adjusts the SS so that the obtained CI is no wider than desired with some specified degree of certainty (e.g., 99% certain the 95% CI will be no wider than omega). The rationale of the AIPE approach to SS planning is given, as is a discussion of the analytic approach to CI formation for the population standardized mean difference. Tables with values of necessary SS are provided. The freely available Methods for the Behavioral, Educational, and Social Sciences (K. Kelley, 2006a) R (R Development Core Team, 2006) software package easily implements the methods discussed.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-07T19:29:25.553Z,"Effect Sizes: Why, When, and How to Use Them",https://doi.org/10.1027/0044-3409.217.1.6,"Rosnow, R. L., & Rosenthal, R. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The effect size (ES) is the magnitude of a study outcome or research finding, such as the strength of the relationship obtained between an independent variable and a dependent variable. Two types of ES indicators are sampled here: the difference-type and the correlational (or r-type). Both are well suited to situations in which there are two groups or two conditions, whereas the r-type, used in association with focused statistical procedures (contrasts), is also ideal in situations where there are more than two groups or conditions and there are predicted overall patterns to be evaluated. Also discussed are procedures for computing confidence intervals and null-counternull intervals as well as a systematic approach to comparing and combining competing predictions expressed in the form of contrast weights and ES indicators.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T19:31:18.607Z,Interpreting effect sizes: Toward a quantitative cumulative social psychology,https://doi.org/10.1002/ejsp.2019,ARTHUR A. STUKAS AND GEOFF CUMMING,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Improved research practice is based on estimation of effect sizes rather than statistical significance. We discuss the challenging task of interpreting effect sizes in the research context, with particular attention to social psychological research. We emphasize the need to acknowledge the uncertainty in an effect size estimate, as signaled by the confidence interval. Interpretation must consider the independent variables, participants, measures, and other aspects of the research. Comparison with other results in the research field, and consideration of theoretical and practical implications are useful strategies. Researchers should consider the possible value of agreeing on benchmarks to help guide effect size interpretation, at least within focused research fields. More broadly, researchers should wherever possible think of experimental manipulations as well as results in quantitative terms. Doing so is fundamental for designing ingenious, informative experiments, understanding research results and their implications, developing theory, and building a quantitative cumulative social psychology",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-07T19:33:16.857Z,"Fishing, Commitment, and Communication: A Proposal for Comprehensive Nonbinding Research Registration",https://doi.org/10.1093/pan/mps021,"Macartan Humphreys, Raul Sanchez de la Sierra  and Peter van der Windt","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Social scientists generally enjoy substantial latitude in selecting measures and models for hypothesis testing. Coupled with publication and related biases, this latitude raises the concern that researchers may intentionally or unintentionally select models that yield positive findings, leading to an unreliable body of published research. To combat this “fishing” problem in medical studies, leading journals now require preregistration of designs that emphasize the prior identification of dependent and independent variables. However, we demonstrate here that even with this level of advanced specification, the scope for fishing is considerable when there is latitude over selection of covariates, subgroups, and other elements of an analysis plan. These concerns could be addressed through the use of a form of comprehensive registration. We experiment with such an approach in the context of an ongoing field experiment for which we drafted a complete “mock report” of findings using fake data on treatment assignment. We describe the advantages and disadvantages of this form of registration and propose that a comprehensive but nonbinding approach be adopted as a first step to combat fishing by social scientists. Likely effects of comprehensive but nonbinding registration are discussed, the principal advantage being communication rather than commitment, in particular that it generates a clear distinction between exploratory analyses and genuine tests.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-07T19:35:51.581Z,Pre-registration in social psychology—A discussion and suggested template.,https://doi.org/10.1016/j.jesp.2016.03.004,"van ‘t Veer, A.E., & Giner-Sorolla, R.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied—reviewed and unreviewed pre-registration—and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts. (",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Preregistration",Transparency
2020-06-08T18:04:03.232Z,Instead of “playing the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond. ,https://doi.org/10.3934/Neuroscience2014.1.4,"Chambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The last ten years have witnessed increasing awareness of questionable research practices (QRPs) in the life sciences [1,2], including p-hacking [3], HARKing [4], lack of replication [5], publication bias [6], low statistical power [7] and lack of data sharing ([8]; see Figure 1). Concerns about such behaviours have been raised repeatedly for over half a century [9–11] but the incentive structure of academia has not changed to address them. Despite the complex motivations that drive academia, many QRPs stem from the simple fact that the incentives which offer success to individual scientists conflict with what is best for science [12]. On the one hand are a set of gold standards that centuries of the scientific method have proven to be crucial for discovery: rigour, reproducibility, and transparency. On the other hand are a set of opposing principles born out of the academic career model: the drive to produce novel and striking results, the importance of confirming prior expectations, and the need to protect research interests from competitors. Within a culture that pressures scientists to produce rather than discover, the outcome is a biased and impoverished science in which most published results are either unconfirmed genuine discoveries or unchallenged fallacies [13]. This observation implies no moral judgement of scientists, who are as much victims of this system as they are perpetrators.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:07:14.930Z,Logical and methodological issues affecting genetic studies of humans reported in top neuroscience journals. ,https://doi.org/10.1162/jocn_a_01192,"Grabitz, C.R. et al.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Genetics and neuroscience are two areas of science that pose particular methodological problems because they involve detecting weak signals (i.e., small effects) in noisy data. In recent years, increasing numbers of studies have attempted to bridge these disciplines by looking for genetic factors associated with individual differences in behavior, cognition, and brain structure or function. However, different methodological approaches to guarding against false positives have evolved in the two disciplines. To explore methodological issues affecting neurogenetic studies, we conducted an in-depth analysis of 30 consecutive articles in 12 top neuroscience journals that reported on genetic associations in nonclinical human samples. It was often difficult to estimate effect sizes in neuroimaging paradigms. Where effect sizes could be calculated, the studies reporting the largest effect sizes tended to have two features: (i) they had the smallest samples and were generally underpowered to detect genetic effects, and (ii) they did not fully correct for multiple comparisons. Furthermore, only a minority of studies used statistical methods for multiple comparisons that took into account correlations between phenotypes or genotypes, and only nine studies included a replication sample or explicitly set out to replicate a prior finding. Finally, presentation of methodological information was not standardized and was often distributed across Methods sections and Supplementary Material, making it challenging to assemble basic information from many studies. Space limits imposed by journals could mean that highly complex statistical methods were described in only a superficial fashion. In summary, methods that have become standard in the genetics literature—stringent statistical standards, use of large samples, and replication of findings—are not always adopted when behavioral, cognitive, or neuroimaging phenotypes are used, leading to an increased risk of false-positive findings. Studies need to correct not just for the number of phenotypes collected but also for the number of genotypes examined, genetic models tested, and subsamples investigated. The field would benefit from more widespread use of methods that take into account correlations between the factors corrected for, such as spectral decomposition, or permutation approaches. Replication should become standard practice; this, together with the need for larger sample sizes, will entail greater emphasis on collaboration between research groups. We conclude with some specific suggestions for standardized reporting in this area.",English,I don't see any of these,Student,"Life Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:08:58.444Z,Excess Success for Psychology Articles in the Journal Science,https://doi.org/10.1371/journal.pone.0114255,"Gregory Francis ,Jay Tanzman,William J. Matthews","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"This article describes a systematic analysis of the relationship between empirical data and theoretical conclusions for a set of experimental psychology articles published in the journal Science between 2005–2012. When the success rate of a set of empirical studies is much higher than would be expected relative to the experiments’ reported effects and sample sizes, it suggests that null findings have been suppressed, that the experiments or analyses were inappropriate, or that the theory does not properly follow from the data. The analyses herein indicate such excess success for 83% (15 out of 18) of the articles in Science that report four or more studies and contain sufficient information for the analysis. This result suggests a systematic pattern of excess success among psychology articles in the journal Science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:10:23.082Z,A Duty to Describe: Better the Devil You Know Than the Devil You Don't,https://doi.org/10.1177/1745691614551749,"Sacha D Brown, David Furrow, Daniel F Hill, Jonathon C Gable, Liam P Porter, W Jake Jacobs","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Although many researchers have discussed replication as a means to facilitate self-correcting science, in this article, we identify meta-analyses and evaluating the validity of correlational and causal inferences as additional processes crucial to self-correction. We argue that researchers have a duty to describe sampling decisions they make; without such descriptions, self-correction becomes difficult, if not impossible. We developed the Replicability and Meta-Analytic Suitability Inventory (RAMSI) to evaluate the descriptive adequacy of a sample of studies taken from current psychological literature. Authors described only about 30% of the sampling decisions necessary for self-correcting science. We suggest that a modified RAMSI can be used by authors to guide their written reports and by reviewers to inform editorial recommendations. Finally, we claim that when researchers do not describe their sampling decisions, both readers and reviewers may assume that those decisions do not matter to the outcome of the study, do not affect inferences made from the research findings, do not inhibit inclusion in meta-analyses, and do not inhibit replicability of the study. If these assumptions are in error, as they often are, and the neglected decisions are relevant, then the neglect may create a good deal of mischief in the field.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:13:21.881Z,We have to break up,https://doi.org/10.1111/j.1745-6924.2009.01091.x,Robert B. Cialdini,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Three mostly positive developments in academic psychology—the cognitive revolution, the virtual requirement for multiple study reports in our top journals, and the prioritization of mediational evidence in our data—have had the unintended effect of making field research on naturally occurring behavior less suited to publication in the leading outlets of the discipline. Two regrettable consequences have ensued. The first is a reduction in the willingness of researchers, especially those young investigators confronting hiring and promotion issues, to undertake such field work. The second is a reduction in the clarity with which nonacademic audiences (e.g., citizens and legislators) can see the relevance of academic psychology to their lives and self-interest, which has contributed to a concomitant reduction in the availability of federal funds for basic behavioral science. Suggestions are offered for countering this problem",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:16:12.222Z,Preregistration Becoming the Norm in Psychological Science,https://www.psychologicalscience.org/observer/preregistration-becoming-the-norm-in-psychological-science/comment-page-1,Brian Nosek and Stephen Lindsay,"Reading, blog",College / Upper Division (Undergraduates),A blog about Preregistration Becoming the Norm in Psychological Science,English,I don't see any of these,Student,Social Science,"Conceptual and Statistical Knowledge,Preregistration",Blog
2020-06-08T18:19:24.175Z,Tools for De-Identification of Personal Health Information ,http://www.ehealthinformation.ca/wp-content/uploads/2014/08/2009-Tools-for-De-Identification-of-Personal-Health.pdf,Ross Fraser and Don Willison,Reading,College / Upper Division (Undergraduates),"This report identifies useful and available tools and techniques for the deidentification of personal information from interoperable electronic health records and health information related data warehouses. Section 1 contains a general introduction and defines some of the terms commonly used when discussing de-identification. Section 2 describes the principles of de-identification, including two basic models of how data warehouse are operated; the distinction between record-level and aggregate data; the distinction between direct and indirect identifiers; a description of the types of secondary uses that data warehouses support (health research, health system planning, public health surveillance, and generation of deidentified data for system testing); an explanation of k-anonymity as a measure of de-identification; a discussion of the special problems inherent in free-text data; a discussion of the special problems posed by genetic information; and guidance to prevent unintended disclosures through lapses in security. Section 3 describes the various approaches to de-identification, including a flow diagram of when to use each approach. Record-level data can be de-identified through data reduction (including removal of direct identifiers, reduction in the detail of the data, and sampling), data modification (random addition of noise to the data, randomization of data values, and data swapping), and data suppression. Each approach is briefly described and examples are given. Pseudonymisation is described; including the distinction between reversible and irreversible pseudonymisation, and the two basic ways in which pseudonymisation is carried out. Aggregate data has its own approaches to de-identification, including restriction-based methods (cell suppression and changing the classification scheme for the data) and heuristics. These are described with examples. Section 5 contains some best practices for de-identification. These include how direct identifiers should be handled in data warehouses; how date variables should be presented in released datasets; how location data such as postal codes should be handled in released datasets; special guidelines for diagnostic imaging data; how pseudonymous IDs should be handled in released datasets to prevent unintended data linkages to other datasets; and elements of contractual agreements on use and disclosure of datasets. Section 5 contains a description of readily available tools for de-identification of both record-level data and aggregate data. Tools described for handling direct identifiers in record-level data include Oracle Data Masking Pack, Camouflage, Informatica Data Privacy, and Data Masker. Tools for handling indirect identifiers in record-level data include PARAT, μ-Argus, and the Cornell Anonymization Toolkit. Additional tools for aggregate data include τ-ARGUS. Additional tools are also discussed for postal code conversion. Third-party evaluation of de-identification (or the lack thereof) is also briefly discussed. Section 6 briefly discusses re-identification risks. The report concludes with two observations: that the tools described can significantly reduce the risk of reidentification but only when sensibly combined with administrative controls such as end-user agreements and good security practices; and that the de-identified data will only be of value to the end-users if the approach to de-identification supports the intended use.",English,I don't see any of these,"Administrator, Researcher","Life Science, Social Science","Reproducible Analyses,Open Data and Materials",
2020-06-08T18:20:48.021Z,Open Science in Sport and Exercise Psychology: Review of Current Approaches and Considerations for Qualitative Inquiry,https://doi.org/10.1016/j.psychsport.2017.12.010,"Tamminen, K.A. and Poucher, Z.A.","Primary Source, Reading",College / Upper Division (Undergraduates),"Open science practices including open access (OA) publication, open methods, study preregistration, and open data are gaining acceptance across diverse fields of research. These practices are promoted as strategies to improve the reproducibility of research findings and the replicability of studies to accumulate knowledge and advance science. However, these arguments may raise concerns for qualitative researchers, and open science practices pose several challenges for qualitative researchers. The purpose of this paper is: (1) to review the state of open science practices within sport and exercise psychology, and (2) to discuss the implications of open science for qualitative inquiry. We examined open science practices across quantitative and qualitative articles in 11 sport and exercise psychology journals. While OA publication is a relatively recent phenomenon, OA articles were cited slightly more often than non-OA articles, although this difference was not significant. Some researchers provided supplementary materials alongside published articles, but researchers do not appear to be openly sharing the methods and data from their studies. No articles were published as preregistered studies at the time of our review. Some benefits of open science practices for qualitative inquiry include transparent documentation of the research process, opportunities for collaborative and pluralistic analyses, access to data across multiple research sites and from difficult-to-access settings and participants, and opportunities for teaching qualitative inquiry. We conclude by addressing several key questions including participant consent, confidentiality and anonymity, analyzing de-contextualized qualitative data, storing and accessing data, study preregistration, and the principle of emergent design within qualitative inquiry.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:23:12.189Z,Empirical Evaluation of Very Large Treatment Effects of Medical Interventions,https://doi.org/10.1001/jama.2012.13444,"Tiago V. Pereira, PhD Ralph I. Horwitz, MD John P. A. Ioannidis, MD, DSc","Primary Source, Reading",College / Upper Division (Undergraduates),"Context Most medical interventions have modest effects, but occasionally some clinical trials may find very large effects for benefits or harms. Objective To evaluate the frequency and features of very large effects in medicine. Data Sources Cochrane Database of Systematic Reviews (CDSR, 2010, issue 7). Study Selection We separated all binary-outcome CDSR forest plots with comparisons of interventions according to whether the first published trial, a subsequent trial (not the first), or no trial had a nominally statistically significant (P < .05) very large effect (odds ratio [OR], ≥5). We also sampled randomly 250 topics from each group for further in-depth evaluation. Data Extraction We assessed the types of treatments and outcomes in trials with very large effects, examined how often large-effect trials were followed up by other trials on the same topic, and how these effects compared against the effects of the respective meta-analyses. Results Among 85 002 forest plots (from 3082 reviews), 8239 (9.7%) had a significant very large effect in the first published trial, 5158 (6.1%) only after the first published trial, and 71 605 (84.2%) had no trials with significant very large effects. Nominally significant very large effects typically appeared in small trials with median number of events: 18 in first trials and 15 in subsequent trials. Topics with very large effects were less likely than other topics to address mortality (3.6% in first trials, 3.2% in subsequent trials, and 11.6% in no trials with significant very large effects) and were more likely to address laboratory-defined efficacy (10% in first trials,10.8% in subsequent, and 3.2% in no trials with significant very large effects). First trials with very large effects were as likely as trials with no very large effects to have subsequent published trials. Ninety percent and 98% of the very large effects observed in first and subsequently published trials, respectively, became smaller in meta-analyses that included other trials; the median odds ratio decreased from 11.88 to 4.20 for first trials, and from 10.02 to 2.60 for subsequent trials. For 46 of the 500 selected topics (9.2%; first and subsequent trials) with a very large-effect trial, the meta-analysis maintained very large effects with P < .001 when additional trials were included, but none pertained to mortality-related outcomes. Across the whole CDSR, there was only 1 intervention with large beneficial effects on mortality, P < .001, and no major concerns about the quality of the evidence (for a trial on extracorporeal oxygenation for severe respiratory failure in newborns). Conclusions Most large treatment effects emerge from small studies, and when additional trials are performed, the effect sizes become typically much smaller. Well-validated large effects are uncommon and pertain to nonfatal outcomes.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:24:57.469Z,"Open Science: What, Why, and How",https://psyarxiv.com/ak6jr/,"Bobbie Spellman, Elizabeth Gilbert and Katherine Corker","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Open Science is a collection of actions designed to make scientific processes more transparent and results more accessible. Its goal is to build a more replicable and robust science; it does so using new technologies, altering incentives, and changing attitudes. The current movement towards open science was spurred, in part, by a recent “series of unfortunate events” within psychology and other sciences. These events include the large number of studies that have failed to replicate and the prevalence of common research and publication procedures that could explain why. Many journals and funding agencies now encourage, require, or reward some open science practices, including pre-registration, providing full materials, posting data, distinguishing between exploratory and confirmatory analyses, and running replication studies. Individuals can practice and encourage open science in their many roles as researchers, authors, reviewers, editors, teachers, and members of hiring, tenure, promotion, and awards committees. A plethora of resources are available to help scientists, and science, achieve these goals.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:27:18.244Z,"Registered Replication Report: Rand, Greene, and Nowak (2012). ",https://doi.org/10.1177/1745691617693624,"Bouwmeester, S., Verkoeijen, P. P., Aczel, B., Barbosa, F., Bègue, L., Brañas-Garza, P., ... & Evans, A. M. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In an anonymous 4-person economic game, participants contributed more money to a common project (i.e., cooperated) when required to decide quickly than when forced to delay their decision (Rand, Greene & Nowak, 2012), a pattern consistent with the social heuristics hypothesis proposed by Rand and colleagues. The results of studies using time pressure have been mixed, with some replication attempts observing similar patterns (e.g., Rand et al., 2014) and others observing null effects (e.g., Tinghög et al., 2013; Verkoeijen & Bouwmeester, 2014). This Registered Replication Report (RRR) assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate, preregistered replications of the critical conditions from Study 7 of the original article (Rand et al., 2012). The primary planned analysis used data from all participants who were randomly assigned to conditions and who met the protocol inclusion criteria (an intent-to-treat approach that included the 65.9% of participants in the time-pressure condition and 7.5% in the forced-delay condition who did not adhere to the time constraints), and we observed a difference in contributions of -0.37 percentage points compared with an 8.6 percentage point difference calculated from the original data. Analyzing the data as the original article did, including data only for participants who complied with the time constraints, the RRR observed a 10.37 percentage point difference in contributions compared with a 15.31 percentage point difference in the original study. In combination, the results of the intent-to-treat analysis and the compliant-only analysis are consistent with the presence of selection biases and the absence of a causal effect of time pressure on cooperation.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:28:49.937Z,A multilab preregistered replication of the ego-depletion effect.,https://doi.org/10.1177/1745691616652873,"Hagger, M. S., Chatzisarantis, N. L., Alberts, H., Anggono, C. O., Batailler, C., Birt, A. R., ... & Calvillo, D. P.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95% confidence intervals (CIs) that encompassed zero (d = 0.04, 95% CI [−0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:31:05.327Z,Why most of psychology is statistically unfalsifiable,https://github.com/richarddmorey/psychology_resolution/blob/master/paper/response.pdf,Richard Morey and Daniel Lakens,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Low power in experimental psychology is an oft-discussed problem. We show in the context of the Replicability Project: Psychology (Open Science Collaboration, 2015) that sample sizes are so small in psychology that often one cannot detect even large differences between studies. High-powered replications cannot answer this problem, because the power to find differences in results from a previous study is limited by the sample size in the original study. This is not simply a problem with replications; cumulative science, which critically depends on assessing differences between results published in the literature, is practically impossible with typical sample sizes in experimental psychology. We diagnose misconceptions about power and suggest a solution to increase the resolution of published results",English,I don't see any of these,Student,Social Science,Conceptual and Statistical Knowledge,
2020-06-08T18:32:27.348Z,Reproducible research in sport and exercise psychology: The role of sample sizes.,https://doi.org/10.1016/j.psychsport.2015.11.005,"Schweizer, G., & Furley, P.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Objectives: We aim to introduce the discussion on the crisis of confidence to sport and exercise psychology. We focus on an important aspect of this debate, the impact of sample sizes, by assessing sample sizes within sport and exercise psychology. Researchers have argued that publications in psychological research contain numerous false-positive findings and inflated effect sizes due to small sample sizes. Method: We analyse the four leading journals in sport and exercise psychology regarding sample sizes of all quantitative studies published in these journals between 2009 and 2013. Subsequently, we conduct power analyses. Results: A substantial proportion of published studies does not have sufficient power to detect effect sizes typical for psychological research. Sample sizes and power vary between research designs. Although many correlational studies have adequate sample sizes, experimental studies are often underpowered to detect small-to-medium effects. Conclusions: As sample sizes are small, research in sport and exercise psychology may suffer from false-positive results and inflated effect sizes, while at the same time failing to detect meaningful small effects. Larger sample sizes are warranted, particularly in experimental studies.",English,I don't see any of these,Student,Social Science,Conceptual and Statistical Knowledge,
2020-06-08T18:34:16.962Z,Best research practices in psychology: Illustrating epistemological and pragmatic considerations with the case of relationship science. ,https://doi.org/10.1037/pspi0000007,"Finkel, E. J., Eastwick, P. W., & Reis, H. T. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In recent years, a robust movement has emerged within psychology to increase the evidentiary value of our science. This movement, which has analogs throughout the empirical sciences, is broad and diverse, but its primary emphasis has been on the reduction of statistical false positives. The present article addresses epistemological and pragmatic issues that we, as a field, must consider as we seek to maximize the scientific value of this movement. Regarding epistemology, this article contrasts the false-positives-reduction (FPR) approach with an alternative, the error balance (EB) approach, which argues that any serious consideration of optimal scientific practice must contend simultaneously with both false-positive and false-negative errors. Regarding pragmatics, the movement has devoted a great deal of attention to issues that frequently arise in laboratory experiments and one-shot survey studies, but it has devoted less attention to issues that frequently arise in intensive and/or longitudinal studies. We illustrate these epistemological and pragmatic considerations with the case of relationship science, one of the many research domains that frequently employ intensive and/or longitudinal methods. Specifically, we examine 6 research prescriptions that can help to reduce false-positive rates: preregistration, prepublication sharing of materials, postpublication sharing of data, close replication, avoiding piecemeal publication, and increasing sample size. For each, we offer concrete guidance not only regarding how researchers can improve their research practices and balance the risk of false-positive and false-negative errors, but also how the movement can capitalize upon insights from research practices within relationship science to make the movement stronger and more inclusive",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:36:57.024Z,The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask,https://pure.mpg.de/rest/items/item_2101291/component/file_3080636/content,"Gerd Gigerenzer, Stefan Krauss, and Oliver Vitouch","Primary Source, Reading",College / Upper Division (Undergraduates),A chapter about significance testing,English,I don't see any of these,Student,Social Science,Conceptual and Statistical Knowledge,
2020-06-08T18:39:01.563Z,Continuously Cumulating Meta-Analysis and Replicability,https://doi.org/10.1177/1745691614529796,"Sanford L. Braver, Felix J. Thoemmes, Robert Rosenthal","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The current crisis in scientific psychology about whether our findings are irreproducible was presaged years ago by Tversky and Kahneman (1971), who noted that even sophisticated researchers believe in the fallacious Law of Small Numbers—erroneous intuitions about how imprecisely sample data reflect population phenomena. Combined with the low power of most current work, this often leads to the use of misleading criteria about whether an effect has replicated. Rosenthal (1990) suggested more appropriate criteria, here labeled the continuously cumulating meta-analytic (CCMA) approach. For example, a CCMA analysis on a replication attempt that does not reach significance might nonetheless provide more, not less, evidence that the effect is real. Alternatively, measures of heterogeneity might show that two studies that differ in whether they are significant might have only trivially different effect sizes. We present a nontechnical introduction to the CCMA framework (referencing relevant software), and then explain how it can be used to address aspects of replicability or more generally to assess quantitative evidence from numerous studies. We then present some examples and simulation results using the CCMA approach that show how the combination of evidence can yield improved results over the consideration of single studies.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:43:48.197Z,Change starts with journal editors: In response to Makel (2014).,https://doi.org/10.1037/a0035801,Matthew T. McBee and Michael S. Matthews,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The editors of the Journal of Advanced Academics comment on Makel (2014). The replicability crisis in psychology is summarized in terms of three focal issues: the “file drawer” problem, lack of replication studies, and the null hypothesis significance testing paradigm. The authors argue that journal editors are uniquely positioned to address all three of these problems via the adoption of new policies for review and publication.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-08T18:45:37.869Z,RIPOSTE: A Framework for Improving the Design and Analysis of Laboratory-Based Research,https://doi.org/10.7554/eLife.05519,Masca et al. ,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Lack of reproducibility is an ongoing problem in some areas of the biomedical sciences. Poor experimental design and a failure to engage with experienced statisticians at key stages in the design and analysis of experiments are two factors that contribute to this problem. The RIPOSTE (Reducing IrreProducibility in labOratory STudiEs) framework has been developed to support early and regular discussions between scientists and statisticians in order to improve the design, conduct and analysis of laboratory studies and, therefore, to reduce irreproducibility. This framework is intended for use during the early stages of a research project, when specific questions or hypotheses are proposed. The essential points within the framework are explained and illustrated using three examples (a medical equipment test, a macrophage study and a gene expression study). Sound study design minimises the possibility of bias being introduced into experiments and leads to higher quality research with more reproducible results.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:00:24.913Z,"What’s wrong with statistical tests – and where do we go from here? In R. B. Kline, Beyond significance testing: Reforming data analysis methods in behavioral research",https://doi.org/10.1037/10693-003,R.B. Kline,"Primary Source, Reading, Chapter",College / Upper Division (Undergraduates),"This chapter considers problems with null hypothesis significance testing (NHST). The literature in this area is quite large. D. Anderson, Burnham, and W. Thompson (2000) recently found more than 300 articles in different disciplines about the indiscriminate use of NHST, and W. Thompson (2001) lists more than 400 references about this topic. As a consequence, it is possible to cite only a few representative works. After review of the debate about NHST, the author argues that the criticisms have sufficient merit to support the minimization or elimination of NHST in the behavioral sciences. The author offers specific suggestions along these lines. Some concern alternatives that may replace or supplement NHST and thus are directed at researchers. Others concern editorial policies or educational curricula. Few of the recommendations given are original in that many have been made over the years by various authors. However, as a set they deal with issues often considered in separate works. For simplicity, the context for NHST assumed is reject-support (RS) instead of accept-support (AS). The RS context is more common, and many of the arguments can be reframed for the AS context.",English,I don't see any of these,Student,Social Science,Conceptual and Statistical Knowledge,
2020-06-09T16:09:33.815Z,Graphical Causal Models,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.364.7505&rep=rep1&type=pdf,Felix Elwert,"Primary Source, Reading, Chapter",College / Upper Division (Undergraduates),"This chapter discusses the use of directed acyclic graphs (DAGs) for causal inference in the observational social sciences. It focuses on DAGs’ main uses, discusses central principles, and gives applied examples. DAGs are visual representations of qualitative causal assumptions: They encode researchers’ beliefs about how the world works. Straightforward rules map these causal assumptions onto the associations and independencies in observable data. The two primary uses of DAGs are (1) determining the identifiability of causal effects from observed data and (2) deriving the testable implications of a causal model. Concepts covered in this chapter include identification, d-separation, confounding, endogenous selection, and overcontrol. Illustrative applications then demonstrate that conditioning on variables at any stage in a causal process can induce as well as remove bias, that confounding is a fundamentally causal rather than an associational concept, that conventional approaches to causal mediation analysis are often biased, and that causal inference in social networks inherently faces endogenous selection bias. The chapter discusses several graphical criteria for the identification of causal effects of single, time-point treatments (including the famous backdoor criterion), as well identification criteria for multiple, time-varying treatments.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-09T16:14:25.870Z,Constraints on generality statements are needed to define direct replication,https://doi.org/10.1017/S0140525X18000845,"Daniel J. Simons, Yuichi Shoda and D. Stephen Lindsay","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Whether or not a replication attempt counts as “direct” often cannot be determined definitively after the fact as a result of flexibility in how procedural differences are interpreted. Specifying constraints on generality in original articles can eliminate ambiguity in advance, thereby leading to a more cumulative science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:16:56.514Z,Opening Science,http://doi.org/10.1007/978-3-319-00026-8,Sönke Bartling and Sascha Friesike,"Primary Source, Reading",College / Upper Division (Undergraduates),"The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing",English,CC BY-NC,Student,"Applied Science, Life Science, Physical Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:19:09.067Z,"Transparent science: A more credible, reproducible, and publishable way to do science",https://psyarxiv.com/7wkdn/,"David Mellor, Simine Vazire and D. Lindsay","Primary Source, Reading, Chapter",College / Upper Division (Undergraduates),"This is an exciting time to be a psychological scientist. There is a major new movement that seeks to promote the credibility and replicability of psychological research by enhancing its transparency, with scholarly societies promoting the principles (http://www.psychologicalscience.org/publications/open-science) and groups formed specifically to advance that mission (see http://improvingpsych.org/ and https://cos.io for two examples). While relatively low rates of replicability among scientific findings (Begley & Ellis, 2012; OSC, 2015; Chang & Li 2015) inspired the existence of these groups, in this chapter we describe how striving to maximize transparency in your research can benefit both science and your career.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:21:23.599Z,Trust your science? Open your data and code.,https://web.stanford.edu/~vcs/papers/TrustYourScience-STODDEN.pdf,Victoria Stodden,"Primary Source, Reading",College / Upper Division (Undergraduates),A paper about Open Your Data and Code,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:25:58.562Z,Quality Uncertainty Erodes Trust in Science,http://doi.org/10.1525/collabra.74,S. Vazire,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars – they cannot reliably tell the difference between lemons and high quality findings. This phenomenon – quality uncertainty – has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals’ reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so – we should be held to a higher standard than used car salespeople",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:27:51.512Z,Standard Operating Procedures: A Safety Net for Pre-Analysis Plans,https://doi.org/10.1017/S1049096516000810,Winston Lin and Donald P.Green,"Primary Source, Reading",College / Upper Division (Undergraduates),"Across the social sciences, growing concerns about research transparency have led to calls for pre-analysis plans (PAPs) that specify in advance how researchers intend to analyze the data they are about to gather. PAPs promote transparency and credibility by helping readers distinguish between exploratory and confirmatory analyses. However, PAPs are time-consuming to write and may fail to anticipate contingencies that arise in the course of data collection. This article proposes the use of “standard operating procedures” (SOPs)—default practices to guide decisions when issues arise that were not anticipated in the PAP. We offer an example of an SOP that can be adapted by other researchers seeking a safety net to support their PAPs.",English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-09T16:29:39.613Z,Registered replication report: Schooler and engstler-schooler (1990). ,https://doi.org/10.1177/1745691614545653,"Alogna, V. K et al.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Trying to remember something now typically improves your ability to remember it later. However, after watching a video of a simulated bank robbery, participants who verbally described the robber were 25% worse at identifying the robber in a lineup than were participants who instead listed U.S. states and capitals—this has been termed the “verbal overshadowing” effect (Schooler & Engstler-Schooler, 1990). More recent studies suggested that this effect might be substantially smaller than first reported. Given uncertainty about the effect size, the influence of this finding in the memory literature, and its practical importance for police procedures, we conducted two collections of preregistered direct replications (RRR1 and RRR2) that differed only in the order of the description task and a filler task. In RRR1, when the description task immediately followed the robbery, participants who provided a description were 4% less likely to select the robber than were those in the control condition. In RRR2, when the description was delayed by 20 min, they were 16% less likely to select the robber. These findings reveal a robust verbal overshadowing effect that is strongly influenced by the relative timing of the tasks. The discussion considers further implications of these replications for our understanding of verbal overshadowing.",English,I don't see any of these,Student,"Applied Science, Social Science","Preregistration,Replication Research","Transparency,Open Science,Registered Reports"
2020-06-09T16:31:01.069Z,Registered replication report: Hart & Albarracín (2011).,https://doi.org/10.1177/1745691615605826,"Eerland, A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Language can be viewed as a complex set of cues that shape people’s mental representations of situations. For example, people think of behavior described using imperfective aspect (i.e., what a person was doing) as a dynamic, unfolding sequence of actions, whereas the same behavior described using perfective aspect (i.e., what a person did) is perceived as a completed whole. A recent study found that aspect can also influence how we think about a person’s intentions (Hart & Albarracín, 2011). Participants judged actions described in imperfective as being more intentional (d between 0.67 and 0.77) and they imagined these actions in more detail (d = 0.73). The fact that this finding has implications for legal decision making, coupled with the absence of other direct replication attempts, motivated this registered replication report (RRR). Multiple laboratories carried out 12 direct replication studies, including one MTurk study. A meta-analysis of these studies provides a precise estimate of the size of this effect free from publication bias. This RRR did not find that grammatical aspect affects intentionality (d between 0 and −0.24) or imagery (d = −0.08). We discuss possible explanations for the discrepancy between these results and those of the original study.",English,I don't see any of these,Student,"Applied Science, Social Science","Preregistration,Replication Research","Transparency,Open Science,Registered Reports"
2020-06-09T16:33:06.121Z,Why Psychologists’ Food Fight Matters,https://slate.com/technology/2014/07/replication-controversy-in-psychology-bullying-file-drawer-effect-blog-posts-repligate.html,Michelle N.Meyer and Christopher Chabris,Blog,College / Upper Division (Undergraduates),"“Important findings” haven’t been replicated, and science may have to change its ways.",English,I don't see any of these,Student,"Applied Science, Social Science",Preregistration,"Blog,Open Science,Reproducibility Crisis and Credibility Revolution"
2020-06-09T16:35:33.549Z,The meaning of “significance” for different types of research,https://doi.org/10.1016/j.actpsy.2014.02.001,"De Groot, A. D. translated and annotated by Eric-Jan Wagenmakers, Denny Borsboom, Josine Verhagen, Rogier Kievit, Marjan Bakker, Angelique Cramer, Dora Matzke, Don Mellenbergh, and Han LJ van der Maas","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Adrianus Dingeman de Groot (1914-2006) was one of the most influential Dutch psychologists. He became famous for his work ""Thought and Choice in Chess"", but his main contribution was methodological--De Groot co-founded the Department of Psychological Methods at the University of Amsterdam (together with R. F. van Naerssen), founded one of the leading testing and assessment companies (CITO), and wrote the monograph ""Methodology"" that centers on the empirical-scientific cycle: observation-induction-deduction-testing-evaluation. Here we translate one of De Groot's early articles, published in 1956 in the Dutch journal Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden. This article is more topical now than it was almost 60years ago. De Groot stresses the difference between exploratory and confirmatory (""hypothesis testing"") research and argues that statistical inference is only sensible for the latter: ""One 'is allowed' to apply statistical tests in exploratory research, just as long as one realizes that they do not have evidential impact"". De Groot may have also been one of the first psychologists to argue explicitly for preregistration of experiments and the associated plan of statistical analysis. The appendix provides annotations that connect De Groot's arguments to the current-day debate on transparency and reproducibility in psychological science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science,Transparency"
2020-06-09T16:36:57.877Z,The crisis of confidence in social psychology.,https://doi.org/10.1037/0003-066X.30.10.967,A.C. Elms,"Primary Source, Reading",College / Upper Division (Undergraduates),"Notes that social psychologists' early enthusiasm has been replaced by serious doubts about the future of their field. Difficulties in conducting research, unfulfilled expectations about research payoffs, and outside pressures had all contributed to a sense of crisis. Relief may come from acceptance of theoretical and methodological pluralism, from reevaluation of research expectations and ethical stances, and from the development of realistic responses to societal demands.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:38:26.706Z,"What's wrong with Psychology, anyway?",https://www.gwern.net/docs/psychology/1991-lykken.pdf,David T. Lykken,"Primary Source, Reading",College / Upper Division (Undergraduates),"This chapter considers various factors that have been responsible for the comparatively slow development of psychology into a cumulative empirical science. Special attention is devoted to correctable methodological mistakes, the over-reliance upon significance testing (and the fact that, in psychology, the null hypothesis is almost always false), and an analysis of the concept of replication.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science,Transparency"
2020-06-09T16:40:23.897Z,We Knew the Future All Along: Scientific Hypothesizing Is Much More Accurate Than Other Forms of Precognition-A Satire in One Part,https://doi.org/10.11772F1745691612441216,Arina K Bones,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),A critique about Daryl Bem's (2011) paper and reproducibility ,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:42:32.077Z,Der Umgang mit Forschungsdaten im Fach Psychologie: Konkretisierung der DFG-Leitlinien. [Data Management in Psychological Science: Specification of the DFG Guidelines].,https://psyarxiv.com/vhx89/,"Schönbrodt, F., Gollwitzer, M., & Abele-Brehm, A. ","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Calls for public access to research data have been ongoing for some time. For instance, in their “Recommendations for Secure Storage and Availability of Digital Primary Research Data” (2009) the German Research Foundation (“Deutsche Forschungsgemeinschaft”, DFG) demanded that publicly funded data are freely available after the completion of a project. In line with this, in 2010 the Alliance of Science Organizations in Germany called for long-term storage of and generally free access to research data. In September 2015, the DFG published data management guidelines that affirmed these goals and asked research associations to consider their data management regulations and to develop appropriate standards for discipline-specific use and sharing of research data. The German Psychological Society (DGPs) joins the DFG and the Alliance of Science Organizations in Germany in their mission to specify the DFG guidelines for the field of psychology. This document (a) emphasizes the importance of sustainable research data management, (b) defines what “primary data” are and how they should be stored, (c) defines standards and potential data sharing restrictions, and (d) defines the rights and duties of researchers that share data and researchers that use secondary data. The German Psychological Associations adopted these recommendations in September 2016.","English, German",I don't see any of these,Student,"Applied Science, Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:44:19.528Z,The Null Hypothesis Significance-Testing Debate and Its Implications for Personality Research,https://dl.uswr.ac.ir/bitstream/Hannan/131127/1/Richard_W._Robins%2C_R._Chris_Fraley%2C_Robert_F._Krueger_Handbook_of_Research_Methods_in_Personality_Psychology__2007.pdf#page=166,R. Chris Fraley and Michael J. Marks,"Primary Source, Reading",College / Upper Division (Undergraduates),A chapter about null hypothesis significance testing in personality research,English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-09T16:46:39.509Z,Is There a Free Lunch in Inference?,https://doi.org/10.1111/tops.12214,"Jeffrey N Rouder, Richard D Morey, Josine Verhagen, Jordan M Province, Eric-Jan Wagenmakers ","Primary Source, Reading",College / Upper Division (Undergraduates),"The field of psychology, including cognitive science, is vexed by a crisis of confidence. Although the causes and solutions are varied, we focus here on a common logical problem in inference. The default mode of inference is significance testing, which has a free lunch property where researchers need not make detailed assumptions about the alternative to test the null hypothesis. We present the argument that there is no free lunch; that is, valid testing requires that researchers test the null against a well-specified alternative. We show how this requirement follows from the basic tenets of conventional and Bayesian probability. Moreover, we show in both the conventional and Bayesian framework that not specifying the alternative may lead to rejections of the null hypothesis with scant evidence. We review both frequentist and Bayesian approaches to specifying alternatives, and we show how such specifications improve inference. The field of cognitive science will benefit because consideration of reasonable alternatives will undoubtedly sharpen the intellectual underpinnings of research.",English,I don't see any of these,Student,"Math & Statistics, Social Science","Reproducibility and Replicability Knowledge,Conceptual and Statistical Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science,Transparency"
2020-06-09T16:56:53.673Z,"Priming, Replication, and the Hardest Science",https://doi.org/10.1177/1745691613513470,Joseph Cesairo,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Concerns have been raised recently about the replicability of behavioral priming effects, and calls have been issued to identify priming methodologies with effects that can be obtained in any context and with any population. I argue that such expectations are misguided and inconsistent with evolutionary understandings of the brain as a computational organ. Rather, we should expect priming effects to be highly sensitive to variations in experimental features and subject populations. Such variation does not make priming effects frivolous or capricious but instead can be predicted a priori. However, absent theories specifying the precise contingencies that lead to such variation, failures to replicate another researcher’s findings will necessarily be ambiguous with respect to the inferences that can be made. Priming research is not yet at the stage where such theories exist, and therefore failures are uninformative at the current time. Ultimately, priming researchers themselves must provide direct replications of their own effects; researchers have been deficient in meeting this responsibility and have contributed to the current state of confusion. The recommendations issued in this article reflect concerns both with the practice of priming researchers and with the inappropriate expectations of researchers who have failed to replicate others’ priming effects.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T18:53:53.613Z,The Replication Crisis in Psychology,https://nobaproject.com/modules/the-replication-crisis-in-psychology,Edward Diener and Robert Biswas-Diener,"Primary Source, Reading, Blog",College / Upper Division (Undergraduates),"In science, replication is the process of repeating research to determine the extent to which findings generalize across time and across situations. Recently, the science of psychology has come under criticism because a number of research findings do not replicate. In this module we discuss reasons for non-replication, the impact this phenomenon has on the field, and suggest solutions to the problem.",English,CC BY-NC-SA,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T18:55:24.184Z,Fearing the future of empirical psychology: Bem’s (2011) evidence of psi as a case study in deficiencies in modal research practice. ,http://dx.doi.org/10.1037/a0025172,"LeBel, E.P., & Peters, K.R.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"In this methodological commentary, we use Bem’s (2011) recent article reporting experimental evidence for psi as a case study for discussing important deficiencies in modal research practice in empirical psychology. We focus on (a) overemphasis on conceptual rather than close replication, (b) insufficient attention to verifying the soundness of measurement and experimental procedures, and (c) flawed implementation of null hypothesis significance testing. We argue that these deficiencies contribute to weak method-relevant beliefs that, in conjunction with overly strong theory-relevant beliefs, lead to a systemic and pernicious bias in the interpretation of data that favors a researcher’s theory. Ultimately, this interpretation bias increases the risk of drawing incorrect conclusions about human psychology. Our analysis points to concrete recommendations for improving research practice in empirical psychology. We recommend (a) a stronger emphasis on close replication, (b) routinely verifying the integrity of measurement instruments and experimental procedures, and (c) using stronger, more diagnostic forms of null hypothesis testing.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T18:57:19.428Z,Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect,https://doi.org/10.1037/a0021524,Daryl J Bem,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by ""time-reversing"" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.",English,I don't see any of these,Student,Social Science,"Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:01:00.577Z,Introduction to Meta-Analysis,http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002313.html,"Borenstein, M., Hedges, L.V., Higgins, J. P. T., & Rothstein, H. R. (2009).",Reading,College / Upper Division (Undergraduates),A book about meta analyses,English,I don't see any of these,Student,Math & Statistics,Conceptual and Statistical Knowledge,
2020-06-10T19:03:22.310Z,A Social Priming Data Set With Troubling Oddities,https://doi.org/10.1080/01973533.2015.1124767,"Harold Pashler,Doug Rohrer,Ian Abramson,Tanya Wolfson &Christine R. Harris","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A recent paper by Chatterjee, Rose, and Sinha (2013) reported impressively large “money priming” effects: incidental exposure to concepts relating to cash or credit cards made participants much less generous with their time and money (after cash primes) or much more generous (after credit card primes ). Primes also altered participants’ choices in a word-stem completion task. To explore these effects, we carried out re-analyses of the raw data. A number of strange oddities were brought to light, including a dramatic similarity of the filler word-stem completion responses produced by the 20 subjects who contributed most to the priming effects. We suggest that these oddities undermine the credibility of the paper and require further investigation.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:05:29.504Z,Reviewer Bias Against Replication Research,https://search.proquest.com/docview/1292304227?pq-origsite=gscholar&fromopenview=true&imgSeq=1,"Neuliep, J W; Crandall, R.","Primary Source, Reading",College / Upper Division (Undergraduates),"Social science journal reviewers (N=8) responded to questionnaires regarding their reviewing history, and attitudes towards and perception of replication studies. Results indicate that reviewers are biased against replication studies and toward studies demonstrating some new effects. Several reviewers indicated that replications are a waste of time and journal space.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:11:59.409Z,Eight common but false objections to the discontinuation of significance testing in the analysis of research data.,https://www.phil.vt.edu/dmayo/personal_website/Schmidt_Hunter_Eight_Common_But_False_Objections.pdf,"Schmidt, J. E. H. F. L., Hunter, J. E., Harlow, L., Mulaik, S., & Steiger, J. ","Primary Source, Reading",College / Upper Division (Undergraduates),"Logically and conceptually, the use of statistical significance testing in the analysis of research data has been thoroughly discredited. However, reliance on significance testing is strongly embedded in the minds and habits of researchers, and therefore proposals to replace significance testing with point estimate estimates and confidence intervals often encounter strong resistance. This chapter examines eight of the most commonly voiced objects to reform of data analysis practices and shows each of them to be erroneous. The objections are: (a) Without significance tests we would not know whether a finding is real or just due to chance; (b) hypothesis testing would not be possible without significance tests; (c) the problem is not significance tests but failure to develop a tradition of replicationg studies; (d) when studies have a large number of relationships, we need significance tests to identify those that are real and not just due to chance; (e) confidence intervals are themselves significance tests; (f) significance testing ensures objectivity in the interpretation of research data; (g) it is the misuse, not the use, of significance testing that is the problem; and (h) it is futile to try to reform data analysis methods, so why try? Each of these objections is intuitively appealing and plausible but is easily shown to be logically and intellectually bankrupt. The same is true of the almost 80 other objects we have collected. Statistical significance testing retards the growth of scientific knowledge; it never makes a positive contribution. After decades of unsuccessful efforts, it now appears possible that reform of data analysis procedures will finally succeed. If so, a major impediment to the advance of scientific knowledge will have been removed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:13:18.266Z,"The Superego, the Ego, and the Id in Statistical Reasoning",https://doi.org/10.1093/acprof:oso/9780195153729.003.0013,Gerd Gigerenzer,"Primary Source, Reading",College / Upper Division (Undergraduates),"Statistical reasoning is an art and so demands both mathematical knowledge and informed judgment. When it is mechanized, as with the institutionalized hybrid logic, it becomes ritual, not reasoning. Many experts have argued that it is not going to be easy to get researchers in psychology and other sociobiomedical sciences to drop this comforting crutch unless one offers an easy-to-use substitute. This chapter argues that this should be avoided — the substitution of one mechanistic dogma for another. At the very least, this chapter can serve as a tool in arguments with people who think they have to defend a ritualistic dogma instead of good statistical reasoning. Making and winning such arguments is indispensable to good science.",English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-10T19:15:32.485Z,The 52 symptoms of major depression: Lack of content overlap among seven common depression scales,https://doi.org/10.1016/j.jad.2016.10.019,Eiko I.Fried,"Primary Source, Reading",College / Upper Division (Undergraduates),"Depression severity is assessed in numerous research disciplines, ranging from the social sciences to genetics, and used as a dependent variable, predictor, covariate, or to enroll participants. The routine practice is to assess depression severity with one particular depression scale, and draw conclusions about depression in general, relying on the assumption that scales are interchangeable measures of depression. The present paper investigates to which degree 7 common depression scales differ in their item content and generalizability. A content analysis is carried out to determine symptom overlap among the 7 scales via the Jaccard index (0=no overlap, 1=full overlap). Per scale, rates of idiosyncratic symptoms, and rates of specific vs. compound symptoms, are computed. The 7 instruments encompass 52 disparate symptoms. Mean overlap among all scales is low (0.36), mean overlap of each scale with all others ranges from 0.27 to 0.40, overlap among individual scales from 0.26 to 0.61. Symptoms feature across a mean of 3 scales, 40% of the symptoms appear in only a single scale, 12% across all instruments. Scales differ regarding their rates of idiosyncratic symptoms (0–33%) and compound symptoms (22–90%). Future studies analyzing more and different scales will be required to obtain a better estimate of the number of depression symptoms; the present content analysis was carried out conservatively and likely underestimates heterogeneity across the 7 scales. The substantial heterogeneity of the depressive syndrome and low overlap among scales may lead to research results idiosyncratic to particular scales used, posing a threat to the replicability and generalizability of depression research. Implications and future research opportunities are discussed.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:17:21.953Z,The Test of Insufficient Variance (TIVA): A New Tool for the Detection of Questionable Research Practices,https://replicationindex.wordpress.com/2014/12/30/the-test-ofinsufficientvariance-tiva-a-new-tool-for-the-detection-ofquestionableresearch-practices/,"Shimmack, U. ",Blog,College / Upper Division (Undergraduates),A blog about The Test of Insufficient Variance (TIVA): A New Tool for the Detection of Questionable Research Practices,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:19:01.811Z,Probing Birth-Order Effects on Narrow Traits Using Specification-Curve Analysis,https://doi.org/10.1177/0956797617723726,"Julia M Rohrer, Boris Egloff, Stefan C Schmukle","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The idea that birth-order position has a lasting impact on personality has been discussed for the past 100 years. Recent large-scale studies have indicated that birth-order effects on the Big Five personality traits are negligible. In the current study, we examined a variety of more narrow personality traits in a large representative sample ( n = 6,500-10,500 in between-family analyses; n = 900-1,200 in within-family analyses). We used specification-curve analysis to assess evidence for birth-order effects across a range of models implementing defensible yet arbitrary analytical decisions (e.g., whether to control for age effects or to exclude participants on the basis of sibling spacing). Although specification-curve analysis clearly confirmed the previously reported birth-order effect on intellect, we found no meaningful effects on life satisfaction, locus of control, interpersonal trust, reciprocity, risk taking, patience, impulsivity, or political orientation. The lack of meaningful birth-order effects on self-reports of personality was not limited to broad traits but also held for more narrowly defined characteristics.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Reproducible Analyses,Open Data and Materials,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:21:45.893Z,Novel methods to deal with publication biases: secondary analysis of antidepressant trials in the FDA trial registry database and related journal publications.,https://doi.org/10.1136/bmj.b2981,"Santiago G Moreno, Alex J Sutton, Erick H Turner, Keith R Abrams, Nicola J Cooper, Tom M Palmer, A E Ades","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Objective: To assess the performance of novel contour enhanced funnel plots and a regression based adjustment method to detect and adjust for publication biases. Design: Secondary analysis of a published systematic literature review. Data sources: Placebo controlled trials of antidepressants previously submitted to the US Food and Drug Administration (FDA) and matching journal publications. Methods: Publication biases were identified using novel contour enhanced funnel plots, a regression based adjustment method, Egger's test, and the trim and fill method. Results were compared with a meta-analysis of the gold standard data submitted to the FDA. Results: Severe asymmetry was observed in the contour enhanced funnel plot that appeared to be heavily influenced by the statistical significance of results, suggesting publication biases as the cause of the asymmetry. Applying the regression based adjustment method to the journal data produced a similar pooled effect to that observed by a meta-analysis of the FDA data. Contrasting journal and FDA results suggested that, in addition to other deviations from study protocol, switching from an intention to treat analysis to a per protocol one would contribute to the observed discrepancies between the journal and FDA results. Conclusion: Novel contour enhanced funnel plots and a regression based adjustment method worked convincingly and might have an important part to play in combating publication biases.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:23:20.801Z,Maximizing the Reproducibility of Your Research,https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119095910.ch1,Open Science Collaboration,"Primary Source, Reading, Chapter",College / Upper Division (Undergraduates),A chapter to discuss maximising the reproducibility of research,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:24:51.647Z,Recommendations for increasing replicability in psychology,https://doi.org/10.1037/14805-038,Asendorpf et al.,"Primary Source, Reading, Chapter",College / Upper Division (Undergraduates),"Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. ",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research",
2020-06-10T19:27:50.116Z,"Registered Replication Report: Strack, Martin, & Stepper (1988)",https://doi.org/10.1177/1745691616674458,Wagenmakers et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"According to the facial feedback hypothesis, people’s affective responses can be influenced by their own facial expression (e.g., smiling, pouting), even when their expression did not result from their emotional experiences. For example, Strack, Martin, and Stepper (1988) instructed participants to rate the funniness of cartoons using a pen that they held in their mouth. In line with the facial feedback hypothesis, when participants held the pen with their teeth (inducing a “smile”), they rated the cartoons as funnier than when they held the pen with their lips (inducing a “pout”). This seminal study of the facial feedback hypothesis has not been replicated directly. This Registered Replication Report describes the results of 17 independent direct replications of Study 1 from Strack et al. (1988), all of which followed the same vetted protocol. A meta-analysis of these studies examined the difference in funniness ratings between the “smile” and “pout” conditions. The original Strack et al. (1988) study reported a rating difference of 0.82 units on a 10-point Likert scale. Our meta-analysis revealed a rating difference of 0.03 units with a 95% confidence interval ranging from −0.11 to 0.16",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-10T19:29:31.811Z,The effect of horizontal eye movements on free recall: A preregistered adversarial collaboration.,https://doi.org/10.1037/xge0000038,Matzke et al.,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"A growing body of research has suggested that horizontal saccadic eye movements facilitate the retrieval of episodic memories in free recall and recognition memory tasks. Nevertheless, a minority of studies have failed to replicate this effect. This article attempts to resolve the inconsistent results by introducing a novel variant of proponent-skeptic collaboration. The proposed approach combines the features of adversarial collaboration and purely confirmatory preregistered research. Prior to data collection, the adversaries reached consensus on an optimal research design, formulated their expectations, and agreed to submit the findings to an academic journal regardless of the outcome. To increase transparency and secure the purely confirmatory nature of the investigation, the 2 parties set up a publicly available adversarial collaboration agreement that detailed the proposed design and all foreseeable aspects of the data analysis. As anticipated by the skeptics, a series of Bayesian hypothesis tests indicated that horizontal eye movements did not improve free recall performance. The skeptics suggested that the nonreplication may partly reflect the use of suboptimal and questionable research practices in earlier eye movement studies. The proponents countered this suggestion and used a p curve analysis to argue that the effect of horizontal eye movements on explicit memory did not merely reflect selective reporting. ",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Preregistration,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science,Transparency"
2020-06-10T19:32:33.133Z,Bayes for Beginners: Probability and Likelihood,https://www.psychologicalscience.org/observer/bayes-for-beginners-probability-and-likelihood,C. Randy Gallistel,Blog,College / Upper Division (Undergraduates),A blog about Bayes for Beginners: Probability and Likelihood,English,I don't see any of these,Student,"Math & Statistics, Social Science",Conceptual and Statistical Knowledge,
2020-06-10T19:34:02.572Z,Publication bias and the limited strength model of self-control: has the evidence for ego depletion been overestimated?,https://doi.org/10.3389/fpsyg.2014.00823,Evan C. Carter and Michael E. McCullough,"Primary Source, Reading",College / Upper Division (Undergraduates),"Few models of self-control have generated as much scientific interest as has the limited strength model. One of the entailments of this model, the depletion effect, is the expectation that acts of self-control will be less effective when they follow prior acts of self-control. Results from a previous meta-analysis concluded that the depletion effect is robust and medium in magnitude (d = 0.62). However, when we applied methods for estimating and correcting for small-study effects (such as publication bias) to the data from this previous meta-analysis effort, we found very strong signals of publication bias, along with an indication that the depletion effect is actually no different from zero. We conclude that until greater certainty about the size of the depletion effect can be established, circumspection about the existence of this phenomenon is warranted, and that rather than elaborating on the model, research efforts should focus on establishing whether the basic effect exists. We argue that the evidence for the depletion effect is a useful case study for illustrating the dangers of small-study effects as well as some of the possible tools for mitigating their influence in psychological science.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-11T06:33:37.439Z,I Fooled Millions Into Thinking Chocolate Helps Weight Loss. Here's How.,https://io9.gizmodo.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800,John Bohannon,"Primary Source, Reading, Blog",College / Upper Division (Undergraduates),A blog about reproducibility crisis,English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-11T06:35:21.899Z,Identifying Participants in the Personal Genome Project by Name,http://dataprivacylab.org/projects/pgp/1021-1.pdf,"Sweeney, Latanya, Akua Abu, and Julia Winn.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"We linked names and contact information to publicly available profiles in the Personal Genome Project. These profiles contain medical and genomic information, including details about medications, procedures and diseases, and demographic information, such as date of birth, gender, and postal code. By linking demographics to public records such as voter lists, and mining for names hidden in attached documents, we correctly identified 84 to 97 percent of the profiles for which we provided names. Our ability to learn their names is based on their demographics, not their DNA, thereby revisiting an old vulnerability that could be easily thwarted with minimal loss of research value. So, we propose technical remedies for people to learn about their demographics to make better decisions.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-11T06:39:53.751Z,Making Prospective Registration of Observational Research a Reality,https://doi.org/10.1126/scitranslmed.3007513,"Rafael Dal-Ré, John P. Ioannidis,  Michael B. Bracken, Patricia A. Buffler,  An-Wen Chan,  Eduardo L. Franco,  Carlo La Vecchia,  ElisabeteWeiderpass","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data.",English,I don't see any of these,Student,"Applied Science, Life Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-06-11T06:44:55.555Z,Research preregistration 101. ,https://www.psychologicalscience.org/observer/research-preregistration-101,"Lindsay, D.S., Simons, D.J., Lilienfeld, S.O. ",Blog,College / Upper Division (Undergraduates),A blog about pre-registration,English,I don't see any of these,Student,Social Science,"Conceptual and Statistical Knowledge,Preregistration",Blog
2020-06-11T07:01:18.532Z,A proposal for a new editorial policy in the social sciences.,https://www.tandfonline.com/doi/abs/10.1080/00031305.1970.10478884,G. William Walster &T. Anne Cleary,"Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"“… there's this desert prison, see, with an old prisoner, resigned to his life, and a young one just arrived. The young one talks constantly of escape, and, after a few months, he makes a break. He's gone a week, and then he's brought back by the guards. He's half dead, crazy with hunger and thirst. He describes how awful it was to the old prisoner. The endless stretches of sand, no oasis, no signs of life anywhere. The old prisoner listens for a while, then says, ‘Yep. I know. I tried to escape myself, twenty years ago.’ The young prisoner says, ‘You did? Why didn't you tell me, all these months I was planning my escape? Why didn't you let me know it was impossible?’ And the old prisoner shrugs, and says, ‘So who publishes negative results?’” (Hudson, 1968, p. 168)",English,I don't see any of these,Student,"Applied Science, Social Science",Conceptual and Statistical Knowledge,
2020-06-11T07:03:26.467Z,"Registered Replication Report: Study 1 From Finkel, Rusbult, Kumashiro, & Hannon (2002).",http://journals.sagepub.com/doi/pdf/10.1177/1745691616664694,"Cheung, I., Campbell, L., LeBel, E. P., Ackerman, R. A., Aykutoğlu, B., Bahník, Š., ... & Carcedo, R. J.","Primary Source, Reading, Paper",College / Upper Division (Undergraduates),"Finkel, Rusbult, Kumashiro, and Hannon (2002, Study 1) demonstrated a causal link between subjective commitment to a relationship and how people responded to hypothetical betrayals of that relationship. Participants primed to think about their commitment to their partner (high commitment) reacted to the betrayals with reduced exit and neglect responses relative to those primed to think about their independence from their partner (low commitment). The priming manipulation did not affect constructive voice and loyalty responses. Although other studies have demonstrated a correlation between subjective commitment and responses to betrayal, this study provides the only experimental evidence that inducing changes to subjective commitment can causally affect forgiveness responses. This Registered Replication Report (RRR) meta-analytically combines the results of 16 new direct replications of the original study, all of which followed a standardized, vetted, and preregistered protocol. The results showed little effect of the priming manipulation on the forgiveness outcome measures, but it also did not observe an effect of priming on subjective commitment, so the manipulation did not work as it had in the original study. We discuss possible explanations for the discrepancy between the findings from this RRR and the original study.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-08-06T12:27:08.954Z,Intellectual humility is central to science,https://psyarxiv.com/edh2s/,Rink Hoekstra and Simine Vazire,Reading,College / Upper Division (Undergraduates),"Transparency is indispensable for accuracy and correction in science, and is discussed frequently in the credibility revolution. A less often mentioned aspect of credibility is the need for intellectual humility: When scientific communication is overconfident or contains too many exaggerations, the field stands to lose its credibility, even if the methods and statistics underlying the research are sound. We argue that intellectual humility is given a great deal of lip service, but is too rarely valued - we may say that we as scientists ought to be intellectually humble, but our actions as a field suggest that this is not a priority. Although we acknowledge that intellectual humility is presented as a widely accepted scientific norm, we argue that current research practice does not actually incentivize intellectual humility. A promising solution could be to use our roles as reviewers to incentivize authors putting the flaws and uncertainty in their work front and center, thus giving their critics ammunition to find their errors. We describe several ways reviewers (and authors) can contribute to increasing humility in practice, instead of passively waiting for the system to change.",English,I don't see any of these,Student,"Applied Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"
2020-08-06T15:26:43.117Z,How scientists can stop fooling themselves,https://doi.org/10.1038/d41586-020-02275-8,Professor Dorothy Bishop,Reading,College / Upper Division (Undergraduates),Sampling simulated data can reveal common ways in which our cognitive biases mislead us.,English,I don't see any of these,"Student, Teacher, Researcher","Applied Science, Arts and Humanities, Business and Communication, Career and Technical Education, Education, English Language Arts, History, Law, Life Science, Math & Statistics, Physical Science, Social Science","Reproducibility and Replicability Knowledge,Replication Research","Reproducibility Crisis and Credibility Revolution,Open Science"